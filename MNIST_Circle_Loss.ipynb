{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled96.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJQhIMdRbZeYbRICtRNkTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Colab_Scripts/blob/master/MNIST_Circle_Loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Circle_Loss_MNIST**\n",
        "https://cpp-learning.com/circle-loss/ <br>\n",
        "https://github.com/TinyZeaMays/CircleLoss/blob/master/mnist_example.py <br>"
      ],
      "metadata": {
        "id": "tC45Q7fc4wUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import manifold\n",
        " \n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "use_cuda = torch.cuda.is_available() and True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "BlRIWnlg44V0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader\n",
        "def get_loader(is_train: bool, batch_size: int) -> DataLoader:\n",
        "    trainset = datasets.MNIST(\n",
        "                                root=\"./data\",\n",
        "                                train=is_train,\n",
        "                                transform=ToTensor(),\n",
        "                                download=True\n",
        "                              )\n",
        "    return DataLoader(trainset, batch_size=batch_size, shuffle=is_train)\n",
        "\n",
        "# data loder\n",
        "train_loader = get_loader(is_train=True, batch_size=64)\n",
        "val_loader = get_loader(is_train=False, batch_size=64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Visualize Circle Loss\n",
        "def visualize(features, labels, num_classes):\n",
        "    # t-SNEで2次元に圧縮\n",
        "    tsne = manifold.TSNE(n_components=2, init='pca', random_state=501)\n",
        "    features = tsne.fit_transform(features)\n",
        "    \n",
        "    # カラーマップ\n",
        "    colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
        "    \n",
        "    # 描画\n",
        "    for i in range(num_classes):\n",
        "        plt.plot(features[labels == i, 0], features[labels == i, 1], '.', c=colors[i])\n",
        "\n",
        "    # グラフ設定\n",
        "    plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Gil-jz3u5Ikn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Circle Loss\n",
        "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n",
        "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
        "\n",
        "    positive_matrix = label_matrix.triu(diagonal=1)\n",
        "    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n",
        "\n",
        "    similarity_matrix = similarity_matrix.view(-1)\n",
        "    positive_matrix = positive_matrix.view(-1)\n",
        "    negative_matrix = negative_matrix.view(-1)\n",
        "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n",
        "\n",
        "\n",
        "class CircleLoss(nn.Module):\n",
        "    def __init__(self, m: float, gamma: float) -> None:\n",
        "        super(CircleLoss, self).__init__()\n",
        "        self.m = m\n",
        "        self.gamma = gamma\n",
        "        self.soft_plus = nn.Softplus()\n",
        "\n",
        "    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n",
        "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
        "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
        "\n",
        "        delta_p = 1 - self.m\n",
        "        delta_n = self.m\n",
        "\n",
        "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
        "        logit_n = an * (sn - delta_n) * self.gamma\n",
        "\n",
        "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "YlO05sZT5M0_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KdPdQ7cosK0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make CNN Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Model, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        feature = self.feature_extractor(input).mean(dim=[2, 3])\n",
        "        return nn.functional.normalize(feature)"
      ],
      "metadata": {
        "id": "rGLPF07a5jXc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, loss_func, optimizer, patience, n_epochs):\n",
        "    \n",
        "    # to track the training loss as the model trains\n",
        "    train_losses = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_losses = []\n",
        "    # to track the average training loss per epoch as the model trains\n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "\n",
        "        running_corrects, train_acc= 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                preds = model(inputs)\n",
        "                loss = criterion(*convert_label_to_similarity(preds, labels))\n",
        "\n",
        "                # backward + optimize\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # record training loss\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "       \n",
        "        model.eval() # prep model for evaluation\n",
        "\n",
        "        running_corrects, val_acc= 0, 0\n",
        "        all_features, all_labels = [], []\n",
        "\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            preds = model(inputs)\n",
        "            loss = criterion(*convert_label_to_similarity(preds, labels))\n",
        "\n",
        "            all_features.append(preds.data.cpu().numpy())\n",
        "            all_labels.append(labels.data.cpu().numpy())\n",
        "\n",
        "            # record validation loss\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'Epoch: [{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +'\\n'\n",
        "                     f'train_loss: {train_loss:.5f} ' +'\\n'\n",
        "                     f'valid_loss: {valid_loss:.5f} ' )\n",
        "        \n",
        "        print(print_msg)\n",
        "\n",
        "        \n",
        "        # 可視化\n",
        "        #all_features = np.concatenate(all_features, 0)\n",
        "        #all_labels = np.concatenate(all_labels, 0)\n",
        "        #visualize(all_features, all_labels, 10)\n",
        "        \n",
        "        \n",
        "        # clear lists to track next epoch\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        # early_stopping needs the validation loss to check if it has decresed, \n",
        "        # and if it has, it will make a checkpoint of the current model\n",
        "        early_stopping(valid_loss, model)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "        print('')\n",
        "\n",
        "    # load the last checkpoint with the best model\n",
        "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return  model, avg_train_losses, avg_valid_losses"
      ],
      "metadata": {
        "id": "z0YROFbuiIYr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ranger_adabelief\n",
        "from ranger_adabelief import RangerAdaBelief\n",
        "\n",
        "# model\n",
        "model_ft = Model().to(device)\n",
        "\n",
        "# optimzer\n",
        "#optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\n",
        "#sheduler = lr_scheduler.StepLR(optimizer_ft, 20, gamma=0.8)\n",
        "optimizer_ft = RangerAdaBelief(model_ft.parameters(), lr=1e-3, eps=1e-12, betas=(0.9,0.999))\n",
        "\n",
        "\n",
        "# CircleLoss\n",
        "criterion = CircleLoss(m=0.25, gamma=80).to(device)\n",
        "\n",
        "# Training\n",
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=20, n_epochs=150)\n",
        "\n",
        "\"\"\"\n",
        "for epoch in range(20):\n",
        "    sheduler.step()\n",
        "    train(criterion, epoch+1)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5obrO6kB5quB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize results\n",
        "\n",
        "# data loader\n",
        "val_loader = get_loader(is_train=False, batch_size=2)\n",
        "\n",
        "# 保存用\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "# 推論\n",
        "for img, label in tqdm(val_loader):\n",
        "    img, label = img.to(device), label.to(device)\n",
        "    pred = model_ft(img)\n",
        "\n",
        "    all_features.append(pred.data.cpu().numpy())\n",
        "    all_labels.append(label.data.cpu().numpy())\n",
        "    \n",
        "# 可視化\n",
        "all_features = np.concatenate(all_features, 0)\n",
        "all_labels = np.concatenate(all_labels, 0)\n",
        "visualize(all_features, all_labels, 10)"
      ],
      "metadata": {
        "id": "aJVZXHJj5v5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = get_loader(is_train=False, batch_size=2)\n",
        "\n",
        "tp = 0\n",
        "fn = 0\n",
        "fp = 0\n",
        "thresh = 0.75\n",
        "\n",
        "for img, label in val_loader:\n",
        "    img = img.to(device)\n",
        "    pred = model_ft(img)\n",
        "    gt_label = label[0] == label[1]\n",
        "    pred_label = torch.sum(pred[0] * pred[1]) > thresh\n",
        "\n",
        "    if gt_label and pred_label:\n",
        "        tp += 1\n",
        "    elif gt_label and not pred_label:\n",
        "        fn += 1\n",
        "    elif not gt_label and pred_label:\n",
        "        fp += 1\n",
        "\n",
        "print(\"Recall: {:.4f}\".format(tp / (tp + fn)))\n",
        "print(\"Precision: {:.4f}\".format(tp / (tp + fp)))"
      ],
      "metadata": {
        "id": "MiUIODdEn1_X",
        "outputId": "a2e9d5d3-c955-4ff2-d6d1-7b62b32777c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.9580\n",
            "Precision: 0.7760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "En3Yx8ojovKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}