{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOYKldz8Gl+6geavIe7QJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Colab_Scripts/blob/master/Web_image_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**画像のスクレイピング**\n",
        "\n",
        "準備：\n",
        "- Microsoft AZUREに登録\n",
        "\n",
        "    https://learn.microsoft.com/ja-jp/azure/cognitive-services/bing-web-search/\n",
        "\n",
        "- 左のタブ → リソースの作成 → Bing Search v7を取得\n",
        "\n",
        "- ダッシュボード → キーとエンドポイントからキーを取得する\n"
      ],
      "metadata": {
        "id": "25UMQJsL7UPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "bing_api_key = key[13]"
      ],
      "metadata": {
        "id": "Sa4lK_QK7-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2e0e2e-896f-4937-bd0a-f785b54eda9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_SIZE = 10\n",
        "GROUP_SIZE = 5\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/save_dir'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "    exceptions.RequestException, exceptions.HTTPError,\n",
        "    exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "search_terms = [\"forest\", \"river\", \"house\"]\n",
        "\n",
        "# set the output csv file name\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "# create the csv file and write the headers\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image URL'])\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "    \n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # initialize the total number of images downloaded thus far\n",
        "    total = 0\n",
        "\n",
        "    # loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        # update the search parameters using the current offset, then\n",
        "        # make the request to fetch the results\n",
        "        params[\"offset\"] = offset\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "        \n",
        "        # loop over the results\n",
        "        for v in results[\"value\"]:\n",
        "            # try to download the image\n",
        "            try:\n",
        "                # make a request to download the image\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                # build the path to the output image\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                # write the image to disk\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                # write the URL to the csv file\n",
        "                with open(csv_file, 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, v[\"contentUrl\"]])\n",
        "\n",
        "            # catch any errors that would not unable us to download the\n",
        "            # image\n",
        "            except Exception as e:\n",
        "                print(f\"[INFO] skipping: {v['contentUrl']}\")\n",
        "\n",
        "            # if we have reached the maximum number of images, break out\n",
        "            # of the loop\n",
        "            total += 1\n",
        "            print(f\"{total} images downloaded!\")\n",
        "            if total >= MAX_SIZE:\n",
        "                break\n",
        "\n",
        "        # if we have reached the maximum number of images, break out of\n",
        "        # the loop\n",
        "        if total >= MAX_SIZE:\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_NgBbvCwOrs",
        "outputId": "7b9627c7-b8fe-45b2-c5fa-cea835d5c188"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] searching Bing API for 'forest'\n",
            "[INFO] 10 total results for 'forest'\n",
            "[INFO] fetching: https://www.goodfreephotos.com/albums/new-zealand/other-new-zealand/magical-forest-scene-in-new-zealand.jpg\n",
            "1 images downloaded!\n",
            "[INFO] fetching: https://images3.alphacoders.com/600/600155.jpg\n",
            "2 images downloaded!\n",
            "[INFO] fetching: http://wallpapercave.com/wp/XnRmqt5.jpg\n",
            "3 images downloaded!\n",
            "[INFO] fetching: https://orig00.deviantart.net/ddf1/f/2012/303/0/0/forest_stock_by_dl_stockandresources-d5jffdv.jpg\n",
            "4 images downloaded!\n",
            "[INFO] fetching: https://wallup.net/wp-content/uploads/2015/12/190398-nature-landscape-trees-forest-wood-branch-leaves-path-grass-HDR.jpg\n",
            "5 images downloaded!\n",
            "[INFO] fetching: http://wallpapercave.com/wp/t8Yifov.jpg\n",
            "6 images downloaded!\n",
            "[INFO] fetching: https://cdn.wallpapersafari.com/1/49/LcprYi.jpg\n",
            "7 images downloaded!\n",
            "[INFO] fetching: https://wallup.net/wp-content/uploads/2015/12/190398-nature-landscape-trees-forest-wood-branch-leaves-path-grass-HDR.jpg\n",
            "8 images downloaded!\n",
            "[INFO] fetching: https://wallpapercave.com/wp/T7i4bY7.jpg\n",
            "9 images downloaded!\n",
            "[INFO] fetching: http://wallpapercave.com/wp/wp1853040.jpg\n",
            "10 images downloaded!\n",
            "[INFO] searching Bing API for 'river'\n",
            "[INFO] 10 total results for 'river'\n",
            "[INFO] fetching: http://getwallpapers.com/wallpaper/full/2/9/b/108855.jpg\n",
            "1 images downloaded!\n",
            "[INFO] fetching: https://3.bp.blogspot.com/-pALHrs4KKw4/T2i34rmzGgI/AAAAAAAABLc/xHlFbzK4W1A/s1600/Peaceful+River+Wallpapers+2.jpg\n",
            "2 images downloaded!\n",
            "[INFO] fetching: http://1.bp.blogspot.com/-StFt-IMHzL8/T_h-PbeQcSI/AAAAAAAAFHs/E8uLcQHypmg/s1600/River+Wallpapers+3.jpg\n",
            "3 images downloaded!\n",
            "[INFO] fetching: https://images7.alphacoders.com/417/417427.jpg\n",
            "4 images downloaded!\n",
            "[INFO] fetching: https://cdn.tourradar.com/s3/serp/original/221144_u3D5ZQdx.jpg\n",
            "5 images downloaded!\n",
            "[INFO] fetching: https://cdn.tourradar.com/s3/serp/original/221144_u3D5ZQdx.jpg\n",
            "6 images downloaded!\n",
            "[INFO] fetching: https://www.hdnicewallpapers.com/Walls/Big/River/Amazon_River_Photo.jpg\n",
            "7 images downloaded!\n",
            "[INFO] fetching: https://jooinn.com/images/fastflowing-river-2.jpg\n",
            "8 images downloaded!\n",
            "[INFO] fetching: https://images5.alphacoders.com/366/366294.jpg\n",
            "9 images downloaded!\n",
            "[INFO] fetching: http://getwallpapers.com/wallpaper/full/6/5/2/84697.jpg\n",
            "10 images downloaded!\n",
            "[INFO] searching Bing API for 'house'\n",
            "[INFO] 10 total results for 'house'\n",
            "[INFO] fetching: http://cdn.wallpapersafari.com/37/82/BI78kU.jpg\n",
            "1 images downloaded!\n",
            "[INFO] fetching: https://www.accentslighting.com/wp-content/uploads/2016/07/House-Front.jpg\n",
            "2 images downloaded!\n",
            "[INFO] fetching: https://i1.wp.com/googodecor.com/wp-content/uploads/2019/02/65-Stunning-Modern-Dream-House-Exterior-Design-Ideas-15.jpg?fit=1737%2C1200&ssl=1\n",
            "3 images downloaded!\n",
            "[INFO] fetching: https://images.dwell.com/photos/6133431940611203072/6460670652388048896/large.jpg\n",
            "4 images downloaded!\n",
            "[INFO] fetching: https://wallpapercave.com/wp/wp2406796.jpg\n",
            "5 images downloaded!\n",
            "[INFO] fetching: https://i.pinimg.com/originals/7a/4f/82/7a4f822186760d294917410a65a2db22.jpg\n",
            "6 images downloaded!\n",
            "[INFO] fetching: https://homestratosphere.s3.amazonaws.com/wp-content/uploads/2016/06/07101616/6ad-contemporary.jpg\n",
            "7 images downloaded!\n",
            "[INFO] fetching: https://i.pinimg.com/originals/31/9d/cf/319dcf3fcb8bb9d9519a2ed501a8c84f.jpg\n",
            "8 images downloaded!\n",
            "[INFO] fetching: https://wallpapercave.com/wp/wp2406796.jpg\n",
            "9 images downloaded!\n",
            "[INFO] fetching: http://www.freshpalace.com/wp-content/uploads/2014/12/Front-Facade1.jpg\n",
            "10 images downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chromedriverを用いる方法**"
      ],
      "metadata": {
        "id": "e0WP5ZQnAfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium==4.1.0 #新しいバージョンだとエラーが出るので旧バージョンにする"
      ],
      "metadata": {
        "id": "9frhTgD4BYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BoWBLiRVBne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これだとサムネイルしか取得できない\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Search query\n",
        "search_query = \"flowers\"\n",
        "\n",
        "# Number of images to download\n",
        "num_images = 10\n",
        "\n",
        "# Create a new folder for the images\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "# URL to search Google Images\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image tags\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Iterate through the images and download them\n",
        "for i, img in enumerate(images[:num_images]):\n",
        "    url = img['src']\n",
        "    print(i)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        open(f\"{search_query}/{search_query}_{i}.jpg\", \"wb\").write(response.content)\n",
        "    except:\n",
        "        print(\"download error\")"
      ],
      "metadata": {
        "id": "YRrPYIguIEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!curl -O https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip #Chromeのバージョンに合ったchromedriverのアドレスを設定\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Chromeドライバーの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-browser-side-navigation')\n",
        "\n",
        "# Googleで検索する\n",
        "search_query = 'flowers'\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "browser.get(url)\n",
        "\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 画像のURLを取得する\n",
        "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "img_tags = soup.find_all('img', class_='rg_i')\n",
        "\n",
        "\n",
        "urls = []\n",
        "for img in img_tags:\n",
        "    try:\n",
        "        urls.append(img[\"src\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# 画像をダウンロードする\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "counter = 0\n",
        "for i in range(num_images):\n",
        "    print(urls[i])\n",
        "    image_data = base64.b64decode(urls[i].split(',')[1])\n",
        "\n",
        "    # バイナリデータをBytesIOオブジェクトに書き込む\n",
        "    image_stream = BytesIO(image_data)\n",
        "\n",
        "    # PILで画像オブジェクトを作成する\n",
        "    image = Image.open(image_stream)\n",
        "    image_format = image.format\n",
        "\n",
        "    # 画像のネーミング\n",
        "    num= \"{:04d}\".format(i)\n",
        "    file_name = f\"{search_query}_{num}\"\n",
        "    new_image_path = f\"{search_query}/{file_name}.{image_format}\"\n",
        "\n",
        "\n",
        "    # Save image to file\n",
        "    image.save(new_image_path)\n"
      ],
      "metadata": {
        "id": "RupG3_zyQ7QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}