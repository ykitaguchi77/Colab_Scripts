{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Colab_Scripts/blob/master/yolov5_gradCAM_corneAI_revision_20250705.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42M6k9QpvSq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cakhs2BZLRA"
      },
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "è«–æ–‡revisionç”¨\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUdHWgh79uc"
      },
      "source": [
        "###**â­ï¸â­ï¸Area of interestã®è¨ˆç®—**\n",
        "\n",
        "çµæœã‚’csvã«ä¿å­˜ã™ã‚‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNjX_T_cVa-9",
        "outputId": "302a5988-72dc-48ca-cac4-ee9ca37ba20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 134 (delta 36), reused 36 (delta 36), pack-reused 94 (from 2)\u001b[K\n",
            "Receiving objects: 100% (134/134), 5.17 MiB | 21.99 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### GPUã®ç¨®é¡ã«ã‚ˆã‚ŠçµæœãŒç•°ãªã‚‹ãŸã‚ã€CPUã§è¨ˆç®—ã—ãŸã‚‚ã®ã‚’æ¡ç”¨\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colabç’°å¢ƒã§cv2_imshowã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "# FutureWarningã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶ï¼ˆregister_backward_hookä½¿ç”¨æ™‚ï¼‰\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - attempt_loadã®å ´æ‰€ã‚’ä¿®æ­£\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # ä¿®æ­£: models.experimentalã‹ã‚‰\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# === æœ€é©åŒ–: PYTORCH_CUDA_ALLOC_CONFã®è©³ç´°è¨­å®š ===\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "\n",
        "# === æœ€é©åŒ–: PyTorch 2.0ä»¥ä¸Šã®å ´åˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆ ===\n",
        "if hasattr(torch._dynamo, 'reset'):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ========== çµ±åˆç‰ˆï¼šãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==========\n",
        "class GPUMemoryMonitor:\n",
        "    \"\"\"GPU ãƒ¡ãƒ¢ãƒªã®ç›£è¦–ã¨ç®¡ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹\"\"\"\n",
        "\n",
        "    def __init__(self, warning_threshold=0.8, critical_threshold=0.9):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            warning_threshold: è­¦å‘Šã‚’å‡ºã™ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®é–¾å€¤ (0-1)\n",
        "            critical_threshold: ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«è­¦å‘Šã‚’å‡ºã™ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®é–¾å€¤ (0-1)\n",
        "        \"\"\"\n",
        "        self.warning_threshold = warning_threshold\n",
        "        self.critical_threshold = critical_threshold\n",
        "        self.device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "        self.total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
        "\n",
        "    def get_memory_stats(self):\n",
        "        \"\"\"ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’å–å¾—\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {\n",
        "                'allocated': 0,\n",
        "                'reserved': 0,\n",
        "                'free': 0,\n",
        "                'total': 0,\n",
        "                'usage_ratio': 0\n",
        "            }\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        total = self.total_memory\n",
        "        free = total - reserved\n",
        "        usage_ratio = reserved / total if total > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'allocated': allocated,\n",
        "            'reserved': reserved,\n",
        "            'free': free,\n",
        "            'total': total,\n",
        "            'usage_ratio': usage_ratio\n",
        "        }\n",
        "\n",
        "    def display_memory_status(self, prefix=\"\"):\n",
        "        \"\"\"ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¦–è¦šçš„ã«è¡¨ç¤º\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        # ä½¿ç”¨ç‡ã«å¿œã˜ãŸè‰²ä»˜ã‘ï¼ˆANSIã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚³ãƒ¼ãƒ‰ï¼‰\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            color = \"\\033[91m\"  # èµ¤\n",
        "            status = \"âš ï¸  CRITICAL\"\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            color = \"\\033[93m\"  # é»„\n",
        "            status = \"âš ï¸  WARNING\"\n",
        "        else:\n",
        "            color = \"\\033[92m\"  # ç·‘\n",
        "            status = \"âœ… OK\"\n",
        "        reset_color = \"\\033[0m\"\n",
        "\n",
        "        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ä½œæˆ\n",
        "        bar_length = 30\n",
        "        filled_length = int(bar_length * stats['usage_ratio'])\n",
        "        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)\n",
        "\n",
        "        print(f\"\\n{prefix}GPU Memory Status ({self.device_name}) {status}\")\n",
        "        print(f\"â”œâ”€ Usage: {color}[{bar}] {stats['usage_ratio']*100:.1f}%{reset_color}\")\n",
        "        print(f\"â”œâ”€ Allocated: {stats['allocated']:.2f} GB\")\n",
        "        print(f\"â”œâ”€ Reserved:  {stats['reserved']:.2f} GB\")\n",
        "        print(f\"â”œâ”€ Free:      {stats['free']:.2f} GB\")\n",
        "        print(f\"â””â”€ Total:     {stats['total']:.2f} GB\")\n",
        "\n",
        "    def check_memory_health(self):\n",
        "        \"\"\"ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’æ¨å¥¨\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            return 'critical', stats\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            return 'warning', stats\n",
        "        else:\n",
        "            return 'ok', stats\n",
        "\n",
        "    def cleanup_if_needed(self, force=False):\n",
        "        \"\"\"å¿…è¦ã«å¿œã˜ã¦ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ\"\"\"\n",
        "        health, stats = self.check_memory_health()\n",
        "\n",
        "        if health == 'critical' or force:\n",
        "            print(f\"\\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­... (ä½¿ç”¨ç‡: {stats['usage_ratio']*100:.1f}%)\")\n",
        "            aggressive_memory_cleanup()  # çµ±åˆï¼šã‚ˆã‚Šç©æ¥µçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°ã‚’ä½¿ç”¨\n",
        "\n",
        "            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "            new_stats = self.get_memory_stats()\n",
        "            freed = stats['reserved'] - new_stats['reserved']\n",
        "            print(f\"âœ¨ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {freed:.2f} GB è§£æ”¾ã•ã‚Œã¾ã—ãŸ\")\n",
        "\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# === çµ±åˆï¼šæ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æœ‰ç”¨ãªé–¢æ•°ã‚’è¿½åŠ  ===\n",
        "def find_cuda_tensors():\n",
        "    \"\"\"ãƒ¡ãƒ¢ãƒªã«æ®‹ã£ã¦ã„ã‚‹CUDAãƒ†ãƒ³ã‚½ãƒ«ã‚’æ¤œå‡º\"\"\"\n",
        "    cuda_tensors = []\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj) and obj.is_cuda:\n",
        "                cuda_tensors.append((type(obj), obj.size(), obj.element_size() * obj.nelement()))\n",
        "        except:\n",
        "            pass\n",
        "    return cuda_tensors\n",
        "\n",
        "def log_cuda_tensors(message=\"\"):\n",
        "    \"\"\"CUDAãƒ†ãƒ³ã‚½ãƒ«ã®çŠ¶æ…‹ã‚’ãƒ­ã‚°å‡ºåŠ›\"\"\"\n",
        "    cuda_tensors = find_cuda_tensors()\n",
        "    if cuda_tensors:\n",
        "        print(f\"{message} æ¤œå‡ºã•ã‚ŒãŸCUDAãƒ†ãƒ³ã‚½ãƒ«æ•°: {len(cuda_tensors)}\")\n",
        "        # å¤§ãã„ãƒ†ãƒ³ã‚½ãƒ«ã®ã¿è¡¨ç¤º\n",
        "        large_tensors = [t for t in cuda_tensors if t[2] > 1024*1024]  # 1MBä»¥ä¸Š\n",
        "        if large_tensors:\n",
        "            print(f\"  å¤§ããªãƒ†ãƒ³ã‚½ãƒ« (>1MB): {len(large_tensors)}å€‹\")\n",
        "            for i, (tensor_type, size, bytes_size) in enumerate(large_tensors[:5]):\n",
        "                print(f\"    {i+1}. Size: {size}, Memory: {bytes_size/1024**2:.2f}MB\")\n",
        "\n",
        "def aggressive_memory_cleanup():\n",
        "    \"\"\"ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ\"\"\"\n",
        "    # Pythonã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
        "    gc.collect()\n",
        "\n",
        "    # PyTorchã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # CUDAã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆPyTorch 1.10ä»¥é™ï¼‰\n",
        "        if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):\n",
        "            torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "    # PyTorch 2.0ä»¥ä¸Šã®å ´åˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ã‚¯ãƒªã‚¢\n",
        "    if hasattr(torch._dynamo, 'reset'):\n",
        "        torch._dynamo.reset()\n",
        "\n",
        "def set_model_gradients(model, layer_name, enable=True):\n",
        "    \"\"\"ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–\"\"\"\n",
        "    # ã¾ãšå…¨ä½“ã®å‹¾é…ã‚’ç„¡åŠ¹åŒ–\n",
        "    for param in model.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–\n",
        "    if enable:\n",
        "        target_layer = find_yolo_layer(model, layer_name)\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# ========== æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã¯å¤‰æ›´ãªã— ==========\n",
        "# YOLOV5TorchObjectDetectorã‚¯ãƒ©ã‚¹ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === æœ€é©åŒ–: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ– ===\n",
        "        # Grad-CAMä½¿ç”¨æ™‚ã®ã¿å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æœ‰åŠ¹åŒ–\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === å¤‰æ›´ç‚¹: æ¨è«–éƒ¨åˆ†ã®with torch.no_grad()ã‚’å‰Šé™¤ ===\n",
        "        # Grad-CAMã®ãŸã‚ã«å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: NMSå¾Œã®å‡¦ç†ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’ä½¿ç”¨ ===\n",
        "        with torch.no_grad():\n",
        "            # ä»¥ä¸‹ã®å‡¦ç†ã¯CPUã§è¡Œã†ã“ã¨ã§GPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                            range(4)]\n",
        "            for i, det in enumerate(prediction):  # detections per image\n",
        "                if len(det):\n",
        "                    # detã‚’CPUã«ç§»å‹•ã—ã€å…ƒã®GPUãƒ†ãƒ³ã‚½ãƒ«ã¯å³åº§ã«å‰Šé™¤\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det  # GPUãƒ¡ãƒ¢ãƒªã‚’å³åº§ã«è§£æ”¾\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            # predictionã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        # ãƒªã‚µã‚¤ã‚ºå‡¦ç†ã‚’åŠ¹ç‡åŒ–\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0  # ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMã‚¯ãƒ©ã‚¹ï¼ˆå¤‰æ›´ãªã—ï¼‰\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\", debug=False):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.debug = debug  # ãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ©ã‚°\n",
        "        self.layer_name = layer_name  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ä¿å­˜\n",
        "\n",
        "        # === æ”¹å–„ç‚¹1: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            # === å¤‰æ›´ç‚¹: detach()ã—ã¦ã‹ã‚‰clone()ã§ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            try:\n",
        "                # detach()ã§è¨ˆç®—ã‚°ãƒ©ãƒ•ã‹ã‚‰åˆ‡ã‚Šé›¢ã—ã¦ã‹ã‚‰clone()\n",
        "                self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Successfully detached and cloned grad_output for {layer_name}\")\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Error in backward_hook: {e}\")\n",
        "                raise\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            # === å¤‰æ›´ç‚¹: outputã‚‚detach().clone()ã—ã¦ã‚ˆã‚Šå®‰å…¨ã« ===\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # === æœ€é©åŒ–: å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ– ===\n",
        "        set_model_gradients(self.model, layer_name, enable=True)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹2: ãƒ•ãƒƒã‚¯ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ ===\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"ãƒ•ãƒƒã‚¯ã¨ãƒ¡ãƒ¢ãƒªã‚’æ˜ç¤ºçš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹\"\"\"\n",
        "        # === æ”¹å–„ç‚¹3: ãƒ•ãƒƒã‚¯ã®å‰Šé™¤ ===\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Forward hook removed for {self.layer_name}\")\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Backward hook removed for {self.layer_name}\")\n",
        "\n",
        "        # === æ”¹å–„ç‚¹4: è¾æ›¸ã®ã‚¯ãƒªã‚¢ ===\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # === æœ€é©åŒ–: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ– ===\n",
        "        set_model_gradients(self.model, self.layer_name, enable=False)\n",
        "\n",
        "        # === æ”¹å–„ç‚¹5: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¬ã‚¤ãƒ¤ãƒ¼ã¸ã®å‚ç…§ã‚’å‰Šé™¤ ===\n",
        "        self.target_layer = None\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] GradCAM cleanup completed for {self.layer_name}\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Entering GradCAM context for {self.layer_name}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†ãƒã‚¤ãƒ³ãƒˆï¼ˆè‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Exiting GradCAM context for {self.layer_name}\")\n",
        "            if exc_type is not None:\n",
        "                print(f\"[DEBUG] Exception occurred: {exc_type.__name__}: {exc_val}\")\n",
        "        self.cleanup()\n",
        "        # Falseã‚’è¿”ã™ã“ã¨ã§ä¾‹å¤–ã‚’å†ç™ºç”Ÿã•ã›ã‚‹\n",
        "        return False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§ã‚‚å¿µã®ãŸã‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\"\"\"\n",
        "        try:\n",
        "            self.cleanup()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # === æ”¹å–„ç‚¹: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«é †ä¼æ’­ã‚’å†å®Ÿè¡Œ ===\n",
        "        # ã¾ãšã€ä¸€åº¦ã ã‘ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œã—ã¦top3_indicesã‚’å–å¾—\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            # top3_indicesã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "            del top3_indices\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            # EigenCAMã®å ´åˆã¯ä¸€åº¦ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§è¨ˆç®—\n",
        "            with torch.no_grad():  # EigenCAMã¯å‹¾é…ä¸è¦\n",
        "                saliency_map = self._eigencam()\n",
        "                saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            # GradCAMã¾ãŸã¯GradCAM++ã®å ´åˆã€å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«å‡¦ç†\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                # === é‡è¦ãªå¤‰æ›´: å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å®Ÿè¡Œ ===\n",
        "                # ãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # === è¿½åŠ : ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹æ“ä½œã‚’é¿ã‘ã‚‹ãŸã‚ã«input_imgã‚’clone ===\n",
        "                input_img_clone = input_img.clone()\n",
        "\n",
        "                # æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "                _, new_logits = self.model(input_img_clone)\n",
        "\n",
        "                # å³åº§ã«ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del input_img_clone\n",
        "\n",
        "                if class_idx:\n",
        "                    score = new_logits[0][0][cls]\n",
        "                else:\n",
        "                    score = new_logits[0][0].max()\n",
        "\n",
        "                # === å¤‰æ›´ç‚¹: retain_graph=Trueã‚’å‰Šé™¤ ===\n",
        "                score.backward()  # retain_graph=Trueã‚’å‰Šé™¤\n",
        "\n",
        "                # === è¿½åŠ : gradientsã¨activationsãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª ===\n",
        "                if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                    print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                    del score, new_logits  # å­˜åœ¨ã—ãªã„å ´åˆã¯ã“ã“ã§å‰Šé™¤\n",
        "                    continue\n",
        "\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # scoreã¨new_logitsã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
        "                del score\n",
        "                del new_logits\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    del weights  # weightsã‚’å³åº§ã«å‰Šé™¤\n",
        "\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                    # min/maxã¯ä¸è¦ãªã®ã§å‰Šé™¤\n",
        "                    del saliency_map_min, saliency_map_max\n",
        "\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "                # === è¿½åŠ : å„ã‚¯ãƒ©ã‚¹ã®å‡¦ç†å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                # å‹¾é…ã¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å³åº§ã«å‰Šé™¤\n",
        "                if \"value\" in self.gradients:\n",
        "                    del self.gradients[\"value\"]\n",
        "                if \"value\" in self.activations:\n",
        "                    del self.activations[\"value\"]\n",
        "\n",
        "                # ä¸è¦ãªä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "                del gradients, activations\n",
        "\n",
        "                # CUDAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # viewæ“ä½œã¯æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆã—ãªã„ã®ã§åŠ¹ç‡çš„\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha  # alphaã‚’å‰Šé™¤\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        del cov  # å…±åˆ†æ•£è¡Œåˆ—ã‚’å‰Šé™¤\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        del eigenvalues, eigenvectors  # ä¸è¦ãªå›ºæœ‰å€¤ãƒ»å›ºæœ‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        del activations_reshaped, leading_eigenvector  # ä¸­é–“å¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        del eigen_cam_min, eigen_cam_max  # min/maxã‚’å‰Šé™¤\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layeré–¢æ•°\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoié–¢æ•°ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ç‰ˆï¼‰\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():  # å…¨ä½“ã‚’å‹¾é…è¨ˆç®—ä¸è¦ã§å›²ã‚€\n",
        "        for mask in masks:\n",
        "            # ãƒã‚¹ã‚¯ã‚’CPUã§å‡¦ç†ï¼ˆGPUãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask  # å…ƒã®ãƒã‚¹ã‚¯ã¯å³åº§ã«å‰Šé™¤\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # é–¾å€¤å‡¦ç†ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ–¹æ³•ï¼‰\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu  # å‡¦ç†æ¸ˆã¿ã®ãƒã‚¹ã‚¯ã¯å‰Šé™¤\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask  # ä¸è¦ãªå¤‰æ•°ã‚’å‰Šé™¤\n",
        "\n",
        "        mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# ========== çµ±åˆç‰ˆï¼šcalculate_aoié–¢æ•°ï¼ˆè¦–è¦šçš„ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰ ==========\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True, debug_mode=False):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # ãƒ¡ãƒ¢ãƒªãƒ¢ãƒ‹ã‚¿ãƒ¼ã®åˆæœŸåŒ–\n",
        "    memory_monitor = GPUMemoryMonitor(warning_threshold=0.75, critical_threshold=0.85)\n",
        "\n",
        "    # é–‹å§‹æ™‚ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ³ã‚’è¡¨ç¤º\n",
        "    memory_monitor.display_memory_status(\"ğŸš€ å‡¦ç†é–‹å§‹æ™‚\")\n",
        "\n",
        "    # === ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º ===\n",
        "    print(\"\\n=== ã‚·ã‚¹ãƒ†ãƒ æƒ…å ± ===\")\n",
        "    print(f\"Python version: {sys.version}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "            props = torch.cuda.get_device_properties(i)\n",
        "            print(f\"    Total memory: {props.total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # AOIã‚«ãƒ©ãƒ ã®å®šç¾©\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ \n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # å‡¦ç†æ¸ˆã¿ç”»åƒã®åˆ¤å®šé–¢æ•°\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # æœªå‡¦ç†ã®ç”»åƒã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # å‡¦ç†å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’æŒ‡å®šã—ãŸå ´åˆ\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedãŒTrueã®å ´åˆã€ç¯„å›²å†…ã§ã‚‚å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(target_indices)}\")\n",
        "\n",
        "    # é€²æ—çŠ¶æ³ã®è¡¨ç¤º\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    cuda_error_occurred = False\n",
        "    memory_cleanup_count = 0\n",
        "\n",
        "    # ã‚¨ãƒ©ãƒ¼è©³ç´°ã®è¨˜éŒ²ç”¨\n",
        "    error_details = {}\n",
        "    # æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    successful_layers = 0\n",
        "\n",
        "    # å®šæœŸçš„ãªä¿å­˜ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
        "    save_interval = 5  # 5ç”»åƒã”ã¨ã«ä¿å­˜ï¼ˆã‚ˆã‚Šé »ç¹ã«ï¼‰\n",
        "    memory_check_interval = 3  # 3ç”»åƒã”ã¨ã«ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        # å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯\n",
        "        if i % memory_check_interval == 0 and i > 0:\n",
        "            health, stats = memory_monitor.check_memory_health()\n",
        "\n",
        "            if health == 'critical':\n",
        "                print(f\"\\nâš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ« ({stats['usage_ratio']*100:.1f}%) ã§ã™ï¼\")\n",
        "                memory_monitor.display_memory_status(\"ğŸ“Š ç¾åœ¨\")\n",
        "\n",
        "                # å¼·åˆ¶ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "                if memory_monitor.cleanup_if_needed(force=True):\n",
        "                    memory_cleanup_count += 1\n",
        "                    time.sleep(2)  # GPUã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³\n",
        "\n",
        "                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®çŠ¶æ…‹ã‚’è¡¨ç¤º\n",
        "                    memory_monitor.display_memory_status(\"ğŸ”„ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ\")\n",
        "\n",
        "                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€è©³ç´°ãªãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ã‚‚è¡¨ç¤º\n",
        "                    if debug_mode:\n",
        "                        log_cuda_tensors(\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "            elif health == 'warning':\n",
        "                if i % (memory_check_interval * 3) == 0:  # è­¦å‘Šæ™‚ã¯é »åº¦ã‚’ä¸‹ã’ã¦è¡¨ç¤º\n",
        "                    memory_monitor.display_memory_status(\"âš ï¸  ãƒ¡ãƒ¢ãƒªè­¦å‘Š\")\n",
        "\n",
        "                # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ\n",
        "                if memory_monitor.cleanup_if_needed():\n",
        "                    memory_cleanup_count += 1\n",
        "\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # === æ”¹å–„ç‚¹: å‰å‡¦ç†å¾Œã€å…ƒã®ç”»åƒã¯å³åº§ã«å‰Šé™¤ ===\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "                del img  # å…ƒã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯debug=Trueã‚’æ¸¡ã™\n",
        "                        debug_flag = debug_mode and processed_count < 5\n",
        "\n",
        "                        # === æ”¹å–„ç‚¹: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ ===\n",
        "                        with YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                          img_size=input_size, method=\"gradcampp\",\n",
        "                                          debug=debug_flag) as saliency_method:\n",
        "\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "                                successful_layers += 1\n",
        "\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šAOIå€¤ãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
        "                                if processed_count < 5:  # æœ€åˆã®5ç”»åƒã®ã¿ãƒ­ã‚°å‡ºåŠ›\n",
        "                                    print(f\"  Layer {layer_name}: AOI = {aoi:.4f}\")\n",
        "\n",
        "                                # === æ”¹å–„ç‚¹: ãƒã‚¹ã‚¯ã¨ãƒœãƒƒã‚¯ã‚¹ã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                                del mask, bbox\n",
        "                            else:\n",
        "                                # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒã‚¹ã‚¯ã¾ãŸã¯ãƒœãƒƒã‚¯ã‚¹ãŒç©ºã®å ´åˆ\n",
        "                                if processed_count < 5:\n",
        "                                    print(f\"  Layer {layer_name}: No masks or boxes detected\")\n",
        "\n",
        "                            # === æ”¹å–„ç‚¹: çµæœã‚’å³åº§ã«å‰Šé™¤ ===\n",
        "                            del masks, logits, boxes, cls_names\n",
        "\n",
        "                        # withæ–‡ã‚’æŠœã‘ãŸæ™‚ç‚¹ã§è‡ªå‹•çš„ã«cleanup()ãŒå‘¼ã°ã‚Œã‚‹\n",
        "                        # è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå„ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†å¾Œï¼‰\n",
        "                        aggressive_memory_cleanup()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                        if \"CUDA out of memory\" in str(e):\n",
        "                            print(f\"\\n\\nCUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                            print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                            print(f\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ¬ã‚¤ãƒ¤ãƒ¼: {layer_name}\")\n",
        "                            memory_monitor.display_memory_status(\"ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚\")\n",
        "                            if debug_mode:\n",
        "                                log_cuda_tensors(\"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚:\")\n",
        "                            cuda_error_occurred = True\n",
        "                            break\n",
        "                        else:\n",
        "                            error_count += 1\n",
        "                            # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¨˜éŒ²\n",
        "                            error_key = f\"{img_basename}_{layer_name}\"\n",
        "                            if error_key not in error_details:\n",
        "                                error_details[error_key] = str(e)\n",
        "\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "                if cuda_error_occurred:\n",
        "                    break\n",
        "\n",
        "                # === æ”¹å–„ç‚¹: å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†å¾Œã«torch_imgã‚’å‰Šé™¤ ===\n",
        "                del torch_img\n",
        "\n",
        "                # === æœ€é©åŒ–: å„ç”»åƒå‡¦ç†å¾Œã«å®Œå…¨ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                aggressive_memory_cleanup()\n",
        "\n",
        "                processed_count += 1\n",
        "\n",
        "                # é€²æ—è¡¨ç¤ºã®æ”¹å–„ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚‚è¡¨ç¤ºï¼‰\n",
        "                if processed_count % 10 == 0:\n",
        "                    stats = memory_monitor.get_memory_stats()\n",
        "                    print(f\"\\nğŸ“ˆ é€²æ—: {processed_count}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿ \"\n",
        "                          f\"(GPUä½¿ç”¨ç‡: {stats['usage_ratio']*100:.1f}%)\")\n",
        "\n",
        "                # å®šæœŸçš„ãªä¸­é–“ä¿å­˜\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šä¿å­˜å‰ã®ãƒ‡ãƒ¼ã‚¿ç¢ºèª\n",
        "                    saved_count = sum(1 for idx in target_indices[:i+1]\n",
        "                                    if idx < len(df) and is_processed(df.iloc[idx]))\n",
        "                    print(f\"\\nğŸ’¾ ä¸­é–“ä¿å­˜: {saved_count}å€‹ã®ç”»åƒãŒå®Œå…¨ã«å‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ­ã‚°\n",
        "                    memory_monitor.display_memory_status(\"ğŸ’¾ ä¿å­˜æ™‚\")\n",
        "\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"âœ… ä¸­é–“ä¿å­˜å®Œäº†: {i + 1}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # === æœ€é©åŒ–: ã‚ˆã‚Šç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "                    aggressive_memory_cleanup()\n",
        "\n",
        "                    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "                    memory_monitor.display_memory_status(\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "                    if debug_mode:\n",
        "                        log_cuda_tensors(\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # CUDA out of memoryã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    print(f\"\\n\\nğŸ’¥ CUDA out of memory ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼\")\n",
        "                    print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}\")\n",
        "                    print(f\"ç¾åœ¨ã®ç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "                    memory_monitor.display_memory_status(\"ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚\")\n",
        "                    cuda_error_occurred = True\n",
        "                    break\n",
        "                else:\n",
        "                    error_count += 1\n",
        "                    if error_count <= 5:\n",
        "                        print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "        # CUDA out of memoryã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹\n",
        "        if cuda_error_occurred:\n",
        "            break\n",
        "\n",
        "    # æœ€çµ‚ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚å«ã‚€ï¼‰\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n",
        "    memory_monitor.display_memory_status(\"ğŸ å‡¦ç†çµ‚äº†æ™‚\")\n",
        "    if debug_mode:\n",
        "        log_cuda_tensors(\"æœ€çµ‚çŠ¶æ…‹:\")\n",
        "\n",
        "    if cuda_error_occurred:\n",
        "        print(f\"\\n\\n========== CUDA OUT OF MEMORY ã‚¨ãƒ©ãƒ¼ã§å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸ ==========\")\n",
        "        print(f\"æœ€å¾Œã«å‡¦ç†ã—ã‚ˆã†ã¨ã—ãŸç”»åƒ: {img_basename} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index})\")\n",
        "        print(f\"å‡¦ç†æ¸ˆã¿ç”»åƒæ•°: {processed_count}\")\n",
        "        print(f\"æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°: {memory_cleanup_count}å›\")\n",
        "        print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {len(target_indices) - i}\")\n",
        "        print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "        print(f\"\\næ¬¡å›å®Ÿè¡Œæ™‚ã¯è‡ªå‹•çš„ã«ç¶šãã‹ã‚‰å‡¦ç†ã•ã‚Œã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nâœ¨ å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "        print(f\"â”œâ”€ å‡¦ç†ã—ãŸç”»åƒ: {processed_count}å€‹\")\n",
        "        print(f\"â”œâ”€ æˆåŠŸã—ãŸãƒ¬ã‚¤ãƒ¤ãƒ¼å‡¦ç†: {successful_layers}å€‹\")\n",
        "        print(f\"â”œâ”€ ã‚¹ã‚­ãƒƒãƒ—ã—ãŸç”»åƒ: {skipped_count}å€‹\")\n",
        "        print(f\"â”œâ”€ è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "        print(f\"â”œâ”€ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "        print(f\"â””â”€ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°: {memory_cleanup_count}å›\")\n",
        "\n",
        "        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º\n",
        "        if error_details:\n",
        "            print(f\"\\nâš ï¸  ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ï¼ˆæœ€åˆã®10ä»¶ï¼‰:\")\n",
        "            for i, (key, error_msg) in enumerate(list(error_details.items())[:10]):\n",
        "                print(f\"  {i+1}. {key}: {error_msg[:100]}...\")\n",
        "\n",
        "        # æœ€çµ‚çš„ãªå®Œå…¨å‡¦ç†æ¸ˆã¿ç”»åƒæ•°ã‚’ç¢ºèª\n",
        "        fully_processed = sum(1 for idx, row in df.iterrows() if is_processed(row))\n",
        "        print(f\"\\nğŸ“Š å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒæ•°: {fully_processed}/{len(df)}\")\n",
        "\n",
        "        print(f\"\\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ========== å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰è¨­å®š ==========\n",
        "# ä»¥ä¸‹ã®å¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã§ãã¾ã™\n",
        "\n",
        "# MODE = \"auto\"ï¼šæœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†\n",
        "# MODE = \"manual\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "# MODE = \"force\"ï¼šstart_indexã¨end_indexã§ç¯„å›²ã‚’æŒ‡å®šï¼ˆå‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "# MODE = \"debug\"ï¼šæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "MODE = \"auto\"  # \"auto\", \"manual\", \"force\", \"debug\" ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®š\n",
        "\n",
        "# manualãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯forceãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã®ç¯„å›²æŒ‡å®š\n",
        "MANUAL_START_INDEX = 0\n",
        "MANUAL_END_INDEX = 10\n",
        "\n",
        "# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ† ==========\n",
        "# ãƒ‡ãƒãƒƒã‚°é–¢æ•°ï¼šCSVãƒ•ã‚¡ã‚¤ãƒ«ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "def check_csv_status(csv_path, threshold=0.5):\n",
        "    \"\"\"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†çŠ¶æ³ã‚’ç¢ºèªã™ã‚‹é–¢æ•°\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    all_layers_cols = [\n",
        "        f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layer24_m_0\",\n",
        "        f\"AOI_{threshold}_layer24_m_1\",\n",
        "        f\"AOI_{threshold}_layer24_m_2\"\n",
        "    ]\n",
        "\n",
        "    print(f\"CSVãƒ•ã‚¡ã‚¤ãƒ«: {csv_path}\")\n",
        "    print(f\"ç·è¡Œæ•°: {len(df)}\")\n",
        "\n",
        "    # å„ã‚«ãƒ©ãƒ ã®çŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    for col in all_layers_cols:\n",
        "        if col in df.columns:\n",
        "            non_null_count = df[col].notna().sum()\n",
        "            print(f\"  {col}: {non_null_count}/{len(df)} ({non_null_count/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  {col}: ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
        "\n",
        "    # å®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®è¡Œæ•°\n",
        "    fully_processed = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if all(pd.notna(row[col]) and row[col] is not None for col in all_layers_cols if col in df.columns):\n",
        "            fully_processed += 1\n",
        "\n",
        "    print(f\"\\nå®Œå…¨ã«å‡¦ç†æ¸ˆã¿ã®ç”»åƒ: {fully_processed}/{len(df)} ({fully_processed/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # æœ€åˆã®5è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
        "    print(\"\\næœ€åˆã®5è¡Œã®AOIå€¤:\")\n",
        "    for idx in range(min(5, len(df))):\n",
        "        print(f\"  è¡Œ{idx}: \", end=\"\")\n",
        "        for col in all_layers_cols:\n",
        "            if col in df.columns:\n",
        "                val = df.iloc[idx][col]\n",
        "                if pd.notna(val):\n",
        "                    print(f\"{val:.4f} \", end=\"\")\n",
        "                else:\n",
        "                    print(\"NaN \", end=\"\")\n",
        "        print()\n",
        "\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # is_processedé–¢æ•°ã‚’å®šç¾©ï¼ˆcalculate_aoié–¢æ•°å¤–ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # æœªå‡¦ç†ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nãƒãƒƒãƒ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {batch_indices}\")\n",
        "\n",
        "            # ãƒãƒƒãƒå†…ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True, debug_mode=False)\n",
        "\n",
        "            # å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            aggressive_memory_cleanup()\n",
        "\n",
        "            print(f\"ãƒãƒƒãƒ {i//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "            # ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‚‰ã€å†åº¦æœªå‡¦ç†ç”»åƒã‚’ç¢ºèªï¼ˆä¸­æ–­ã•ã‚ŒãŸå ´åˆã®å¯¾ç­–ï¼‰\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "    print(f\"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "\n",
        "    # å‡¦ç†å‰ã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å‰ã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    if MODE == \"auto\":\n",
        "        # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœªå‡¦ç†ç”»åƒã‚’æ¤œå‡ºï¼‰\n",
        "        print(\"\\næœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"manual\":\n",
        "        # æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å‡¦ç†ã—ã¾ã™ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"force\":\n",
        "        # å¼·åˆ¶å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆç¯„å›²æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MANUAL_START_INDEX} ã‹ã‚‰ {MANUAL_END_INDEX-1} ã¾ã§å¼·åˆ¶çš„ã«å†å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=False, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"debug\":\n",
        "        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ï¼‰\n",
        "        print(\"\\nãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®æœªå‡¦ç†ç”»åƒ1æšã®ã¿å‡¦ç†ã—ã¾ã™...\")\n",
        "        # æœªå‡¦ç†ç”»åƒã‚’æ¢ã™\n",
        "        df_debug = pd.read_csv(csv_path)\n",
        "        debug_idx = None\n",
        "        for idx, row in df_debug.iterrows():\n",
        "            all_layers_cols = [\n",
        "                f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layer24_m_0\",\n",
        "                f\"AOI_{threshold}_layer24_m_1\",\n",
        "                f\"AOI_{threshold}_layer24_m_2\"\n",
        "            ]\n",
        "            if any(pd.isna(row[col]) or row[col] is None for col in all_layers_cols if col in df_debug.columns):\n",
        "                debug_idx = idx\n",
        "                break\n",
        "\n",
        "        if debug_idx is not None:\n",
        "            print(f\"ãƒ‡ãƒãƒƒã‚°å¯¾è±¡: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {debug_idx}, ç”»åƒ: {df_debug.iloc[debug_idx]['image_basename']}\")\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=debug_idx, end_index=debug_idx+1,\n",
        "                         skip_processed=False, debug_mode=True)\n",
        "        else:\n",
        "            print(\"æœªå‡¦ç†ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    else:\n",
        "        print(f\"ç„¡åŠ¹ãªãƒ¢ãƒ¼ãƒ‰: {MODE}\")\n",
        "        print(\"MODE ã¯ 'auto', 'manual', 'force', 'debug' ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    # å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ã‚’ç¢ºèª\n",
        "    print(\"\\n=== å‡¦ç†å¾Œã®CSVçŠ¶æ…‹ ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "# === æœ€é©åŒ–: æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\n",
        "print(\"\\n=== æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ===\")\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # æœ€çµ‚ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®è¡¨ç¤º\n",
        "    memory_monitor = GPUMemoryMonitor()\n",
        "    memory_monitor.display_memory_status(\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "    if MODE == \"debug\":\n",
        "        log_cuda_tensors(\"æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ:\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤\n",
        "del model\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\nå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "j0yjAERw1EL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Versionã«ã‚ˆã‚‹AOIã®å·®ã‚’ç¢ºèª (çµè«–ï¼šå¤‰ã‚ã‚Šãªã—)"
      ],
      "metadata": {
        "id": "D_zPZJSgBnbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "\n",
        "# è­¦å‘Šã®æŠ‘åˆ¶\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ========== ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ==========\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"YOLOãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹\"\"\"\n",
        "    hierarchy = layer_name.split(\"_\")\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "def set_model_gradients(model, layer_name, enable=True):\n",
        "    \"\"\"ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹åŒ–\"\"\"\n",
        "    # ã¾ãšå…¨ä½“ã®å‹¾é…ã‚’ç„¡åŠ¹åŒ–\n",
        "    for param in model.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # ç‰¹å®šã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿æœ‰åŠ¹åŒ–\n",
        "    if enable:\n",
        "        target_layer = find_yolo_layer(model, layer_name)\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    \"\"\"Area of Interest (AOI)ã‚’è¨ˆç®—\"\"\"\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mask in masks:\n",
        "            # ãƒã‚¹ã‚¯ã‚’CPUã§å‡¦ç†\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # é–¾å€¤å‡¦ç†\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask\n",
        "\n",
        "    mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "    AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# ========== YOLOv5ç‰©ä½“æ¤œå‡ºå™¨ã‚¯ãƒ©ã‚¹ ==========\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "                          \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
        "                          \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
        "                          \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
        "                          \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
        "                          \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\",\n",
        "                          \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
        "                          \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
        "                          \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        import time\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]\n",
        "                v[:, 4] = 1.0\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img\n",
        "\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# ========== ãƒãƒ¼ã‚¸ãƒ§ãƒ³1: å®Œå…¨åˆæœŸç‰ˆï¼ˆå…¨ä½“å‹¾é… + 1å›ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=Trueï¼‰ ==========\n",
        "class YOLOV5GradCAM_V1_Original:\n",
        "    \"\"\"å®Œå…¨åˆæœŸç‰ˆï¼šå…¨ä½“å‹¾é…æœ‰åŠ¹åŒ– + 1å›ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ + retain_graph=True\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # å…¨ä½“ã®å‹¾é…ã‚’æœ‰åŠ¹åŒ–\n",
        "        for param in self.model.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # 1å›ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã—ã¦åŒã˜logitsã‚’ä½¿ç”¨\n",
        "        for i, (cls, cls_name) in enumerate(zip(preds[1][0], preds[2][0])):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "\n",
        "            # retain_graph=Trueã‚’ä½¿ç”¨ï¼ˆæœ€å¾Œã®ã‚¯ãƒ©ã‚¹ä»¥å¤–ï¼‰\n",
        "            if i < len(preds[1][0]) - 1:\n",
        "                score.backward(retain_graph=True)\n",
        "            else:\n",
        "                score.backward(retain_graph=False)\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== ãƒãƒ¼ã‚¸ãƒ§ãƒ³2: å…¨ä½“å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=False ==========\n",
        "class YOLOV5GradCAM_V2_AllGrad:\n",
        "    \"\"\"å…¨ä½“å‹¾é…æœ‰åŠ¹åŒ– + å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ + retain_graph=False\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # å…¨ä½“ã®å‹¾é…ã‚’æœ‰åŠ¹åŒ–\n",
        "        for param in self.model.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # æœ€åˆã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§top3ã‚’å–å¾—\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            input_img_clone = input_img.clone()\n",
        "            _, new_logits = self.model(input_img_clone)\n",
        "            del input_img_clone\n",
        "\n",
        "            if class_idx:\n",
        "                score = new_logits[0][0][cls]\n",
        "            else:\n",
        "                score = new_logits[0][0].max()\n",
        "\n",
        "            # retain_graph=False\n",
        "            score.backward()\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                del score, new_logits\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            del score, new_logits\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            if \"value\" in self.gradients:\n",
        "                del self.gradients[\"value\"]\n",
        "            if \"value\" in self.activations:\n",
        "                del self.activations[\"value\"]\n",
        "\n",
        "            del gradients, activations\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== ãƒãƒ¼ã‚¸ãƒ§ãƒ³3: é¸æŠçš„å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=False ==========\n",
        "class YOLOV5GradCAM_V3_Optimized:\n",
        "    \"\"\"é¸æŠçš„å‹¾é…æœ‰åŠ¹åŒ– + å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ + retain_graph=False\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # é¸æŠçš„å‹¾é…æœ‰åŠ¹åŒ–\n",
        "        set_model_gradients(self.model, layer_name, enable=True)\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
        "        set_model_gradients(self.model, self.layer_name, enable=False)\n",
        "\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # æœ€åˆã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§top3ã‚’å–å¾—\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«æ–°ã—ã„ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            input_img_clone = input_img.clone()\n",
        "            _, new_logits = self.model(input_img_clone)\n",
        "            del input_img_clone\n",
        "\n",
        "            if class_idx:\n",
        "                score = new_logits[0][0][cls]\n",
        "            else:\n",
        "                score = new_logits[0][0].max()\n",
        "\n",
        "            # retain_graph=False\n",
        "            score.backward()\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                del score, new_logits\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            del score, new_logits\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            if \"value\" in self.gradients:\n",
        "                del self.gradients[\"value\"]\n",
        "            if \"value\" in self.activations:\n",
        "                del self.activations[\"value\"]\n",
        "\n",
        "            del gradients, activations\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== 3ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æ¯”è¼ƒã™ã‚‹é–¢æ•° ==========\n",
        "def compare_all_versions(image_path, model, layer_names, threshold=0.5):\n",
        "    \"\"\"3ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å…¨ã¦æ¯”è¼ƒ\"\"\"\n",
        "\n",
        "    print(f\"\\nç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­: {image_path}\")\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"ç”»åƒã‚µã‚¤ã‚º: {img.shape}\")\n",
        "\n",
        "    # å‰å‡¦ç†\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    print(f\"å‰å‡¦ç†å¾Œã®ãƒ†ãƒ³ã‚½ãƒ«ã‚µã‚¤ã‚º: {torch_img.shape}\")\n",
        "\n",
        "    # ã¾ãšç‰©ä½“æ¤œå‡ºã‚’ç¢ºèª\n",
        "    with torch.no_grad():\n",
        "        [boxes_test, _, _, _], _ = model(torch_img.clone())\n",
        "        if len(boxes_test[0]) == 0:\n",
        "            print(\"ç‰©ä½“ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "            return None\n",
        "        print(f\"æ¤œå‡ºã•ã‚ŒãŸç‰©ä½“æ•°: {len(boxes_test[0])}\")\n",
        "        print(f\"æœ€åˆã®ç‰©ä½“ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹: {boxes_test[0][0]}\")\n",
        "\n",
        "    # çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
        "    results = {\n",
        "        'layer_name': [],\n",
        "        'aoi_v1_original': [],  # å®Œå…¨åˆæœŸç‰ˆ\n",
        "        'aoi_v2_allgrad': [],   # å…¨ä½“å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰\n",
        "        'aoi_v3_optimized': [], # é¸æŠçš„å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰\n",
        "        'diff_v1_v2': [],       # V1ã¨V2ã®å·®\n",
        "        'diff_v2_v3': [],       # V2ã¨V3ã®å·®\n",
        "        'diff_v1_v3': []        # V1ã¨V3ã®å·®\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== ç”»åƒ: {os.path.basename(image_path)} ã®æ¯”è¼ƒ ===\")\n",
        "    print(\"-\" * 120)\n",
        "    print(f\"{'ãƒ¬ã‚¤ãƒ¤ãƒ¼å':<25} {'V1:å®Œå…¨åˆæœŸç‰ˆ':>15} {'V2:å…¨ä½“å‹¾é…':>15} {'V3:é¸æŠçš„å‹¾é…':>15} {'V1-V2å·®':>12} {'V2-V3å·®':>12} {'V1-V3å·®':>12}\")\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    for layer_name in layer_names:\n",
        "        print(f\"\\nå‡¦ç†ä¸­ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼: {layer_name}\")\n",
        "\n",
        "        # === V1: å®Œå…¨åˆæœŸç‰ˆ ===\n",
        "        print(\"  V1: å®Œå…¨åˆæœŸç‰ˆï¼ˆå…¨ä½“å‹¾é… + 1å›ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=Trueï¼‰...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V1_Original(model=model, layer_name=layer_name,\n",
        "                                          img_size=(640, 640), method=\"gradcampp\") as gradcam_v1:\n",
        "                masks_v1, _, preds_v1, _ = gradcam_v1(torch_img.clone())\n",
        "                boxes_v1 = preds_v1[0]\n",
        "\n",
        "                if len(masks_v1) > 0 and len(boxes_v1) > 0 and len(boxes_v1[0]) > 0:\n",
        "                    mask_v1 = masks_v1[0]\n",
        "                    bbox_v1 = boxes_v1[0][0]\n",
        "                    aoi_v1 = get_aoi(bbox_v1, [mask_v1], threshold)\n",
        "                    print(f\"    AOIå€¤: {aoi_v1}\")\n",
        "                else:\n",
        "                    aoi_v1 = None\n",
        "                    print(\"    æ¤œå‡ºãªã—\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "            aoi_v1 = None\n",
        "\n",
        "        # === V2: å…¨ä½“å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ ===\n",
        "        print(\"  V2: å…¨ä½“å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=False...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V2_AllGrad(model=model, layer_name=layer_name,\n",
        "                                          img_size=(640, 640), method=\"gradcampp\") as gradcam_v2:\n",
        "                masks_v2, _, preds_v2, _ = gradcam_v2(torch_img.clone())\n",
        "                boxes_v2 = preds_v2[0]\n",
        "\n",
        "                if len(masks_v2) > 0 and len(boxes_v2) > 0 and len(boxes_v2[0]) > 0:\n",
        "                    mask_v2 = masks_v2[0]\n",
        "                    bbox_v2 = boxes_v2[0][0]\n",
        "                    aoi_v2 = get_aoi(bbox_v2, [mask_v2], threshold)\n",
        "                    print(f\"    AOIå€¤: {aoi_v2}\")\n",
        "                else:\n",
        "                    aoi_v2 = None\n",
        "                    print(\"    æ¤œå‡ºãªã—\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "            aoi_v2 = None\n",
        "\n",
        "        # === V3: é¸æŠçš„å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ ===\n",
        "        print(\"  V3: é¸æŠçš„å‹¾é… + å„ã‚¯ãƒ©ã‚¹ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ + retain_graph=False...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V3_Optimized(model=model, layer_name=layer_name,\n",
        "                                            img_size=(640, 640), method=\"gradcampp\") as gradcam_v3:\n",
        "                masks_v3, _, preds_v3, _ = gradcam_v3(torch_img.clone())\n",
        "                boxes_v3 = preds_v3[0]\n",
        "\n",
        "                if len(masks_v3) > 0 and len(boxes_v3) > 0 and len(boxes_v3[0]) > 0:\n",
        "                    mask_v3 = masks_v3[0]\n",
        "                    bbox_v3 = boxes_v3[0][0]\n",
        "                    aoi_v3 = get_aoi(bbox_v3, [mask_v3], threshold)\n",
        "                    print(f\"    AOIå€¤: {aoi_v3}\")\n",
        "                else:\n",
        "                    aoi_v3 = None\n",
        "                    print(\"    æ¤œå‡ºãªã—\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "            aoi_v3 = None\n",
        "\n",
        "        # çµæœã®è¨˜éŒ²ã¨è¡¨ç¤º\n",
        "        if aoi_v1 is not None and aoi_v2 is not None and aoi_v3 is not None:\n",
        "            diff_v1_v2 = aoi_v2 - aoi_v1\n",
        "            diff_v2_v3 = aoi_v3 - aoi_v2\n",
        "            diff_v1_v3 = aoi_v3 - aoi_v1\n",
        "\n",
        "            results['layer_name'].append(layer_name)\n",
        "            results['aoi_v1_original'].append(aoi_v1)\n",
        "            results['aoi_v2_allgrad'].append(aoi_v2)\n",
        "            results['aoi_v3_optimized'].append(aoi_v3)\n",
        "            results['diff_v1_v2'].append(diff_v1_v2)\n",
        "            results['diff_v2_v3'].append(diff_v2_v3)\n",
        "            results['diff_v1_v3'].append(diff_v1_v3)\n",
        "\n",
        "            print(f\"{layer_name:<25} {aoi_v1:>15.6f} {aoi_v2:>15.6f} {aoi_v3:>15.6f} \"\n",
        "                  f\"{diff_v1_v2:>+12.6f} {diff_v2_v3:>+12.6f} {diff_v1_v3:>+12.6f}\")\n",
        "        else:\n",
        "            v1_str = f\"{aoi_v1:.6f}\" if aoi_v1 is not None else \"æ¤œå‡ºãªã—\"\n",
        "            v2_str = f\"{aoi_v2:.6f}\" if aoi_v2 is not None else \"æ¤œå‡ºãªã—\"\n",
        "            v3_str = f\"{aoi_v3:.6f}\" if aoi_v3 is not None else \"æ¤œå‡ºãªã—\"\n",
        "            print(f\"{layer_name:<25} {v1_str:>15} {v2_str:>15} {v3_str:>15} {'N/A':>12} {'N/A':>12} {'N/A':>12}\")\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º\n",
        "    if results['diff_v1_v2']:\n",
        "        print(f\"\\nçµ±è¨ˆæƒ…å ±:\")\n",
        "        print(f\"  V1â†’V2ã®å¹³å‡å·®åˆ†: {np.mean(results['diff_v1_v2']):+.6f}\")\n",
        "        print(f\"  V2â†’V3ã®å¹³å‡å·®åˆ†: {np.mean(results['diff_v2_v3']):+.6f}\")\n",
        "        print(f\"  V1â†’V3ã®å¹³å‡å·®åˆ†: {np.mean(results['diff_v1_v3']):+.6f}\")\n",
        "\n",
        "        # è¦–è¦šåŒ–\n",
        "        if len(results['layer_name']) > 0:\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # AOIå€¤ã®æ¯”è¼ƒ\n",
        "            plt.subplot(2, 2, 1)\n",
        "            x = np.arange(len(results['layer_name']))\n",
        "            width = 0.25\n",
        "\n",
        "            plt.bar(x - width, results['aoi_v1_original'], width, label='V1:å®Œå…¨åˆæœŸç‰ˆ', alpha=0.8)\n",
        "            plt.bar(x, results['aoi_v2_allgrad'], width, label='V2:å…¨ä½“å‹¾é…', alpha=0.8)\n",
        "            plt.bar(x + width, results['aoi_v3_optimized'], width, label='V3:é¸æŠçš„å‹¾é…', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('ãƒ¬ã‚¤ãƒ¤ãƒ¼')\n",
        "            plt.ylabel('AOIå€¤')\n",
        "            plt.title('å„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®AOIå€¤æ¯”è¼ƒ')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # å·®åˆ†ã®æ¯”è¼ƒ\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.plot(x, results['diff_v1_v2'], 'o-', label='V1â†’V2', alpha=0.8)\n",
        "            plt.plot(x, results['diff_v2_v3'], 's-', label='V2â†’V3', alpha=0.8)\n",
        "            plt.plot(x, results['diff_v1_v3'], '^-', label='V1â†’V3', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('ãƒ¬ã‚¤ãƒ¤ãƒ¼')\n",
        "            plt.ylabel('å·®åˆ†')\n",
        "            plt.title('ãƒãƒ¼ã‚¸ãƒ§ãƒ³é–“ã®å·®åˆ†')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # ç›¸å¯¾çš„ãªå·®åˆ†ï¼ˆ%ï¼‰\n",
        "            plt.subplot(2, 2, 3)\n",
        "            rel_diff_v1_v2 = [(d/v1)*100 if v1 != 0 else 0 for d, v1 in zip(results['diff_v1_v2'], results['aoi_v1_original'])]\n",
        "            rel_diff_v2_v3 = [(d/v2)*100 if v2 != 0 else 0 for d, v2 in zip(results['diff_v2_v3'], results['aoi_v2_allgrad'])]\n",
        "            rel_diff_v1_v3 = [(d/v1)*100 if v1 != 0 else 0 for d, v1 in zip(results['diff_v1_v3'], results['aoi_v1_original'])]\n",
        "\n",
        "            plt.bar(x - width/2, rel_diff_v1_v2, width, label='V1â†’V2', alpha=0.8)\n",
        "            plt.bar(x + width/2, rel_diff_v1_v3, width, label='V1â†’V3', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('ãƒ¬ã‚¤ãƒ¤ãƒ¼')\n",
        "            plt.ylabel('ç›¸å¯¾å·®åˆ† (%)')\n",
        "            plt.title('V1ã‹ã‚‰ã®ç›¸å¯¾çš„ãªå¤‰åŒ–')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—\n",
        "            plt.subplot(2, 2, 4)\n",
        "            data = np.array([results['aoi_v1_original'],\n",
        "                            results['aoi_v2_allgrad'],\n",
        "                            results['aoi_v3_optimized']])\n",
        "            im = plt.imshow(data, aspect='auto', cmap='viridis')\n",
        "            plt.colorbar(im, label='AOIå€¤')\n",
        "            plt.yticks([0, 1, 2], ['V1:å®Œå…¨åˆæœŸç‰ˆ', 'V2:å…¨ä½“å‹¾é…', 'V3:é¸æŠçš„å‹¾é…'])\n",
        "            plt.xticks(range(len(results['layer_name'])),\n",
        "                      [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.title('AOIå€¤ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('gradcam_version_comparison.png', dpi=300, bbox_inches='tight')\n",
        "            print(\"\\næ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ 'gradcam_version_comparison.png' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"\\næœ‰åŠ¹ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    # çµæœã‚’DataFrameã¨ã—ã¦è¿”ã™\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ† ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š\n",
        "    os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "    # ãƒ‘ã‚¹ã®è¨­å®š\n",
        "    folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "    csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "    # è¨­å®š\n",
        "    threshold = 0.5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    input_size = (640, 640)\n",
        "    names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "    print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "    if device == \"cuda\":\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "    print(\"\\nãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    # æ¯”è¼ƒã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼\n",
        "    target_layers = [\n",
        "        \"model_17_cv3_conv\",\n",
        "        \"model_20_cv3_conv\",\n",
        "        \"model_23_cv3_conv\",\n",
        "        \"model_24_m_0\",\n",
        "        \"model_24_m_1\",\n",
        "        \"model_24_m_2\"\n",
        "    ]\n",
        "\n",
        "    # CSVã‹ã‚‰æœ€åˆã®ç”»åƒã‚’è‡ªå‹•é¸æŠ\n",
        "    print(\"\\nCSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç”»åƒã‚’é¸æŠä¸­...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    test_image_path = None\n",
        "\n",
        "    for idx in range(min(10, len(df))):\n",
        "        row = df.iloc[idx]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            continue\n",
        "\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                test_image_path = test_path\n",
        "                break\n",
        "\n",
        "        if test_image_path:\n",
        "            break\n",
        "\n",
        "    if test_image_path:\n",
        "        print(f\"ãƒ†ã‚¹ãƒˆç”»åƒ: {test_image_path}\")\n",
        "\n",
        "        # 3ã¤ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ¯”è¼ƒå®Ÿè¡Œ\n",
        "        results_df = compare_all_versions(test_image_path, model, target_layers, threshold)\n",
        "\n",
        "        if results_df is not None and not results_df.empty:\n",
        "            results_df.to_csv('gradcam_version_comparison_results.csv', index=False)\n",
        "            print(\"\\nè©³ç´°ãªçµæœã‚’ 'gradcam_version_comparison_results.csv' ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "            # çµæœã®è¦ç´„ã‚’è¡¨ç¤º\n",
        "            print(\"\\n=== çµæœã®è¦ç´„ ===\")\n",
        "            print(results_df)\n",
        "\n",
        "            # å„è¦ç´ ã®å½±éŸ¿ã‚’åˆ†æ\n",
        "            print(\"\\n=== å„è¦ç´ ã®å½±éŸ¿åˆ†æ ===\")\n",
        "            print(\"1. retain_graph=True â†’ False ã®å½±éŸ¿ï¼ˆV1â†’V2ï¼‰:\")\n",
        "            print(f\"   å¹³å‡å·®åˆ†: {results_df['diff_v1_v2'].mean():+.6f}\")\n",
        "            print(\"\\n2. å…¨ä½“å‹¾é… â†’ é¸æŠçš„å‹¾é… ã®å½±éŸ¿ï¼ˆV2â†’V3ï¼‰:\")\n",
        "            print(f\"   å¹³å‡å·®åˆ†: {results_df['diff_v2_v3'].mean():+.6f}\")\n",
        "            print(\"\\n3. å…¨ä½“çš„ãªæœ€é©åŒ–ã®å½±éŸ¿ï¼ˆV1â†’V3ï¼‰:\")\n",
        "            print(f\"   å¹³å‡å·®åˆ†: {results_df['diff_v1_v3'].mean():+.6f}\")\n",
        "        else:\n",
        "            print(\"\\næ¯”è¼ƒçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\nå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "csH4cVc840Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ä¸Šè¨˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãŸã‚ã®ä¿å­˜ç”¨csvãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ**"
      ],
      "metadata": {
        "id": "y52nNVm4SrDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7ykhgI5l3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338fb2c4-a6a2-4bdb-875e-fdf60a580472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è­¦å‘Š: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ (y/n): n\n",
            "å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\n",
            "\n",
            "ä½œæˆã•ã‚ŒãŸDataFrame:\n",
            "                     AOI_0.5_layermodel_17_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_20_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_23_cv3_conv AOI_0.5_layer24_m_0  \\\n",
            "image_basename                                                            \n",
            "[apac]FKS_130_R_slit                           None                None   \n",
            "[apac]FKS_238_L_slit                           None                None   \n",
            "[apac]FKS_382_L_slit                           None                None   \n",
            "[apac]TKB_001_R_slit                           None                None   \n",
            "[apac]fko0074                                  None                None   \n",
            "\n",
            "                     AOI_0.5_layer24_m_1 AOI_0.5_layer24_m_2  \n",
            "image_basename                                                \n",
            "[apac]FKS_130_R_slit                None                None  \n",
            "[apac]FKS_238_L_slit                None                None  \n",
            "[apac]FKS_382_L_slit                None                None  \n",
            "[apac]TKB_001_R_slit                None                None  \n",
            "[apac]fko0074                       None                None  \n",
            "\n",
            "ç·ç”»åƒæ•°: 1039\n",
            "\n",
            "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "###ã“ã‚Œã‚’æŠ¼ã™ã¨csvãŒæ›´æ–°ã•ã‚Œã¦ã—ã¾ã†ã®ã§æ³¨æ„ï¼ï¼ï¼\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "output_file_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "if os.path.exists(output_file_path):\n",
        "    print(f\"è­¦å‘Š: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: {output_file_path}\")\n",
        "    response = input(\"ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ (y/n): \").lower()\n",
        "\n",
        "    if response != 'y':\n",
        "        print(\"å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚\")\n",
        "\n",
        "# ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_dir}\")\n",
        "    exit()\n",
        "\n",
        "# ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆä¸€èˆ¬çš„ãªç”»åƒæ‹¡å¼µå­ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
        "image_files = []\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "try:\n",
        "    for file in os.listdir(image_dir):\n",
        "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
        "            # basenameã‚’å–å¾—ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰\n",
        "            basename = os.path.splitext(file)[0]\n",
        "            image_files.append(basename)\n",
        "except Exception as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å‡¦ç†\n",
        "if not image_files:\n",
        "    print(f\"è­¦å‘Š: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "    print(f\"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {image_dir}\")\n",
        "    print(f\"å¯¾è±¡æ‹¡å¼µå­: {', '.join(image_extensions)}\")\n",
        "    exit()\n",
        "\n",
        "# ã‚½ãƒ¼ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
        "image_files.sort()\n",
        "\n",
        "# DataFrameã‚’ä½œæˆï¼ˆbasenameã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ï¼‰\n",
        "df = pd.DataFrame(index=image_files)\n",
        "df.index.name = 'image_basename'  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’è¨­å®š\n",
        "\n",
        "# ä¾‹ã¨ã—ã¦thresholdã®å€¤ã‚’è¨­å®š\n",
        "threshold = 0.5\n",
        "\n",
        "# æŒ‡å®šã•ã‚ŒãŸãƒ¬ã‚¤ãƒ¤ãƒ¼ (originalã¯convã§ãªãact??)\n",
        "layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv', '24_m_0', '24_m_1', '24_m_2']\n",
        "\n",
        "# å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å¯¾ã—ã¦æ–°ã—ã„åˆ—ã‚’ä½œæˆ\n",
        "for layer in layers:\n",
        "    df[f'AOI_{threshold}_layer{layer}'] = None  # åˆæœŸå€¤ã‚’Noneã«è¨­å®š\n",
        "\n",
        "# DataFrameã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤º\n",
        "print(\"\\nä½œæˆã•ã‚ŒãŸDataFrame:\")\n",
        "print(df.head())\n",
        "print(f\"\\nç·ç”»åƒæ•°: {len(df)}\")\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
        "try:\n",
        "    df.to_csv(output_file_path, index=True)\n",
        "    print(f\"\\nCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tT3myDD_dWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Expert's annotation**"
      ],
      "metadata": {
        "id": "gIdR9H0J2YjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "metadata": {
        "id": "yPKZg9H8RXow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbbd809-375b-42fb-97c4-0c851d51e8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: /gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Imagesã¨/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Masksã®ä¸­ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®basename(æ‹¡å¼µå­ã¯é•ã†ã“ã¨ã‚ã‚Šï¼‰ã‚’æ¯”è¼ƒï¼\n",
        "\n",
        "import os\n",
        "\n",
        "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "mask_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Masks\"\n",
        "\n",
        "# ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µå­ãªã—ã®basenameï¼‰\n",
        "image_files = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "\n",
        "# ãƒã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µå­ãªã—ã®basenameï¼‰\n",
        "mask_files = [os.path.splitext(f)[0] for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "\n",
        "# ä¸¡æ–¹ã®ãƒªã‚¹ãƒˆã«å­˜åœ¨ã™ã‚‹basenameã‚’ç‰¹å®š\n",
        "common_files = sorted(list(set(image_files) & set(mask_files)))\n",
        "\n",
        "# ã©ã¡ã‚‰ã‹ä¸€æ–¹ã«ã—ã‹å­˜åœ¨ã—ãªã„basenameã‚’ç‰¹å®š\n",
        "image_only = sorted(list(set(image_files) - set(mask_files)))\n",
        "mask_only = sorted(list(set(mask_files) - set(image_files)))\n",
        "\n",
        "print(f\"ç”»åƒã®ç·æ•°: {len(image_files)}\")\n",
        "print(f\"ãƒã‚¹ã‚¯ã®ç·æ•°: {len(mask_files)}\")\n",
        "print(f\"ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(common_files)}\")\n",
        "print(f\"ç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(image_only)}\")\n",
        "print(f\"ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(mask_only)}\")\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
        "print(\"\\nä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ« (å…ˆé ­10ä»¶):\")\n",
        "for f in common_files[:10]:\n",
        "    print(f)\n",
        "\n",
        "print(\"\\nç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
        "for f in image_only[0:]:\n",
        "    print(f)\n",
        "\n",
        "print(\"\\nãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
        "for f in mask_only[0:]:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "U6ACrQR_RXrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390ca302-cfb7-4c58-974b-2fb59e1a1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ç”»åƒã®ç·æ•°: 1039\n",
            "ãƒã‚¹ã‚¯ã®ç·æ•°: 1007\n",
            "ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 996\n",
            "ç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 43\n",
            "ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 10\n",
            "\n",
            "ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ« (å…ˆé ­10ä»¶):\n",
            "[apac]FKS_130_R_slit\n",
            "[apac]FKS_238_L_slit\n",
            "[apac]FKS_382_L_slit\n",
            "[apac]TKB_001_R_slit\n",
            "[apac]fko0074\n",
            "[apac]kei0903_1\n",
            "[apac]kei0934\n",
            "[apac]ttr_0366\n",
            "[apac]ttr_0369_01\n",
            "[apac]ttr_0378\n",
            "\n",
            "ç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]ãƒ¢ãƒ¼ãƒ¬ãƒ³_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]ç´°èŒ_KNZ5048\n",
            "[infection]ç´°èŒ_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[scar]myt 0024\n",
            "[scar]myt 0035\n",
            "[scar]myt 0036\n",
            "[scar]myt 0040\n",
            "[scar]myt 0049\n",
            "[scar]myt 0052\n",
            "[scar]myt 0073\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]ç¿¼çŠ¶ç‰‡_wky7047\n",
            "\n",
            "ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:\n",
            "[infection]ç´°èŒ_fko0032_01-1\n",
            "[infection]ç´°èŒ_fko0032_01-2\n",
            "[scar]FKS_258_L_slit-1\n",
            "[scar]myt_0024\n",
            "[scar]myt_0035\n",
            "[scar]myt_0036\n",
            "[scar]myt_0040\n",
            "[scar]myt_0049\n",
            "[scar]myt_0052\n",
            "[scar]myt_0073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ===== è¨­å®š =====\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Masks\"\n",
        "dry_run   = True        # True: ç¢ºèªã ã‘, False: å®Ÿéš›ã«ãƒªãƒãƒ¼ãƒ \n",
        "# ===============\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"æ¯”è¼ƒç”¨ã‚­ãƒ¼ã‚’ç”Ÿæˆï¼ˆå¤§å°åŒºåˆ¥ã›ãš, ç©ºç™½/ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢/ãƒã‚¤ãƒ•ãƒ³ã‚’é™¤å¤–, æœ«å°¾ã® -1/-2 ã‚‚é™¤å¤–ï¼‰\"\"\"\n",
        "    # æœ«å°¾ã® -æ•°å­—, _æ•°å­— ã‚’å‰Šé™¤\n",
        "    name = re.sub(r'[-_]\\d+$', '', name)\n",
        "    # ç©ºç™½, ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢, ãƒã‚¤ãƒ•ãƒ³ ã‚’æ¶ˆã—ã¦å°æ–‡å­—åŒ–\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()\n",
        "\n",
        "# Images å´: æ­£è¦åŒ– â†’ ã‚ªãƒªã‚¸ãƒŠãƒ«å\n",
        "image_key2name = {normalize(os.path.splitext(f)[0]): os.path.splitext(f)[0]\n",
        "                  for f in os.listdir(image_dir)\n",
        "                  if os.path.isfile(os.path.join(image_dir, f))}\n",
        "\n",
        "# Masks å´: åŒã˜ãæ­£è¦åŒ– â†’ [ (basename, æ‹¡å¼µå­) , ... ]\n",
        "mask_key2names = defaultdict(list)\n",
        "for f in os.listdir(mask_dir):\n",
        "    if os.path.isfile(os.path.join(mask_dir, f)):\n",
        "        base, ext = os.path.splitext(f)\n",
        "        mask_key2names[normalize(base)].append((base, ext))\n",
        "\n",
        "# â‘  1:1 ã§å¯¾å¿œãŒç¢ºå®šã™ã‚‹ã‚‚ã®ã ã‘è‡ªå‹•ãƒªãƒãƒ¼ãƒ å€™è£œã«\n",
        "rename_plan = []\n",
        "ambiguous   = []      # åŒã˜ã‚­ãƒ¼ã§ image ã¨ mask ãŒè¤‡æ•°ã‚ã‚‹ç­‰\n",
        "no_match    = []      # mask å´ã«å¯¾å¿œã™ã‚‹ image ãŒç„¡ã„\n",
        "\n",
        "for key, mask_list in mask_key2names.items():\n",
        "    if key in image_key2name and len(mask_list) == 1:\n",
        "        img_base = image_key2name[key]\n",
        "        mask_base, ext = mask_list[0]\n",
        "        if img_base != mask_base:  # ã™ã§ã«ä¸€è‡´ã—ã¦ã„ãªã‘ã‚Œã°\n",
        "            src = os.path.join(mask_dir, mask_base + ext)\n",
        "            dst = os.path.join(mask_dir, img_base + ext)\n",
        "            rename_plan.append((src, dst))\n",
        "    elif key in image_key2name:\n",
        "        ambiguous.append(mask_list)   # åŒã‚­ãƒ¼ã§ mask ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¤‡æ•°\n",
        "    else:\n",
        "        no_match.extend(mask_list)    # ãã‚‚ãã‚‚ image å´ã«ã‚­ãƒ¼ç„¡ã—\n",
        "\n",
        "# ---- çµæœã‚’ç¢ºèª ----\n",
        "print(f\"è‡ªå‹•ãƒªãƒãƒ¼ãƒ å€™è£œ: {len(rename_plan)} ä»¶\")\n",
        "for src, dst in rename_plan[:20]:\n",
        "    print(f\"  {os.path.basename(src)}  â†’  {os.path.basename(dst)}\")\n",
        "\n",
        "print(f\"\\nã‚ã„ã¾ã„ (æ‰‹å‹•ç¢ºèªæ¨å¥¨): {len(ambiguous)} ä»¶\")\n",
        "print(f\"ãƒãƒƒãƒã—ãªã„ mask:      {len(no_match)} ä»¶\")\n",
        "\n",
        "# ---- å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º ----\n",
        "if not dry_run:\n",
        "    for src, dst in rename_plan:\n",
        "        os.rename(src, dst)\n",
        "    print(f\"\\n{len(rename_plan)} ä»¶ãƒªãƒãƒ¼ãƒ å®Œäº†\")\n",
        "else:\n",
        "    print(\"\\nâ€» dry_run=True ã®ãŸã‚ãƒªãƒãƒ¼ãƒ ã›ãšçµ‚äº†ã€‚ãƒã‚§ãƒƒã‚¯å¾Œ dry_run=False ã«ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n"
      ],
      "metadata": {
        "id": "20O4wz0A7goa",
        "outputId": "24dfd334-6e1c-4d0c-e522-786a9ad9ca47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è‡ªå‹•ãƒªãƒãƒ¼ãƒ å€™è£œ: 0 ä»¶\n",
            "\n",
            "ã‚ã„ã¾ã„ (æ‰‹å‹•ç¢ºèªæ¨å¥¨): 5 ä»¶\n",
            "ãƒãƒƒãƒã—ãªã„ mask:      9 ä»¶\n",
            "\n",
            "â€» dry_run=True ã®ãŸã‚ãƒªãƒãƒ¼ãƒ ã›ãšçµ‚äº†ã€‚ãƒã‚§ãƒƒã‚¯å¾Œ dry_run=False ã«ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ====== è¨­å®š ======\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Masks\"\n",
        "dry_run   = True        # ç¢ºèªã ã‘ãªã‚‰ Trueã€å®Ÿéš›ã«ãƒªãƒãƒ¼ãƒ ã™ã‚‹ãªã‚‰ False\n",
        "# ===================\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"æ¯”è¼ƒã‚­ãƒ¼ã‚’ç”Ÿæˆ\n",
        "       ãƒ»æœ«å°¾ã® -1/_2 ãªã© 1â€“2 æ¡ç•ªå·ã ã‘å‰Šé™¤\n",
        "       ãƒ»ç©ºç™½ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ãƒãƒ¼ãƒ»ãƒã‚¤ãƒ•ãƒ³é™¤å»\n",
        "       ãƒ»å°æ–‡å­—åŒ–\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[-_]\\d{1,2}$', '', name)\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()\n",
        "\n",
        "# â”€â”€ ç”»åƒå´ â”€â”€\n",
        "img_bases = [os.path.splitext(f)[0] for f in os.listdir(image_dir)\n",
        "             if os.path.isfile(os.path.join(image_dir, f))]\n",
        "img_key2base = {normalize(b): b for b in img_bases}\n",
        "\n",
        "# â”€â”€ ãƒã‚¹ã‚¯å´ â”€â”€\n",
        "mask_files = [f for f in os.listdir(mask_dir)\n",
        "              if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "mask_key2info = defaultdict(list)  # key â†’ [(base, ext, full_path), â€¦]\n",
        "for f in mask_files:\n",
        "    base, ext = os.path.splitext(f)\n",
        "    key = normalize(base)\n",
        "    mask_key2info[key].append((base, ext, os.path.join(mask_dir, f)))\n",
        "\n",
        "# â”€â”€ ãƒªãƒãƒ¼ãƒ è¨ˆç”» â”€â”€\n",
        "rename_plan, ambiguous, no_match = [], [], []\n",
        "for key, masks in mask_key2info.items():\n",
        "    if key in img_key2base and len(masks) == 1:\n",
        "        img_base = img_key2base[key]\n",
        "        m_base, ext, src = masks[0]\n",
        "        if m_base != img_base:\n",
        "            dst = os.path.join(mask_dir, img_base + ext)\n",
        "            rename_plan.append((src, dst))\n",
        "    elif key in img_key2base:\n",
        "        ambiguous.extend(masks)\n",
        "    else:\n",
        "        no_match.extend(masks)\n",
        "\n",
        "# â”€â”€ dry-run å‡ºåŠ› â”€â”€\n",
        "print(f\"ãƒªãƒãƒ¼ãƒ äºˆå®š: {len(rename_plan)} ä»¶\")\n",
        "for s, d in rename_plan[:20]:\n",
        "    print(\"  \", os.path.basename(s), \"â†’\", os.path.basename(d))\n",
        "\n",
        "print(f\"\\næ›–æ˜§ (æ‰‹å‹•ç¢ºèª): {len(ambiguous)} ä»¶\")\n",
        "print(f\"å¯¾å¿œã™ã‚‹ç”»åƒãªã—: {len(no_match)} ä»¶\")\n",
        "\n",
        "# â”€â”€ å®Ÿè¡Œ â”€â”€\n",
        "if not dry_run:\n",
        "    for src, dst in rename_plan:\n",
        "        try:\n",
        "            os.rename(src, dst)\n",
        "        except FileExistsError:\n",
        "            print(\"  * æ—¢ã«å­˜åœ¨: \", os.path.basename(dst))\n",
        "    print(\"\\nãƒªãƒãƒ¼ãƒ å®Œäº†\")\n",
        "\n",
        "# â”€â”€ ãƒªãƒãƒ¼ãƒ å¾Œã®å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯ â”€â”€\n",
        "# ï¼ˆdry_run ã§ã‚‚ç¾çŠ¶ã‚’ç¢ºèªã§ãã‚‹ï¼‰\n",
        "img_keys  = set(img_key2base)\n",
        "mask_keys = set(normalize(os.path.splitext(f)[0]) for f in\n",
        "                os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f)))\n",
        "\n",
        "unmatched_imgs  = sorted(img_key2base[k] for k in img_keys - mask_keys)\n",
        "unmatched_masks = sorted(os.path.splitext(f)[0] for f in mask_files\n",
        "                         if normalize(os.path.splitext(f)[0]) not in img_keys)\n",
        "\n",
        "print(\"\\n====== æœ€çµ‚ã‚µãƒãƒª ======\")\n",
        "print(\"ç”»åƒã«ã®ã¿å­˜åœ¨:\", len(unmatched_imgs))\n",
        "print(\"ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨:\", len(unmatched_masks))\n",
        "if unmatched_imgs:\n",
        "    print(\"\\nâ–¼ç”»åƒã®ã¿\")\n",
        "    for b in unmatched_imgs:\n",
        "        print(b)\n",
        "if unmatched_masks:\n",
        "    print(\"\\nâ–¼ãƒã‚¹ã‚¯ã®ã¿\")\n",
        "    for b in unmatched_masks:\n",
        "        print(b)\n"
      ],
      "metadata": {
        "id": "LOkhhHotEFJo",
        "outputId": "6860976c-1b4b-4a3e-a7c0-c213f756d55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒªãƒãƒ¼ãƒ äºˆå®š: 7 ä»¶\n",
            "   [scar]myt_0073.png â†’ [scar]myt 0073.png\n",
            "   [scar]myt_0035.png â†’ [scar]myt 0035.png\n",
            "   [scar]myt_0036.png â†’ [scar]myt 0036.png\n",
            "   [scar]myt_0040.png â†’ [scar]myt 0040.png\n",
            "   [scar]myt_0049.png â†’ [scar]myt 0049.png\n",
            "   [scar]myt_0052.png â†’ [scar]myt 0052.png\n",
            "   [scar]myt_0024.png â†’ [scar]myt 0024.png\n",
            "\n",
            "æ›–æ˜§ (æ‰‹å‹•ç¢ºèª): 4 ä»¶\n",
            "å¯¾å¿œã™ã‚‹ç”»åƒãªã—: 2 ä»¶\n",
            "\n",
            "====== æœ€çµ‚ã‚µãƒãƒª ======\n",
            "ç”»åƒã«ã®ã¿å­˜åœ¨: 36\n",
            "ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨: 2\n",
            "\n",
            "â–¼ç”»åƒã®ã¿\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]ãƒ¢ãƒ¼ãƒ¬ãƒ³_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]ç´°èŒ_KNZ5048\n",
            "[infection]ç´°èŒ_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]ç¿¼çŠ¶ç‰‡_wky7047\n",
            "\n",
            "â–¼ãƒã‚¹ã‚¯ã®ã¿\n",
            "[infection]ç´°èŒ_fko0032_01-1\n",
            "[infection]ç´°èŒ_fko0032_01-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# ===== ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š =====\n",
        "image_dir = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Masks\"\n",
        "# ==========================\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"æ¯”è¼ƒç”¨ã‚­ãƒ¼ã‚’ç”Ÿæˆ\"\"\"\n",
        "    name = re.sub(r'[-_]\\d+$', '', name)       # æœ«å°¾ -1/-2/_1 ãªã©ã‚’å‰Šé™¤\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()  # ç©ºç™½ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é™¤å» â†’ å°æ–‡å­—\n",
        "\n",
        "# â”€â”€ ç”»åƒå´ â”€â”€\n",
        "image_bases = [os.path.splitext(f)[0] for f in os.listdir(image_dir)\n",
        "               if os.path.isfile(os.path.join(image_dir, f))]\n",
        "image_norm2orig = {normalize(b): b for b in image_bases}\n",
        "\n",
        "# â”€â”€ ãƒã‚¹ã‚¯å´ â”€â”€\n",
        "mask_bases = [os.path.splitext(f)[0] for f in os.listdir(mask_dir)\n",
        "              if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "mask_norm2orig = {normalize(b): b for b in mask_bases}\n",
        "\n",
        "# â”€â”€ ä¸€è‡´åˆ¤å®š â”€â”€\n",
        "image_keys = set(image_norm2orig)\n",
        "mask_keys  = set(mask_norm2orig)\n",
        "\n",
        "# ã¾ã ä¸€è‡´ã—ãªã„ã‚‚ã®\n",
        "image_only_keys = image_keys - mask_keys\n",
        "mask_only_keys  = mask_keys  - image_keys\n",
        "\n",
        "unmatched_images = sorted([image_norm2orig[k] for k in image_only_keys])\n",
        "unmatched_masks  = sorted([mask_norm2orig[k]  for k in mask_only_keys])\n",
        "\n",
        "# â”€â”€ çµæœè¡¨ç¤º â”€â”€\n",
        "print(f\"å®Œå…¨ä¸€è‡´ã—ãªã„ç”»åƒ   : {len(unmatched_images)} ä»¶\")\n",
        "print(f\"å®Œå…¨ä¸€è‡´ã—ãªã„ãƒã‚¹ã‚¯ : {len(unmatched_masks)} ä»¶\")\n",
        "\n",
        "print(\"\\nâ–¼ ç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ basename\")\n",
        "for b in unmatched_images:\n",
        "    print(b)\n",
        "\n",
        "print(\"\\nâ–¼ ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ basename\")\n",
        "for b in unmatched_masks:\n",
        "    print(b)\n"
      ],
      "metadata": {
        "id": "sPIdNF8K7-tf",
        "outputId": "124ee30f-b9db-49ca-ab1c-e064e9ab572c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å®Œå…¨ä¸€è‡´ã—ãªã„ç”»åƒ   : 43 ä»¶\n",
            "å®Œå…¨ä¸€è‡´ã—ãªã„ãƒã‚¹ã‚¯ : 2 ä»¶\n",
            "\n",
            "â–¼ ç”»åƒã«ã®ã¿å­˜åœ¨ã™ã‚‹ basename\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]ãƒ¢ãƒ¼ãƒ¬ãƒ³_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]ç´°èŒ_KNZ5048\n",
            "[infection]ç´°èŒ_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[scar]myt 0024\n",
            "[scar]myt 0035\n",
            "[scar]myt 0036\n",
            "[scar]myt 0040\n",
            "[scar]myt 0049\n",
            "[scar]myt 0052\n",
            "[scar]myt 0073\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]ç¿¼çŠ¶ç‰‡_wky7047\n",
            "\n",
            "â–¼ ãƒã‚¹ã‚¯ã«ã®ã¿å­˜åœ¨ã™ã‚‹ basename\n",
            "[infection]ç´°èŒ_fko0032_01-2\n",
            "[scar]myt_0024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dzfX7nR2RaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOe-1hxV2Rby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FekHTfIU2Rdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7sGrpBQ92Rf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVBBhZUM2RiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLvwguGg2RkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colabç’°å¢ƒã§cv2_imshowã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã«ã‚³ãƒ¡ãƒ³ãƒˆè§£é™¤\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "\n",
        "# YOLOv5ã®å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - attempt_loadã®å ´æ‰€ã‚’ä¿®æ­£\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # ä¿®æ­£: models.experimentalã‹ã‚‰\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ã®YOLOV5TorchObjectDetectorã‚¯ãƒ©ã‚¹\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === å¤‰æ›´ç‚¹: ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®requires_grad_(True)ã‚’æ˜ç¤ºçš„ã«è¨­å®š ===\n",
        "        # Grad-CAMã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        # ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã¯ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã™ã‚‹ãŸã‚ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ\n",
        "        # img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        # self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === å¤‰æ›´ç‚¹: æ¨è«–éƒ¨åˆ†ã®with torch.no_grad()ã‚’å‰Šé™¤ ===\n",
        "        # Grad-CAMã®ãŸã‚ã«å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # ä»¥ä¸‹ã®å‡¦ç†ã¯CPUã§è¡Œã†ã“ã¨ã§GPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                # detã‚’CPUã«ç§»å‹•\n",
        "                det_cpu = det.cpu()\n",
        "                for *xyxy, conf, cls in det_cpu:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMã‚¯ãƒ©ã‚¹\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        # === å¤‰æ›´ç‚¹: target_layerã®requires_gradã‚’Trueã«è¨­å®š ===\n",
        "        # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®requires_grad_(True)ã‚’å‰Šé™¤ã—ãŸãŸã‚ã€ã“ã“ã§Grad-CAMã«å¿…è¦ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿Trueã«ã™ã‚‹\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # åˆæœŸåŒ–æ™‚ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å‰Šé™¤\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã¯å‹¾é…è¨ˆç®—ãŒå¿…è¦ãªãŸã‚ã€no_grad()ã‚’é©ç”¨ã—ãªã„\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layeré–¢æ•°\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoié–¢æ•°\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0\n",
        "\n",
        "   for mask in masks:\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255\n",
        "\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "# calculate_aoié–¢æ•°ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ä»˜ãï¼‰\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # AOIã‚«ãƒ©ãƒ ã®å®šç¾©\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ \n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # å‡¦ç†æ¸ˆã¿ç”»åƒã®åˆ¤å®šé–¢æ•°\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # å‡¦ç†å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # æœªå‡¦ç†ã®ç”»åƒã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"æœ€åˆã®æœªå‡¦ç†ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # å‡¦ç†å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’æŒ‡å®šã—ãŸå ´åˆ\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedãŒTrueã®å ´åˆã€ç¯„å›²å†…ã§ã‚‚å‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"å‡¦ç†å¯¾è±¡ç”»åƒæ•°: {len(target_indices)}\")\n",
        "\n",
        "    # é€²æ—çŠ¶æ³ã®è¡¨ç¤º\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    # å®šæœŸçš„ãªä¿å­˜ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼\n",
        "    save_interval = 10  # 10ç”»åƒã”ã¨ã«ä¿å­˜\n",
        "\n",
        "    # ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "                # å®šæœŸçš„ãªä¸­é–“ä¿å­˜\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"ä¸­é–“ä¿å­˜å®Œäº†: {i + 1}/{len(target_indices)} ç”»åƒå‡¦ç†æ¸ˆã¿\")\n",
        "\n",
        "                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "                    if device == \"cuda\":\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    # æœ€çµ‚ä¿å­˜\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼:\")\n",
        "    print(f\"- å‡¦ç†ã—ãŸç”»åƒ: {processed_count}å€‹\")\n",
        "    print(f\"- ã‚¹ã‚­ãƒƒãƒ—ã—ãŸç”»åƒ: {skipped_count}å€‹\")\n",
        "    print(f\"- è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"- ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # is_processedé–¢æ•°ã‚’å®šç¾©ï¼ˆcalculate_aoié–¢æ•°å¤–ã§ã‚‚ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰\n",
        "    def is_processed(row):\n",
        "        \"\"\"ã™ã¹ã¦ã®AOIã‚«ãƒ©ãƒ ã«å€¤ãŒå…¥ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # æœªå‡¦ç†ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"ã™ã¹ã¦ã®ç”»åƒãŒå‡¦ç†æ¸ˆã¿ã§ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"æœªå‡¦ç†ç”»åƒæ•°: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nãƒãƒƒãƒ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {batch_indices}\")\n",
        "\n",
        "            # ãƒãƒƒãƒå†…ã®ç”»åƒã‚’ã¾ã¨ã‚ã¦å‡¦ç†\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True)\n",
        "\n",
        "            # å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"ãƒãƒƒãƒ {i//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "            # ãƒãƒƒãƒãŒå®Œäº†ã—ãŸã‚‰ã€å†åº¦æœªå‡¦ç†ç”»åƒã‚’ç¢ºèªï¼ˆä¸­æ–­ã•ã‚ŒãŸå ´åˆã®å¯¾ç­–ï¼‰\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"æ®‹ã‚Šæœªå‡¦ç†ç”»åƒæ•°: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "    mode = input(\"å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:\\n1: è‡ªå‹•ï¼ˆæœªå‡¦ç†ç”»åƒã‚’æ¤œå‡ºï¼‰\\n2: æ‰‹å‹•ï¼ˆç¯„å›²ã‚’æŒ‡å®šï¼‰\\n3: å¼·åˆ¶å†å‡¦ç†ï¼ˆç¯„å›²ã‚’æŒ‡å®šã€å‡¦ç†æ¸ˆã¿ã‚‚å†å‡¦ç†ï¼‰\\né¸æŠ (1/2/3): \")\n",
        "\n",
        "    if mode == \"1\":\n",
        "        # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰\n",
        "        print(\"\\næœªå‡¦ç†ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True)\n",
        "\n",
        "    elif mode == \"2\":\n",
        "        # æ‰‹å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n",
        "        start_index = int(input(\"é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        end_index = int(input(\"çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {start_index} ã‹ã‚‰ {end_index-1} ã¾ã§å‡¦ç†ã—ã¾ã™ï¼ˆå‡¦ç†æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=True)\n",
        "\n",
        "    elif mode == \"3\":\n",
        "        # å¼·åˆ¶å†å‡¦ç†ãƒ¢ãƒ¼ãƒ‰\n",
        "        start_index = int(input(\"é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        end_index = int(input(\"çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \"))\n",
        "        print(f\"\\nã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {start_index} ã‹ã‚‰ {end_index-1} ã¾ã§å¼·åˆ¶çš„ã«å†å‡¦ç†ã—ã¾ã™...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=False)\n",
        "\n",
        "    else:\n",
        "        print(\"ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "aHMNNdM1dWJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbMtDZ6MdWNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXxnkbradWPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "\n",
        "# YOLOv5ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# ã“ã‚Œã¾ã§ã«ç™»å ´ã—ãŸã‚¯ãƒ©ã‚¹ã‚’ã™ã¹ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyã®å€¤ã‚’ä¿®æ­£\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS3iA_8QKWZC"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrjCYpUtJlyR"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "##################\n",
        "## 6-layerã§è§£æ##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
        "\n",
        "   for mask in masks:\n",
        "       # ãƒã‚¹ã‚¯ã®å‰å‡¦ç†\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # é–¾å€¤ã‚’è¶…ãˆã‚‹éƒ¨åˆ†ã‚’äºŒå€¤åŒ–\n",
        "\n",
        "       # bboxã®ç¯„å›²å†…ã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’å–å¾—\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # é–¾å€¤ã‚’è¶…ãˆã‚‹å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI ã‚’è¨ˆç®—\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨­å®š\n",
        "    all_layers = [\n",
        "        ('model_17_cv3_act', f'AOI_{threshold}_layermodel_17_cv3_act'),\n",
        "        ('model_20_cv3_act', f'AOI_{threshold}_layermodel_20_cv3_act'),\n",
        "        ('model_23_cv3_act', f'AOI_{threshold}_layermodel_23_cv3_act'),\n",
        "        ('model_24_m_0', f'AOI_{threshold}_layer24_m_0'),\n",
        "        ('model_24_m_1', f'AOI_{threshold}_layer24_m_1'),\n",
        "        ('model_24_m_2', f'AOI_{threshold}_layer24_m_2')\n",
        "    ]\n",
        "\n",
        "    # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
        "    for layer_name, col_name in all_layers:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸç”»åƒã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row['image_basename']\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # æ‹¡å¼µå­ã‚’è©¦ã™\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '']:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‡¦ç†\n",
        "                for i, (layer_name, col_name) in enumerate(all_layers):\n",
        "                    if i < len(saliency_methods):\n",
        "                        try:\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_methods[i](torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "\n",
        "                        except Exception as e:\n",
        "                            error_count += 1\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # é€²æ—è¡¨ç¤º\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†: {processed_count}å€‹ã®ç”»åƒã‚’å‡¦ç†\")\n",
        "    print(f\"è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨saliency_methodã®å®šç¾©\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®target_layersï¼ˆconvã«ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼‰\n",
        "target_layers = [\n",
        "    'model_17_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_20_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_23_cv3_conv',  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    'model_24_m_0',\n",
        "    'model_24_m_1',\n",
        "    'model_24_m_2'\n",
        "]\n",
        "\n",
        "# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "USE_BATCH_MODE = False  # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
        "BATCH_SIZE = 20       # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’ç¢ºèª\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"ç·ç”»åƒæ•°: {total_images}\")\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®saliency_methods ã‚’ä½œæˆ\n",
        "    print(\"\\nGradCAMãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆæœŸåŒ–ä¸­...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  ã‚¨ãƒ©ãƒ¼: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\nåˆæœŸåŒ–å®Œäº†: {len(saliency_methods)}å€‹ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\")\n",
        "\n",
        "    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"ç”»åƒ {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†ç¯„å›²ã®è¨­å®š\n",
        "    start_index = 0\n",
        "    end_index = 5  # Noneã§å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®saliency_methods ã‚’ä½œæˆ\n",
        "    print(\"GradCAMãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆæœŸåŒ–ä¸­...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  ã‚¨ãƒ©ãƒ¼: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\nåˆæœŸåŒ–å®Œäº†: {len(saliency_methods)}å€‹ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼\")\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®å€‹åˆ¥å‡¦ç†ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã®ä»£æ›¿æ¡ˆï¼‰\n",
        "\"\"\"\n",
        "print(\"=== ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®å€‹åˆ¥å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ===\")\n",
        "for layer_idx, layer_name in enumerate(target_layers):\n",
        "    print(f\"\\n--- ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_idx+1}/6: {layer_name} ---\")\n",
        "\n",
        "    try:\n",
        "        # å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®GradCAMã‚’ä½œæˆ\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                       img_size=input_size, method=\"gradcampp\")\n",
        "\n",
        "        # ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã§å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model,\n",
        "                     [saliency_method], start_index=0, end_index=None)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        del saliency_method\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ãƒ¬ã‚¤ãƒ¤ãƒ¼ {layer_name} ã§ã‚¨ãƒ©ãƒ¼: {str(e)}\")\n",
        "        continue\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ãƒ¡ãƒ¢ãƒªç¯€ç´„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ ###\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFã‚’è¨­å®š\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # åˆæœŸåŒ–æ™‚ã®ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã€ãƒ€ãƒŸãƒ¼ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’å‰Šé™¤\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapã®è¨ˆç®—ã¯å‹¾é…ä¸è¦ãªã®ã§no_grad()ã‚’é©ç”¨\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«epsilonã‚’è¿½åŠ \n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
        "\n",
        "   for mask in masks:\n",
        "       # ãƒã‚¹ã‚¯ã®å‰å‡¦ç†\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       # RuntimeWarning: invalid value encountered in cast å¯¾ç­–\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # é–¾å€¤ã‚’è¶…ãˆã‚‹éƒ¨åˆ†ã‚’äºŒå€¤åŒ–\n",
        "\n",
        "       # bboxã®ç¯„å›²å†…ã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã‚’å–å¾—\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # é–¾å€¤ã‚’è¶…ãˆã‚‹å…±é€šéƒ¨åˆ†ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox ã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI ã‚’è¨ˆç®—\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨­å®š\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # å®Ÿéš›ã«å‡¦ç†ã•ã‚ŒãŸç”»åƒã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # æ‹¡å¼µå­ã‚’è©¦ã™\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«å‡¦ç†\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«YOLOV5GradCAMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆãƒ»ç ´æ£„\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # é€²æ—è¡¨ç¤º\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\nå‡¦ç†å®Œäº†: {processed_count}å€‹ã®ç”»åƒã‚’å‡¦ç†\")\n",
        "    print(f\"è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸç”»åƒ: {not_found_count}å€‹\")\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒ: {error_count}å€‹\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {csv_path}\")\n",
        "\n",
        "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
        "folder_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨saliency_methodã®å®šç¾©\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®target_layersï¼ˆconvã«ä¿®æ­£ãŒå¿…è¦ãªå ´åˆï¼‰\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_20_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_23_cv3_conv\",  # actã‹ã‚‰convã«å¤‰æ›´\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ\n",
        "USE_BATCH_MODE = False  # ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
        "BATCH_SIZE = 20       # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== ãƒãƒƒãƒå‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’ç¢ºèª\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"ç·ç”»åƒæ•°: {total_images}\")\n",
        "\n",
        "    # calculate_aoié–¢æ•°å†…ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯saliency_methodsã‚’ç”Ÿæˆã—ãªã„\n",
        "    saliency_methods = [] # ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦å®šç¾©\n",
        "\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"ç”»åƒ {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"ãƒãƒƒãƒ {batch_start//BATCH_SIZE + 1} å®Œäº†\")\n",
        "\n",
        "    print(\"\\n=== å…¨ãƒãƒƒãƒå‡¦ç†å®Œäº† ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ ===\")\n",
        "\n",
        "    # å‡¦ç†ç¯„å›²ã®è¨­å®š\n",
        "    start_index = 30\n",
        "    end_index = 100  # Noneã§å…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†\n",
        "\n",
        "    # calculate_aoié–¢æ•°å†…ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯saliency_methodsã‚’ç”Ÿæˆã—ãªã„\n",
        "    saliency_methods = [] # ç©ºã®ãƒªã‚¹ãƒˆã¨ã—ã¦å®šç¾©\n",
        "\n",
        "    print(\"\\nè§£æã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index, end_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uMYSR4IdpbaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0C6tEWVkpbca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhDcRK1apbeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNL_j2lkuem5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XxKs84GkBwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "outputs": [],
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
        "csv_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameã¨ã—ã¦CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameã®æœ€åˆã®æ•°è¡Œã‚’è¡¨ç¤ºã™ã‚‹\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGNyvTdM3c4"
      },
      "source": [
        "#**æ³¨ç›®ç‚¹è‰²å¡—ã‚Š**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "outputs": [],
      "source": [
        "#æ³¨ç›®ç‚¹ã«è‰²ã‚’å¡—ã‚‹ï¼ˆåè»¢ã‚ã‚Šï¼‰\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ã‚¹ãƒãƒ›_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #ãƒã‚¹ã‚¯ã®è‰²ï¼šç™½ã¯[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ãƒ•ã‚©ãƒˆã‚¹ãƒªãƒƒãƒˆ_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ã“ã“ã§é–¾å€¤ã‚’æŒ‡å®š\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "outputs": [],
      "source": [
        "# æ³¨ç›®ç‚¹ã‚’blurã™ã‚‹ï¼ˆåè»¢ã‚ã‚Šï¼‰\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ã‚¹ãƒãƒ›_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ã¼ã‹ã—åŠ¹æœã‚’é©ç”¨\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•/ãƒ•ã‚©ãƒˆã‚¹ãƒªãƒƒãƒˆ_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ã“ã“ã§é–¾å€¤ã‚’æŒ‡å®š\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WoakWi-ryq"
      },
      "source": [
        "#**Analyze results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03rgBUoWi_L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names as shown in the image\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\", \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Create a figure with higher DPI\n",
        "plt.figure(figsize=(15, 6), dpi=350)  # æ¨ªå¹…ã‚’å°‘ã—åºƒã’ã¾ã—ãŸ\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # æ¨ªè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å‚¾ã‘ã‚‹\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # æ¨ªè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å‚¾ã‘ã‚‹\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure at 350 DPI\n",
        "plt.savefig('confusion_matrices_350dpi.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjTt2gTWAAD3"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ã‚¹ãƒãƒ›vsã‚¹ãƒªãƒƒãƒˆ (GradCAM++)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º\n",
        "for df, device_name in [(df_slit, \"Slit Lamp\"), (df_sumaho, \"Smartphone\")]:\n",
        "    print(f\"\\n============= {device_name} ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ =============\")\n",
        "\n",
        "    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è©³ç´°ãªçµ±è¨ˆé‡ã‚’è¨ˆç®—\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} ã®çµ±è¨ˆå€¤ -------\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(stats['count'])}\")\n",
        "        print(f\"å¹³å‡å€¤: {stats['mean']:.6f}\")\n",
        "        print(f\"æ¨™æº–åå·®: {stats['std']:.6f}\")\n",
        "        print(f\"æœ€å°å€¤: {stats['min']:.6f}\")\n",
        "        print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['25%']:.6f}\")\n",
        "        print(f\"ä¸­å¤®å€¤: {stats['50%']:.6f}\")\n",
        "        print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['75%']:.6f}\")\n",
        "        print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"æœ€å¤§å€¤: {stats['max']:.6f}\")\n",
        "\n",
        "        # æ­ªåº¦ã¨å°–åº¦ã‚‚è¨ˆç®—\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"æ­ªåº¦: {skewness:.6f}\")\n",
        "        print(f\"å°–åº¦: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆã‚’è¨ˆç®—\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆ -------\")\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(sum_stats['count'])}\")\n",
        "    print(f\"å¹³å‡å€¤: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"æ¨™æº–åå·®: {sum_stats['std']:.6f}\")\n",
        "    print(f\"æœ€å°å€¤: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"ä¸­å¤®å€¤: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"æœ€å¤§å€¤: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—\n",
        "    print(f\"\\n------- ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ç›¸é–¢ä¿‚æ•° -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "NDEHCADOprhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v9DUXYHkJ5x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¯ãƒ©ã‚¹åã‚’è¨­å®š\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"Mean Â± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean Â± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ã‚¹ãƒªãƒƒãƒˆï¼ˆGradCAM vs GradCAM++ï¼‰\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¡¨ç¤º\n",
        "for df, device_name in [(df_slit, \"GradCAM\"), (df_sumaho, \"GradCAM++\")]:\n",
        "    print(f\"\\n============= {device_name} ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤ =============\")\n",
        "\n",
        "    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è©³ç´°ãªçµ±è¨ˆé‡ã‚’è¨ˆç®—\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} ã®çµ±è¨ˆå€¤ -------\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(stats['count'])}\")\n",
        "        print(f\"å¹³å‡å€¤: {stats['mean']:.6f}\")\n",
        "        print(f\"æ¨™æº–åå·®: {stats['std']:.6f}\")\n",
        "        print(f\"æœ€å°å€¤: {stats['min']:.6f}\")\n",
        "        print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['25%']:.6f}\")\n",
        "        print(f\"ä¸­å¤®å€¤: {stats['50%']:.6f}\")\n",
        "        print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {stats['75%']:.6f}\")\n",
        "        print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"æœ€å¤§å€¤: {stats['max']:.6f}\")\n",
        "\n",
        "        # æ­ªåº¦ã¨å°–åº¦ã‚‚è¨ˆç®—\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"æ­ªåº¦: {skewness:.6f}\")\n",
        "        print(f\"å°–åº¦: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆã‚’è¨ˆç®—\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆè¨ˆå€¤ã®çµ±è¨ˆ -------\")\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿æ•°: {int(sum_stats['count'])}\")\n",
        "    print(f\"å¹³å‡å€¤: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"æ¨™æº–åå·®: {sum_stats['std']:.6f}\")\n",
        "    print(f\"æœ€å°å€¤: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"ä¸­å¤®å€¤: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"æœ€å¤§å€¤: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # ç›¸é–¢ä¿‚æ•°ã®è¨ˆç®—\n",
        "    print(f\"\\n------- ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ç›¸é–¢ä¿‚æ•° -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®è¡¨ç¤º"
      ],
      "metadata": {
        "id": "sMhTeXg39b16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# åˆ—åã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—\n",
        "mean_sd_aoi_gradcam = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_gradcampp = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚¯ãƒ©ã‚¹åã‚’è¨­å®š\n",
        "mean_sd_aoi_gradcam.index = class_names\n",
        "mean_sd_aoi_gradcampp.index = class_names\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(\"Mean Â± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_gradcam)\n",
        "print(\"\\nMean Â± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_gradcampp)"
      ],
      "metadata": {
        "id": "XYWp2le2-Vjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      },
      "source": [
        "##**Layerã”ã¨ã«è§£æ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkZ4ZbaoUEMX"
      },
      "outputs": [],
      "source": [
        "#@title ã‚¹ãƒªãƒƒãƒˆvsã‚¹ãƒãƒ›\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# åˆç®—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# é’ã¨ã‚ªãƒ¬ãƒ³ã‚¸ã®ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’å®šç¾©\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2: åˆç®—ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•3: slitã¨sumahoã®å…¨ä½“æ¯”è¼ƒ\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between Slit Lamp and Smartphone', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GradCAM vs GradCAM++ (ã‚¹ãƒªãƒƒãƒˆ)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "df_slit['Type'] = 'GradCAM'\n",
        "df_sumaho['Type'] = 'GradCAM++'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# åˆç®—ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# é’ã¨ã‚ªãƒ¬ãƒ³ã‚¸ã®ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’å®šç¾©\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradcam_overall_comparison.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2: åˆç®—ãƒ‡ãƒ¼ã‚¿\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•3: slitã¨sumahoã®å…¨ä½“æ¯”è¼ƒ\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between GradCAM and GradCAM++', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8ZCGTRoJO7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.labelsize': 18,\n",
        "    'axes.titlesize': 20,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14,\n",
        "    'figure.titlesize': 22\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã®ãƒ—ãƒ­ãƒƒãƒˆã¨çµ±è¨ˆé‡è¨ˆç®—\n",
        "for layer in [17, 20, 23]:\n",
        "    layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã®ä½œæˆï¼ˆç¸¦æ¨ªæ¯”3:4ï¼‰\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºï¼ˆå…ˆã»ã©ã¨åŒã˜ç°è‰²ã®ã‚¹ã‚¿ã‚¤ãƒ«ï¼‰\n",
        "    sns.boxplot(x='class_name', y=layer_name, data=df_slit, order=class_names,\n",
        "                color='#CCCCCC',\n",
        "                boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "                medianprops={'color':'black', 'linewidth':2.5},\n",
        "                flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "                whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "                capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "    plt.xlabel('Class', fontsize=18, labelpad=10)\n",
        "    plt.ylabel(layer_name, fontsize=18, labelpad=10)\n",
        "    plt.title(f'Box Plot of {layer_name} for Slit Lamp Data', fontsize=20, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆé«˜è§£åƒåº¦ï¼‰\n",
        "    plt.savefig(f'boxplot_{layer_name}.png', dpi=350, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "    stats = df_slit.groupby('class_name')[layer_name].describe()\n",
        "    # ã‚¯ãƒ©ã‚¹ã®é †åºã‚’æŒ‡å®šã—ã¦ä¸¦ã³æ›¿ãˆ\n",
        "    stats = stats.reindex(class_names)\n",
        "    print(f\"\\n{layer_name}ã®åŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "    print(stats)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "v5jSgs_QuUly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJlHqzTQpuHP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "# plt.rcParams.update({\n",
        "#     'font.size': 12,\n",
        "#     'axes.labelsize': 16,\n",
        "#     'axes.titlesize': 16,\n",
        "#     'xtick.labelsize': 12,\n",
        "#     'ytick.labelsize': 12,\n",
        "#     'legend.fontsize': 12,\n",
        "#     'figure.titlesize': 18\n",
        "# })\n",
        "\n",
        "# # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "# file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "# file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "# df_slit = pd.read_csv(file_path1)\n",
        "# df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# # ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "# class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# # ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "# df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "# df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢ã—ã¦Seabornã§ãƒ—ãƒ­ãƒƒãƒˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
        "# df_slit['Type'] = 'Slit'\n",
        "# df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# # æ¯”è¼ƒã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "# layers_group1 = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "# layers_group2 = [\"17\", \"20\", \"23\"]\n",
        "\n",
        "# def create_boxplots(layers, group_name):\n",
        "#     for layer in layers:\n",
        "#         layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "#         # å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "#         df_combined = pd.concat([\n",
        "#             df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "#             df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "#         ]).reset_index(drop=True)\n",
        "\n",
        "#         # ã‚°ãƒ©ãƒ•: ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ã¨ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
        "#         plt.figure(figsize=(15, 10))\n",
        "#         sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=['#333333', '#999999'])\n",
        "#         plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "#         plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "#         plt.title(f'Box Plot of {layer_name} for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "#         plt.xticks(rotation=45, ha='right')\n",
        "#         legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#         legend.get_title().set_fontsize(14)\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#     # ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®æ¯”è¼ƒï¼ˆå¹…ã‚’åŠåˆ†ã«ï¼‰\n",
        "#     plt.figure(figsize=(7.5, 10))  # å¹…ã‚’åŠåˆ†ã«å¤‰æ›´\n",
        "#     df_melted = pd.melt(df_slit, id_vars=['class_name'], value_vars=[f'AOI_0.5_layer{layer}' for layer in layers], var_name='Layer', value_name='Value')\n",
        "#     sns.boxplot(x='Layer', y='Value', data=df_melted, palette='Blues')\n",
        "#     plt.xlabel('Layer', fontsize=14, labelpad=10)\n",
        "#     plt.ylabel('AOI_0.5 Value', fontsize=14, labelpad=10)\n",
        "#     plt.title(f'Comparison of AOI_0.5 Values\\nAcross Different Layers\\n({group_name})', fontsize=16, pad=20)\n",
        "#     plt.xticks(rotation=45, ha='right')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # ã‚°ãƒ«ãƒ¼ãƒ—1ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "# create_boxplots(layers_group1, \"Group 1: 24_m_0, 24_m_1, 24_m_2\")\n",
        "\n",
        "# # ã‚°ãƒ«ãƒ¼ãƒ—2ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "# create_boxplots(layers_group2, \"Group 2: 17, 20, 23\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ¬ã‚¤ãƒ¤ãƒ¼æ¯ã®AOIå€¤ï¼ˆ17, 20, 23ï¼‰(24-0, 24-1, 24-2)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®šã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,          # 16 â†’ 18\n",
        "    'axes.labelsize': 24,     # 22 â†’ 24\n",
        "    'axes.titlesize': 26,     # 24 â†’ 26\n",
        "    'xtick.labelsize': 18,    # 16 â†’ 18\n",
        "    'ytick.labelsize': 18,    # 16 â†’ 18\n",
        "    'legend.fontsize': 18,    # 16 â†’ 18\n",
        "    'figure.titlesize': 28    # 26 â†’ 28\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers_group1 = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\"]\n",
        "layers_group2 = [\"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•1ï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(9, 12))  # 3:4ã®æ¯”ç‡\n",
        "\n",
        "df_melted_1 = pd.melt(df[layers_group1], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_1['Layer'] = df_melted_1['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_1,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayers 17, 20, and 23', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_group1.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•2ï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(9, 12))  # 3:4ã®æ¯”ç‡\n",
        "\n",
        "df_melted_2 = pd.melt(df[layers_group2], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_2['Layer'] = df_melted_2['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_2,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayer 24 Outputs', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_group2.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ - ã‚°ãƒ«ãƒ¼ãƒ—1 (Layers 17, 20, 23):\")\n",
        "print(df[layers_group1].describe())\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ - ã‚°ãƒ«ãƒ¼ãƒ—2 (Layer 24 Outputs):\")\n",
        "print(df[layers_group2].describe())"
      ],
      "metadata": {
        "id": "C92rF-kAx8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,\n",
        "    'axes.labelsize': 24,\n",
        "    'axes.titlesize': 26,\n",
        "    'xtick.labelsize': 18,\n",
        "    'ytick.labelsize': 18,\n",
        "    'legend.fontsize': 18,\n",
        "    'figure.titlesize': 28\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# ã™ã¹ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆ3:4ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values Across Different Layers', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_all.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "print(df[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½\n",
        "h_statistic, p_value = stats.kruskal(*[df[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results:')\n",
        "print(f'H-statistic: {h_statistic:.3f}')\n",
        "print(f'p-value: {p_value:.3e}')\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰\n",
        "dunn = posthoc_dunn(df_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (p-values):')\n",
        "print(dunn.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰\n",
        "significant_pairs = []\n",
        "for i in dunn.index:\n",
        "    for j in dunn.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn.loc[i, j] < 0.05:\n",
        "                significant_pairs.append((i, j, dunn.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):')\n",
        "for pair in significant_pairs:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "31icyQtM1UNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š\n",
        "plt.rcParams.update({\n",
        "    'font.size': 22,  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤§ãã‚ã«è¨­å®š\n",
        "    'axes.labelsize': 28,  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'axes.titlesize': 30,  # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'xtick.labelsize': 24,  # xè»¸ã®ç›®ç››ã‚Šãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'ytick.labelsize': 24,  # yè»¸ã®ç›®ç››ã‚Šãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'legend.fontsize': 24,  # å‡¡ä¾‹ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "    'figure.titlesize': 32  # ãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º\n",
        "})\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df1 = pd.read_csv(file_path1)\n",
        "df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# ä½¿ç”¨ã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’å¤‰æ›´ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢\n",
        "df1_melted = pd.melt(df1[layers], var_name='Layer', value_name='AOI Value')\n",
        "df1_melted['Layer'] = df1_melted['Layer'].map(layer_mapping)\n",
        "df1_melted['Method'] = 'Grad-CAM'\n",
        "\n",
        "df2_melted = pd.melt(df2[layers], var_name='Layer', value_name='AOI Value')\n",
        "df2_melted['Layer'] = df2_melted['Layer'].map(layer_mapping)\n",
        "df2_melted['Method'] = 'Grad-CAM++'\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ¯ã‚Šç›´ã™\n",
        "df_combined = pd.concat([df1_melted, df2_melted], ignore_index=True)\n",
        "# ã‚ã‚‹ã„ã¯\n",
        "# df_combined = pd.concat([df1_melted, df2_melted])\n",
        "# df_combined = df_combined.reset_index(drop=True)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ä½œæˆï¼ˆ4:3ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ï¼‰\n",
        "plt.figure(figsize=(16, 12))  # 4:3ã®æ¯”ç‡ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´\n",
        "\n",
        "# boxplotã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
        "sns.boxplot(x='Layer', y='AOI Value', hue='Method', data=df_combined,\n",
        "            palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
        "            boxprops={'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=28, labelpad=15)  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.ylabel('AOI Value', fontsize=28, labelpad=15)  # è»¸ãƒ©ãƒ™ãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.title('Comparison of AOI Values Across Different Layers and Methods', fontsize=30, pad=25)  # ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å¤§ãã\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Type')\n",
        "plt.tight_layout()\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ï¼ˆ350dpiï¼‰\n",
        "plt.savefig('boxplot_aoi_comparison_all_combined.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º (Grad-CAM)\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ (Grad-CAM):\")\n",
        "print(df1[layers].describe())\n",
        "\n",
        "# åŸºæœ¬çµ±è¨ˆé‡ã®è¨ˆç®—ã¨è¡¨ç¤º (Grad-CAM++)\n",
        "print(\"\\nåŸºæœ¬çµ±è¨ˆé‡ (Grad-CAM++):\")\n",
        "print(df2[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½ (Grad-CAM)\n",
        "h_statistic1, p_value1 = stats.kruskal(*[df1[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM):')\n",
        "print(f'H-statistic: {h_statistic1:.3f}')\n",
        "print(f'p-value: {p_value1:.3e}')\n",
        "\n",
        "# Kruskal-Wallis æ¤œå®šã‚’å®Ÿæ–½ (Grad-CAM++)\n",
        "h_statistic2, p_value2 = stats.kruskal(*[df2[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM++):')\n",
        "print(f'H-statistic: {h_statistic2:.3f}')\n",
        "print(f'p-value: {p_value2:.3e}')\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰(Grad-CAM)\n",
        "dunn1 = posthoc_dunn(df1_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM) (p-values):')\n",
        "print(dunn1.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# Dunn's testã«ã‚ˆã‚‹å¤šé‡æ¯”è¼ƒï¼ˆBonferroniè£œæ­£ï¼‰(Grad-CAM++)\n",
        "dunn2 = posthoc_dunn(df2_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM++) (p-values):')\n",
        "print(dunn2.round(4))  # på€¤ã‚’4æ¡ã«ä¸¸ã‚ã‚‹\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰(Grad-CAM)\n",
        "significant_pairs1 = []\n",
        "for i in dunn1.index:\n",
        "    for j in dunn1.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn1.loc[i, j] < 0.05:\n",
        "                significant_pairs1.append((i, j, dunn1.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Grad-CAM) (p < 0.05):')\n",
        "for pair in significant_pairs1:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')\n",
        "\n",
        "# æœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆp < 0.05ï¼‰(Grad-CAM++)\n",
        "significant_pairs2 = []\n",
        "for i in dunn2.index:\n",
        "    for j in dunn2.columns:\n",
        "        if i < j:  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ã€ä¸Šä¸‰è§’è¡Œåˆ—ã®ã¿ã‚’è¦‹ã‚‹\n",
        "            if dunn2.loc[i, j] < 0.05:\n",
        "                significant_pairs2.append((i, j, dunn2.loc[i, j]))\n",
        "\n",
        "print('\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Grad-CAM++) (p < 0.05):')\n",
        "for pair in significant_pairs2:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "mRmADr6uMAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mt7gOfde1UPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFUt3AFWwvA"
      },
      "outputs": [],
      "source": [
        "####\n",
        "#### ã‚¹ãƒªãƒƒãƒˆ vs ã‚¹ãƒãƒ›ã®æ¯”è¼ƒ\n",
        "####\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# å„ã‚¯ãƒ©ã‚¹ã”ã¨ã«å¯¾å¿œã®ã‚ã‚‹tæ¤œå®šã‚’è¡Œã†\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚‹ãŸã‚ã€æœ€å°ã®é•·ã•ã«åˆã‚ã›ã‚‹\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆå€¤\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# å…¨ã‚¯ãƒ©ã‚¹ã¾ã¨ã‚ãŸå¯¾å¿œã®ã‚ã‚‹tæ¤œå®š\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# å¯¾å¿œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚‹ãŸã‚ã€æœ€å°ã®é•·ã•ã«åˆã‚ã›ã‚‹\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# å…¨ã‚¯ãƒ©ã‚¹ã¾ã¨ã‚ãŸçµ±è¨ˆå€¤\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# çµæœã‚’è¿½åŠ \n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã—ã¦è¡¨ç¤º\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "outputs": [],
      "source": [
        "# #ã‚¯ãƒ©ã‚¹æ¯ã®å·®ï¼ˆã‚¹ãƒãƒ›ï¼‹ã‚¹ãƒªãƒƒãƒˆï¼‰\n",
        "# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# # ANOVAã‚’å®Ÿè¡Œ\n",
        "# anova_result = stats.f_oneway(\n",
        "#     df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        "# )\n",
        "\n",
        "# print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# # äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "# tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# # çµæœã‚’è¡¨ç¤º\n",
        "# print(tukey_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JihM5ooC3VCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def dunn_test(groups, group_names):\n",
        "    \"\"\"\n",
        "    Dunn's testã®å®Ÿè£…\n",
        "    \"\"\"\n",
        "    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘\n",
        "    all_data = np.concatenate(groups)\n",
        "    ranks = stats.rankdata(all_data)\n",
        "\n",
        "    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®å¹³å‡ãƒ©ãƒ³ã‚¯ã‚’è¨ˆç®—\n",
        "    start = 0\n",
        "    mean_ranks = []\n",
        "    ns = []\n",
        "    for group in groups:\n",
        "        n = len(group)\n",
        "        group_ranks = ranks[start:start + n]\n",
        "        mean_ranks.append(np.mean(group_ranks))\n",
        "        ns.append(n)\n",
        "        start += n\n",
        "\n",
        "    # å…¨ãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§Dunn's testã‚’å®Ÿè¡Œ\n",
        "    N = len(ranks)\n",
        "    k = len(groups)\n",
        "    comparisons = []\n",
        "    p_values = []\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            # å¹³å‡ãƒ©ãƒ³ã‚¯ã®å·®\n",
        "            diff = abs(mean_ranks[i] - mean_ranks[j])\n",
        "\n",
        "            # æ¨™æº–èª¤å·®\n",
        "            se = np.sqrt((N * (N + 1) / 12) * (1/ns[i] + 1/ns[j]))\n",
        "\n",
        "            # zçµ±è¨ˆé‡\n",
        "            z = diff / se\n",
        "\n",
        "            # på€¤ï¼ˆä¸¡å´æ¤œå®šï¼‰\n",
        "            p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "            comparisons.append((group_names[i], group_names[j]))\n",
        "            p_values.append(p)\n",
        "\n",
        "    # Bonferroniè£œæ­£\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n",
        "\n",
        "    # æœ‰æ„ãªçµæœã‚’è¿”ã™\n",
        "    significant_results = []\n",
        "    for (name1, name2), p_corr, is_rej in zip(comparisons, p_corrected, rejected):\n",
        "        if is_rej:  # p < 0.05 after correction\n",
        "            significant_results.append({\n",
        "                'group1': name1,\n",
        "                'group2': name2,\n",
        "                'p_value': p_corr\n",
        "            })\n",
        "\n",
        "    return significant_results\n",
        "\n",
        "for layer in layers:\n",
        "    # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’è¨­å®š\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # æ¬ æå€¤ã‚’é™¤å¤–ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæ¬ æå€¤ã‚’é™¤å¤–ï¼‰\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # å°‘ãªãã¨ã‚‚2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦\n",
        "            # å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\nå…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: {same_value:.4f}\")\n",
        "                print(\"çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-testçµæœ:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # Dunn's testã‚’å®Ÿè¡Œ\n",
        "                significant_pairs = dunn_test(class_groups, valid_class_names)\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (Bonferroniè£œæ­£å¾Œ p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}\")\n",
        "        else:\n",
        "            print(f\"\\nè­¦å‘Š: {layer_name}ã®è§£æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nè­¦å‘Š: {layer_name}ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "9mmq-hNs_4Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©ï¼ˆæŒ‡å®šã•ã‚ŒãŸé †åºï¼‰\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def perform_pairwise_mannwhitney(groups, group_names, alpha=0.05):\n",
        "    \"\"\"ãƒšã‚¢ãƒ¯ã‚¤ã‚ºMann-Whitney Uæ¤œå®šã‚’å®Ÿè¡Œ\"\"\"\n",
        "    n_groups = len(groups)\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_groups):\n",
        "        for j in range(i+1, n_groups):\n",
        "            try:\n",
        "                stat, p_value = mannwhitneyu(groups[i], groups[j], alternative='two-sided')\n",
        "                n1, n2 = len(groups[i]), len(groups[j])\n",
        "                effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "                if p_value < alpha:\n",
        "                    results.append({\n",
        "                        'group1': group_names[i],\n",
        "                        'group2': group_names[j],\n",
        "                        'p_value': p_value,\n",
        "                        'effect_size': effect_size\n",
        "                    })\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return results\n",
        "\n",
        "for layer in layers:\n",
        "    # ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’è¨­å®š\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # æ¬ æå€¤ã‚’é™¤å¤–ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆæ¬ æå€¤ã‚’é™¤å¤–ï¼‰\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # å°‘ãªãã¨ã‚‚2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦\n",
        "            # å…¨ã¦ã®å€¤ãŒåŒã˜ã‹ãƒã‚§ãƒƒã‚¯\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\nå…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã§åŒã˜å€¤: {same_value:.4f}\")\n",
        "                print(\"çµ±è¨ˆçš„æ¤œå®šã¯ä¸è¦ã§ã™ã€‚\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-testçµæœ:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºMann-Whitney Uæ¤œå®šã‚’å®Ÿè¡Œ\n",
        "                significant_pairs = perform_pairwise_mannwhitney(\n",
        "                    class_groups,\n",
        "                    valid_class_names\n",
        "                )\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}, effect size = {result['effect_size']:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\nè­¦å‘Š: {layer_name}ã®è§£æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(f\"\\nè­¦å‘Š: {layer_name}ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "NShTeYYE3VEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "file_path = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# è§£æã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒªã‚¹ãƒˆ\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚’é•·å½¢å¼ã«å¤‰æ›\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼åã‚’ç°¡ç•¥åŒ–\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# æ¬ æå€¤ã‚’é™¤å¤–\n",
        "df_melted = df_melted.dropna()\n",
        "\n",
        "# Kruskal-Wallis H-testã‚’å®Ÿè¡Œ\n",
        "groups = [group['AOI Value'].values for name, group in df_melted.groupby('Layer')]\n",
        "h_statistic, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(\"Kruskal-Wallis H-testçµæœ:\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "# ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºæ¯”è¼ƒï¼ˆMann-Whitney U testï¼‰ã‚’å®Ÿè¡Œ\n",
        "layer_names = sorted(df_melted['Layer'].unique())\n",
        "significant_pairs = []\n",
        "\n",
        "print(\"\\næœ‰æ„å·®ã®ã‚ã‚‹ãƒšã‚¢ (p < 0.05):\")\n",
        "for i, layer1 in enumerate(layer_names):\n",
        "    for layer2 in layer_names[i+1:]:\n",
        "        group1 = df_melted[df_melted['Layer'] == layer1]['AOI Value']\n",
        "        group2 = df_melted[df_melted['Layer'] == layer2]['AOI Value']\n",
        "\n",
        "        try:\n",
        "            stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
        "            n1, n2 = len(group1), len(group2)\n",
        "            effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "            if p_value < 0.05:  # Bonferroniè£œæ­£ã‚’é©ç”¨ã™ã‚‹å ´åˆã¯ 0.05/len(pairs) ã‚’ä½¿ç”¨\n",
        "                print(f\"{layer1:10} vs {layer2:10}: p = {p_value:.4e}, effect size = {effect_size:.4f}\")\n",
        "                significant_pairs.append((layer1, layer2, p_value, effect_size))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "# å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡ï¼ˆä¸­å¤®å€¤ã¨å››åˆ†ä½æ•°ã‚’å«ã‚€ï¼‰\n",
        "print(\"\\nå„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡:\")\n",
        "stats_summary = df_melted.groupby('Layer')['AOI Value'].agg([\n",
        "    ('count', 'count'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('std', 'std'),\n",
        "    ('Q1', lambda x: x.quantile(0.25)),\n",
        "    ('Q3', lambda x: x.quantile(0.75))\n",
        "]).round(4)\n",
        "\n",
        "print(stats_summary)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å½¢å¼ã§æœ‰æ„å·®ã‚’è¡¨ç¤º\n",
        "print(\"\\næœ‰æ„å·®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ (â˜…: p < 0.05)\")\n",
        "matrix = pd.DataFrame(index=layer_names, columns=layer_names)\n",
        "np.fill_diagonal(matrix.values, '-')\n",
        "matrix = matrix.fillna('ã€€')\n",
        "\n",
        "for pair in significant_pairs:\n",
        "    matrix.loc[pair[0], pair[1]] = \"â˜…\"\n",
        "    matrix.loc[pair[1], pair[0]] = \"â˜…\"\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "VJkb_wra3VGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zc1fuTqBK7"
      },
      "source": [
        "###**ANOVA_heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# çµæœã‚’DataFrameã«å¤‰æ›\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š\n",
        "file_path1 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/ç ”ç©¶/é€²è¡Œä¸­ã®ç ”ç©¶/è§’è†œã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³AIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ/å‰åŸã®240å•_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# å¿…è¦ãªã‚«ãƒ©ãƒ ã ã‘ã‚’é¸æŠã—ã€æ¬ æå€¤ã‚’å‰Šé™¤ã—ã¦çµåˆ\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# çµæœã‚’DataFrameã«å¤‰æ›\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½œæˆï¼ˆã‚ªãƒ¬ãƒ³ã‚¸ã€ã‚°ãƒ¬ãƒ¼ã€ç™½ï¼‰\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # ã‚ªãƒ¬ãƒ³ã‚¸ã€ã‚°ãƒ¬ãƒ¼ã€ç™½\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# på€¤ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆNaNã¯2ã€p<0.05ã¯0ã€ãã‚Œä»¥å¤–ã¯1ï¼‰\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # å…ƒã®på€¤ã‚’è¡¨ç¤º\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã‚’éè¡¨ç¤ºã«\n",
        "            xticklabels=class_names,  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’ã‚¯ãƒ©ã‚¹åã«è¨­å®š\n",
        "            yticklabels=class_names)  # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’ã‚¯ãƒ©ã‚¹åã«è¨­å®š\n",
        "\n",
        "# ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã‚’è¿½åŠ \n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p â‰¥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’45åº¦å›è»¢\n",
        "plt.yticks(rotation=0)  # yè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’æ°´å¹³ã«\n",
        "plt.tight_layout()  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbxW3P2OqFTb"
      },
      "source": [
        "###**Tukey_sumaho/slitåˆ¥**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # æ¬ æå€¤ã‚’å‰Šé™¤\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAã‚’å®Ÿè¡Œ\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # äº‹å¾Œæ¤œå®šï¼ˆTukeyã®HSDæ¤œå®šï¼‰ã‚’å®Ÿè¡Œ\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å›³ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹åã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã—ã¦ä½¿ç”¨\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«på€¤ã‚’å…¥åŠ›\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # åŒã˜ã‚¯ãƒ©ã‚¹é–“ã®ã‚»ãƒ«ã‚’NaNã«è¨­å®š\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æç”»\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# ã‚¹ãƒªãƒƒãƒˆãƒ©ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ã®è§£æ\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ãƒ‡ãƒ¼ã‚¿ã®è§£æ\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ubPn9Erqnid"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’å®šç¾©\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# ã‚¯ãƒ©ã‚¹åã‚’æ•°å­—ã«å¯¾å¿œã•ã›ã‚‹\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•´å½¢\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" ã‚°ãƒ«ãƒ¼ãƒ—ã¨ãã®ä»–ã®ã‚¯ãƒ©ã‚¹ã«åˆ†é¡\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# tæ¤œå®šã‚’å®Ÿè¡Œ\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# çµæœã‚’è¡¨ç¤º\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# çµ±è¨ˆå€¤ã‚’è¨ˆç®—\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP2qxBYFl8gtN4TFFYrxzEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}