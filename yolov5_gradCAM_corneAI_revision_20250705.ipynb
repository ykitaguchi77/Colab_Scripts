{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Colab_Scripts/blob/master/yolov5_gradCAM_corneAI_revision_20250705.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42M6k9QpvSq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cakhs2BZLRA"
      },
      "source": [
        "#**YOLOv5_GradCAM_CorneAI**\n",
        "\n",
        "論文revision用\n",
        "\n",
        "https://github.com/pooya-mohammadi/yolov5-gradcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUdHWgh79uc"
      },
      "source": [
        "###**⭐️⭐️Area of interestの計算**\n",
        "\n",
        "結果をcsvに保存する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNjX_T_cVa-9",
        "outputId": "302a5988-72dc-48ca-cac4-ee9ca37ba20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/content\n",
            "\u001b[33mWARNING: Skipping deep_utils as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for deep_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h[INFO] To use new installed version of opencv, the session should be restarted!!!!\n",
            "Cloning into 'yolov5-gradcam'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 134 (delta 36), reused 36 (delta 36), pack-reused 94 (from 2)\u001b[K\n",
            "Receiving objects: 100% (134/134), 5.17 MiB | 21.99 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd /content\n",
        "!pip uninstall deep_utils -y\n",
        "!pip install -U git+https://github.com/pooya-mohammadi/deep_utils.git --q\n",
        "!pip install torch --q\n",
        "!pip install torchvision --q\n",
        "!pip install -U opencv-python --q\n",
        "print(\"[INFO] To use new installed version of opencv, the session should be restarted!!!!\")\n",
        "\n",
        "!git clone https://github.com/pooya-mohammadi/yolov5-gradcam\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### GPUの種類により結果が異なるため、CPUで計算したものを採用\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colab環境でcv2_imshowを使用する場合にコメント解除\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "# FutureWarningを一時的に抑制（register_backward_hook使用時）\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5の必要な関数をインポート - attempt_loadの場所を修正\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # 修正: models.experimentalから\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポートします\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# これまでに登場したクラスをすべてリストに追加します\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# === 最適化: PYTORCH_CUDA_ALLOC_CONFの詳細設定 ===\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:512\"\n",
        "\n",
        "# === 最適化: PyTorch 2.0以上の場合、コンパイルキャッシュをリセット ===\n",
        "if hasattr(torch._dynamo, 'reset'):\n",
        "    torch._dynamo.reset()\n",
        "\n",
        "# グローバル変数として定義（エラー回避のため）\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ========== 統合版：メモリ管理ユーティリティ ==========\n",
        "class GPUMemoryMonitor:\n",
        "    \"\"\"GPU メモリの監視と管理を行うクラス\"\"\"\n",
        "\n",
        "    def __init__(self, warning_threshold=0.8, critical_threshold=0.9):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            warning_threshold: 警告を出すメモリ使用率の閾値 (0-1)\n",
        "            critical_threshold: クリティカル警告を出すメモリ使用率の閾値 (0-1)\n",
        "        \"\"\"\n",
        "        self.warning_threshold = warning_threshold\n",
        "        self.critical_threshold = critical_threshold\n",
        "        self.device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "        self.total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0\n",
        "\n",
        "    def get_memory_stats(self):\n",
        "        \"\"\"現在のメモリ使用状況を取得\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {\n",
        "                'allocated': 0,\n",
        "                'reserved': 0,\n",
        "                'free': 0,\n",
        "                'total': 0,\n",
        "                'usage_ratio': 0\n",
        "            }\n",
        "\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        total = self.total_memory\n",
        "        free = total - reserved\n",
        "        usage_ratio = reserved / total if total > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'allocated': allocated,\n",
        "            'reserved': reserved,\n",
        "            'free': free,\n",
        "            'total': total,\n",
        "            'usage_ratio': usage_ratio\n",
        "        }\n",
        "\n",
        "    def display_memory_status(self, prefix=\"\"):\n",
        "        \"\"\"メモリ状況を視覚的に表示\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        # 使用率に応じた色付け（ANSIエスケープコード）\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            color = \"\\033[91m\"  # 赤\n",
        "            status = \"⚠️  CRITICAL\"\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            color = \"\\033[93m\"  # 黄\n",
        "            status = \"⚠️  WARNING\"\n",
        "        else:\n",
        "            color = \"\\033[92m\"  # 緑\n",
        "            status = \"✅ OK\"\n",
        "        reset_color = \"\\033[0m\"\n",
        "\n",
        "        # プログレスバーの作成\n",
        "        bar_length = 30\n",
        "        filled_length = int(bar_length * stats['usage_ratio'])\n",
        "        bar = '█' * filled_length + '░' * (bar_length - filled_length)\n",
        "\n",
        "        print(f\"\\n{prefix}GPU Memory Status ({self.device_name}) {status}\")\n",
        "        print(f\"├─ Usage: {color}[{bar}] {stats['usage_ratio']*100:.1f}%{reset_color}\")\n",
        "        print(f\"├─ Allocated: {stats['allocated']:.2f} GB\")\n",
        "        print(f\"├─ Reserved:  {stats['reserved']:.2f} GB\")\n",
        "        print(f\"├─ Free:      {stats['free']:.2f} GB\")\n",
        "        print(f\"└─ Total:     {stats['total']:.2f} GB\")\n",
        "\n",
        "    def check_memory_health(self):\n",
        "        \"\"\"メモリ使用状況をチェックし、必要に応じてクリーンアップを推奨\"\"\"\n",
        "        stats = self.get_memory_stats()\n",
        "\n",
        "        if stats['usage_ratio'] > self.critical_threshold:\n",
        "            return 'critical', stats\n",
        "        elif stats['usage_ratio'] > self.warning_threshold:\n",
        "            return 'warning', stats\n",
        "        else:\n",
        "            return 'ok', stats\n",
        "\n",
        "    def cleanup_if_needed(self, force=False):\n",
        "        \"\"\"必要に応じてメモリクリーンアップを実行\"\"\"\n",
        "        health, stats = self.check_memory_health()\n",
        "\n",
        "        if health == 'critical' or force:\n",
        "            print(f\"\\n🧹 メモリクリーンアップを実行中... (使用率: {stats['usage_ratio']*100:.1f}%)\")\n",
        "            aggressive_memory_cleanup()  # 統合：より積極的なクリーンアップ関数を使用\n",
        "\n",
        "            # クリーンアップ後の状態を確認\n",
        "            new_stats = self.get_memory_stats()\n",
        "            freed = stats['reserved'] - new_stats['reserved']\n",
        "            print(f\"✨ クリーンアップ完了: {freed:.2f} GB 解放されました\")\n",
        "\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# === 統合：旧バージョンの有用な関数を追加 ===\n",
        "def find_cuda_tensors():\n",
        "    \"\"\"メモリに残っているCUDAテンソルを検出\"\"\"\n",
        "    cuda_tensors = []\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj) and obj.is_cuda:\n",
        "                cuda_tensors.append((type(obj), obj.size(), obj.element_size() * obj.nelement()))\n",
        "        except:\n",
        "            pass\n",
        "    return cuda_tensors\n",
        "\n",
        "def log_cuda_tensors(message=\"\"):\n",
        "    \"\"\"CUDAテンソルの状態をログ出力\"\"\"\n",
        "    cuda_tensors = find_cuda_tensors()\n",
        "    if cuda_tensors:\n",
        "        print(f\"{message} 検出されたCUDAテンソル数: {len(cuda_tensors)}\")\n",
        "        # 大きいテンソルのみ表示\n",
        "        large_tensors = [t for t in cuda_tensors if t[2] > 1024*1024]  # 1MB以上\n",
        "        if large_tensors:\n",
        "            print(f\"  大きなテンソル (>1MB): {len(large_tensors)}個\")\n",
        "            for i, (tensor_type, size, bytes_size) in enumerate(large_tensors[:5]):\n",
        "                print(f\"    {i+1}. Size: {size}, Memory: {bytes_size/1024**2:.2f}MB\")\n",
        "\n",
        "def aggressive_memory_cleanup():\n",
        "    \"\"\"積極的なメモリクリーンアップを実行\"\"\"\n",
        "    # Pythonのガベージコレクション\n",
        "    gc.collect()\n",
        "\n",
        "    # PyTorchのキャッシュクリア\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # CUDAのメモリプールをリセット（PyTorch 1.10以降）\n",
        "        if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "        if hasattr(torch.cuda, 'reset_accumulated_memory_stats'):\n",
        "            torch.cuda.reset_accumulated_memory_stats()\n",
        "\n",
        "    # PyTorch 2.0以上の場合、コンパイルキャッシュもクリア\n",
        "    if hasattr(torch._dynamo, 'reset'):\n",
        "        torch._dynamo.reset()\n",
        "\n",
        "def set_model_gradients(model, layer_name, enable=True):\n",
        "    \"\"\"特定のレイヤーのみ勾配計算を有効化\"\"\"\n",
        "    # まず全体の勾配を無効化\n",
        "    for param in model.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 特定のレイヤーのみ有効化\n",
        "    if enable:\n",
        "        target_layer = find_yolo_layer(model, layer_name)\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# ========== 既存のクラスは変更なし ==========\n",
        "# YOLOV5TorchObjectDetectorクラス（変更なし）\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === 最適化: デフォルトでは勾配計算を無効化 ===\n",
        "        # Grad-CAM使用時のみ必要なレイヤーで有効化\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === 変更点: 推論部分のwith torch.no_grad()を削除 ===\n",
        "        # Grad-CAMのために勾配計算を有効にする\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # === 改善点: NMS後の処理は勾配不要なのでno_grad()を使用 ===\n",
        "        with torch.no_grad():\n",
        "            # 以下の処理はCPUで行うことでGPUメモリを節約\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                            range(4)]\n",
        "            for i, det in enumerate(prediction):  # detections per image\n",
        "                if len(det):\n",
        "                    # detをCPUに移動し、元のGPUテンソルは即座に削除\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det  # GPUメモリを即座に解放\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            # predictionは不要になったので削除\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img  # 元の画像データを削除\n",
        "\n",
        "        # リサイズ処理を効率化\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0  # 中間データを削除\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMクラス（変更なし）\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\", debug=False):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.debug = debug  # デバッグフラグ\n",
        "        self.layer_name = layer_name  # デバッグ用に保存\n",
        "\n",
        "        # === 改善点1: フックハンドルを保存 ===\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            # === 変更点: detach()してからclone()でより安全に ===\n",
        "            try:\n",
        "                # detach()で計算グラフから切り離してからclone()\n",
        "                self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Successfully detached and cloned grad_output for {layer_name}\")\n",
        "            except Exception as e:\n",
        "                if self.debug:\n",
        "                    print(f\"[DEBUG] Error in backward_hook: {e}\")\n",
        "                raise\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            # === 変更点: outputもdetach().clone()してより安全に ===\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # === 最適化: 必要なレイヤーのみ勾配計算を有効化 ===\n",
        "        set_model_gradients(self.model, layer_name, enable=True)\n",
        "\n",
        "        # === 改善点2: フックハンドルを保存 ===\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"フックとメモリを明示的にクリーンアップする\"\"\"\n",
        "        # === 改善点3: フックの削除 ===\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Forward hook removed for {self.layer_name}\")\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "            if self.debug:\n",
        "                print(f\"[DEBUG] Backward hook removed for {self.layer_name}\")\n",
        "\n",
        "        # === 改善点4: 辞書のクリア ===\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # === 最適化: レイヤーの勾配計算を無効化 ===\n",
        "        set_model_gradients(self.model, self.layer_name, enable=False)\n",
        "\n",
        "        # === 改善点5: ターゲットレイヤーへの参照を削除 ===\n",
        "        self.target_layer = None\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] GradCAM cleanup completed for {self.layer_name}\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"コンテキストマネージャーのエントリーポイント\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Entering GradCAM context for {self.layer_name}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"コンテキストマネージャーの終了ポイント（自動クリーンアップ）\"\"\"\n",
        "        if self.debug:\n",
        "            print(f\"[DEBUG] Exiting GradCAM context for {self.layer_name}\")\n",
        "            if exc_type is not None:\n",
        "                print(f\"[DEBUG] Exception occurred: {exc_type.__name__}: {exc_val}\")\n",
        "        self.cleanup()\n",
        "        # Falseを返すことで例外を再発生させる\n",
        "        return False\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"デストラクタでも念のためクリーンアップ\"\"\"\n",
        "        try:\n",
        "            self.cleanup()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # === 改善点: 各クラスごとに順伝播を再実行 ===\n",
        "        # まず、一度だけフォワードパスを実行してtop3_indicesを取得\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkの計算は勾配不要なのでno_grad()を適用\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            # top3_indicesは不要になったので削除\n",
        "            del top3_indices\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            # EigenCAMの場合は一度のフォワードパスで計算\n",
        "            with torch.no_grad():  # EigenCAMは勾配不要\n",
        "                saliency_map = self._eigencam()\n",
        "                saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            # GradCAMまたはGradCAM++の場合、各クラスごとに処理\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                # === 重要な変更: 各クラスごとに新しいフォワードパスを実行 ===\n",
        "                # モデルの勾配をリセット\n",
        "                self.model.zero_grad()\n",
        "\n",
        "                # === 追加: インプレース操作を避けるためにinput_imgをclone ===\n",
        "                input_img_clone = input_img.clone()\n",
        "\n",
        "                # 新しいフォワードパス\n",
        "                _, new_logits = self.model(input_img_clone)\n",
        "\n",
        "                # 即座に不要な変数を削除\n",
        "                del input_img_clone\n",
        "\n",
        "                if class_idx:\n",
        "                    score = new_logits[0][0][cls]\n",
        "                else:\n",
        "                    score = new_logits[0][0].max()\n",
        "\n",
        "                # === 変更点: retain_graph=Trueを削除 ===\n",
        "                score.backward()  # retain_graph=Trueを削除\n",
        "\n",
        "                # === 追加: gradientsとactivationsが存在することを確認 ===\n",
        "                if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                    print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                    del score, new_logits  # 存在しない場合はここで削除\n",
        "                    continue\n",
        "\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # scoreとnew_logitsは不要になったので削除\n",
        "                del score\n",
        "                del new_logits\n",
        "\n",
        "                # saliency_mapの計算は勾配不要なのでno_grad()を適用\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    del weights  # weightsを即座に削除\n",
        "\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ゼロ除算を避けるためにepsilonを追加\n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                    # min/maxは不要なので削除\n",
        "                    del saliency_map_min, saliency_map_max\n",
        "\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "                # === 追加: 各クラスの処理後にメモリクリーンアップ ===\n",
        "                # 勾配とアクティベーションを即座に削除\n",
        "                if \"value\" in self.gradients:\n",
        "                    del self.gradients[\"value\"]\n",
        "                if \"value\" in self.activations:\n",
        "                    del self.activations[\"value\"]\n",
        "\n",
        "                # 不要な中間変数を削除\n",
        "                del gradients, activations\n",
        "\n",
        "                # CUDAキャッシュのクリア（必要に応じて）\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        # view操作は新しいテンソルを作成しないので効率的\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha  # alphaを削除\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom  # 中間変数を削除\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha  # 中間変数を削除\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        del cov  # 共分散行列を削除\n",
        "\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        del eigenvalues, eigenvectors  # 不要な固有値・固有ベクトルを削除\n",
        "\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        del activations_reshaped, leading_eigenvector  # 中間変数を削除\n",
        "\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        del eigen_cam_min, eigen_cam_max  # min/maxを削除\n",
        "\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layer関数\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoi関数（メモリ効率化版）\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():  # 全体を勾配計算不要で囲む\n",
        "        for mask in masks:\n",
        "            # マスクをCPUで処理（GPUメモリ節約）\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask  # 元のマスクは即座に削除\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # 閾値処理（より効率的な方法）\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu  # 処理済みのマスクは削除\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask  # 不要な変数を削除\n",
        "\n",
        "        mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "        AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# ========== 統合版：calculate_aoi関数（視覚的メモリモニタリング機能付き） ==========\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True, debug_mode=False):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # メモリモニターの初期化\n",
        "    memory_monitor = GPUMemoryMonitor(warning_threshold=0.75, critical_threshold=0.85)\n",
        "\n",
        "    # 開始時のメモリ状況を表示\n",
        "    memory_monitor.display_memory_status(\"🚀 処理開始時\")\n",
        "\n",
        "    # === システム情報の表示 ===\n",
        "    print(\"\\n=== システム情報 ===\")\n",
        "    print(f\"Python version: {sys.version}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"CUDA version: {torch.version.cuda}\")\n",
        "        print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "            props = torch.cuda.get_device_properties(i)\n",
        "            print(f\"    Total memory: {props.total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # AOIカラムの定義\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # カラムが存在しない場合は追加\n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # 処理済み画像の判定関数\n",
        "    def is_processed(row):\n",
        "        \"\"\"すべてのAOIカラムに値が入っているかチェック\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # 処理対象のインデックスを決定\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # 未処理の画像のみを対象とする\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"すべての画像が処理済みです。\")\n",
        "            return\n",
        "\n",
        "        print(f\"未処理画像数: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"最初の未処理画像インデックス: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # 処理対象インデックスのリスト\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # 手動でインデックス範囲を指定した場合\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedがTrueの場合、範囲内でも処理済みはスキップ\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"処理対象画像数: {len(target_indices)}\")\n",
        "\n",
        "    # 進捗状況の表示\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "    cuda_error_occurred = False\n",
        "    memory_cleanup_count = 0\n",
        "\n",
        "    # エラー詳細の記録用\n",
        "    error_details = {}\n",
        "    # 成功したレイヤー数のカウント\n",
        "    successful_layers = 0\n",
        "\n",
        "    # 定期的な保存のカウンター\n",
        "    save_interval = 5  # 5画像ごとに保存（より頻繁に）\n",
        "    memory_check_interval = 3  # 3画像ごとにメモリチェック\n",
        "\n",
        "    # メインの処理ループ\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        # 定期的なメモリチェック\n",
        "        if i % memory_check_interval == 0 and i > 0:\n",
        "            health, stats = memory_monitor.check_memory_health()\n",
        "\n",
        "            if health == 'critical':\n",
        "                print(f\"\\n⚠️  メモリ使用率がクリティカル ({stats['usage_ratio']*100:.1f}%) です！\")\n",
        "                memory_monitor.display_memory_status(\"📊 現在\")\n",
        "\n",
        "                # 強制クリーンアップ\n",
        "                if memory_monitor.cleanup_if_needed(force=True):\n",
        "                    memory_cleanup_count += 1\n",
        "                    time.sleep(2)  # GPUのクールダウン\n",
        "\n",
        "                    # クリーンアップ後の状態を表示\n",
        "                    memory_monitor.display_memory_status(\"🔄 クリーンアップ後\")\n",
        "\n",
        "                    # デバッグモードの場合、詳細なテンソル情報も表示\n",
        "                    if debug_mode:\n",
        "                        log_cuda_tensors(\"クリーンアップ後:\")\n",
        "\n",
        "            elif health == 'warning':\n",
        "                if i % (memory_check_interval * 3) == 0:  # 警告時は頻度を下げて表示\n",
        "                    memory_monitor.display_memory_status(\"⚠️  メモリ警告\")\n",
        "\n",
        "                # 自動クリーンアップの実行\n",
        "                if memory_monitor.cleanup_if_needed():\n",
        "                    memory_cleanup_count += 1\n",
        "\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # 画像ファイルの検索\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # === 改善点: 前処理後、元の画像は即座に削除 ===\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "                del img  # 元の画像データを削除してメモリ解放\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # デバッグモードの場合はdebug=Trueを渡す\n",
        "                        debug_flag = debug_mode and processed_count < 5\n",
        "\n",
        "                        # === 改善点: コンテキストマネージャーを使用 ===\n",
        "                        with YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                          img_size=input_size, method=\"gradcampp\",\n",
        "                                          debug=debug_flag) as saliency_method:\n",
        "\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "                                successful_layers += 1\n",
        "\n",
        "                                # デバッグ用：AOI値が正しく計算されているか確認\n",
        "                                if processed_count < 5:  # 最初の5画像のみログ出力\n",
        "                                    print(f\"  Layer {layer_name}: AOI = {aoi:.4f}\")\n",
        "\n",
        "                                # === 改善点: マスクとボックスを即座に削除 ===\n",
        "                                del mask, bbox\n",
        "                            else:\n",
        "                                # デバッグ用：マスクまたはボックスが空の場合\n",
        "                                if processed_count < 5:\n",
        "                                    print(f\"  Layer {layer_name}: No masks or boxes detected\")\n",
        "\n",
        "                            # === 改善点: 結果を即座に削除 ===\n",
        "                            del masks, logits, boxes, cls_names\n",
        "\n",
        "                        # with文を抜けた時点で自動的にcleanup()が呼ばれる\n",
        "                        # 追加のメモリクリーンアップ（各レイヤー処理後）\n",
        "                        aggressive_memory_cleanup()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # CUDA out of memoryエラーのチェック\n",
        "                        if \"CUDA out of memory\" in str(e):\n",
        "                            print(f\"\\n\\nCUDA out of memory エラーが発生しました！\")\n",
        "                            print(f\"エラー詳細: {str(e)}\")\n",
        "                            print(f\"現在の画像: {img_basename} (インデックス: {index})\")\n",
        "                            print(f\"エラー発生レイヤー: {layer_name}\")\n",
        "                            memory_monitor.display_memory_status(\"💥 エラー発生時\")\n",
        "                            if debug_mode:\n",
        "                                log_cuda_tensors(\"エラー発生時:\")\n",
        "                            cuda_error_occurred = True\n",
        "                            break\n",
        "                        else:\n",
        "                            error_count += 1\n",
        "                            # エラー詳細を記録\n",
        "                            error_key = f\"{img_basename}_{layer_name}\"\n",
        "                            if error_key not in error_details:\n",
        "                                error_details[error_key] = str(e)\n",
        "\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                # CUDA out of memoryエラーが発生したらループを抜ける\n",
        "                if cuda_error_occurred:\n",
        "                    break\n",
        "\n",
        "                # === 改善点: 全レイヤー処理後にtorch_imgを削除 ===\n",
        "                del torch_img\n",
        "\n",
        "                # === 最適化: 各画像処理後に完全なメモリクリーンアップ ===\n",
        "                aggressive_memory_cleanup()\n",
        "\n",
        "                processed_count += 1\n",
        "\n",
        "                # 進捗表示の改善（メモリ使用率も表示）\n",
        "                if processed_count % 10 == 0:\n",
        "                    stats = memory_monitor.get_memory_stats()\n",
        "                    print(f\"\\n📈 進捗: {processed_count}/{len(target_indices)} 画像処理済み \"\n",
        "                          f\"(GPU使用率: {stats['usage_ratio']*100:.1f}%)\")\n",
        "\n",
        "                # 定期的な中間保存\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    # デバッグ用：保存前のデータ確認\n",
        "                    saved_count = sum(1 for idx in target_indices[:i+1]\n",
        "                                    if idx < len(df) and is_processed(df.iloc[idx]))\n",
        "                    print(f\"\\n💾 中間保存: {saved_count}個の画像が完全に処理済み\")\n",
        "\n",
        "                    # メモリ使用量のログ\n",
        "                    memory_monitor.display_memory_status(\"💾 保存時\")\n",
        "\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"✅ 中間保存完了: {i + 1}/{len(target_indices)} 画像処理済み\")\n",
        "\n",
        "                    # === 最適化: より積極的なメモリクリーンアップ ===\n",
        "                    aggressive_memory_cleanup()\n",
        "\n",
        "                    # クリーンアップ後のメモリ使用量\n",
        "                    memory_monitor.display_memory_status(\"クリーンアップ後:\")\n",
        "                    if debug_mode:\n",
        "                        log_cuda_tensors(\"クリーンアップ後:\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # CUDA out of memoryエラーのチェック\n",
        "                if \"CUDA out of memory\" in str(e):\n",
        "                    print(f\"\\n\\n💥 CUDA out of memory エラーが発生しました！\")\n",
        "                    print(f\"エラー詳細: {str(e)}\")\n",
        "                    print(f\"現在の画像: {img_basename} (インデックス: {index})\")\n",
        "                    memory_monitor.display_memory_status(\"💥 エラー発生時\")\n",
        "                    cuda_error_occurred = True\n",
        "                    break\n",
        "                else:\n",
        "                    error_count += 1\n",
        "                    if error_count <= 5:\n",
        "                        print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "        # CUDA out of memoryエラーが発生したらループを抜ける\n",
        "        if cuda_error_occurred:\n",
        "            break\n",
        "\n",
        "    # 最終保存（エラーが発生した場合も含む）\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    # 最終的なメモリ使用量\n",
        "    memory_monitor.display_memory_status(\"🏁 処理終了時\")\n",
        "    if debug_mode:\n",
        "        log_cuda_tensors(\"最終状態:\")\n",
        "\n",
        "    if cuda_error_occurred:\n",
        "        print(f\"\\n\\n========== CUDA OUT OF MEMORY エラーで処理を中止しました ==========\")\n",
        "        print(f\"最後に処理しようとした画像: {img_basename} (インデックス: {index})\")\n",
        "        print(f\"処理済み画像数: {processed_count}\")\n",
        "        print(f\"成功したレイヤー処理: {successful_layers}個\")\n",
        "        print(f\"メモリクリーンアップ実行回数: {memory_cleanup_count}回\")\n",
        "        print(f\"残り未処理画像数: {len(target_indices) - i}\")\n",
        "        print(f\"結果を保存しました: {csv_path}\")\n",
        "        print(f\"\\n次回実行時は自動的に続きから処理されます。\")\n",
        "    else:\n",
        "        print(f\"\\n✨ 処理完了サマリー:\")\n",
        "        print(f\"├─ 処理した画像: {processed_count}個\")\n",
        "        print(f\"├─ 成功したレイヤー処理: {successful_layers}個\")\n",
        "        print(f\"├─ スキップした画像: {skipped_count}個\")\n",
        "        print(f\"├─ 見つからなかった画像: {not_found_count}個\")\n",
        "        print(f\"├─ エラーが発生した画像: {error_count}個\")\n",
        "        print(f\"└─ メモリクリーンアップ実行回数: {memory_cleanup_count}回\")\n",
        "\n",
        "        # エラーの詳細を表示\n",
        "        if error_details:\n",
        "            print(f\"\\n⚠️  エラーの詳細（最初の10件）:\")\n",
        "            for i, (key, error_msg) in enumerate(list(error_details.items())[:10]):\n",
        "                print(f\"  {i+1}. {key}: {error_msg[:100]}...\")\n",
        "\n",
        "        # 最終的な完全処理済み画像数を確認\n",
        "        fully_processed = sum(1 for idx, row in df.iterrows() if is_processed(row))\n",
        "        print(f\"\\n📊 完全に処理済みの画像数: {fully_processed}/{len(df)}\")\n",
        "\n",
        "        print(f\"\\n💾 結果を保存しました: {csv_path}\")\n",
        "\n",
        "# ========== 実行モード設定 ==========\n",
        "# 以下の変数を変更することで実行モードを切り替えできます\n",
        "\n",
        "# MODE = \"auto\"：未処理画像を自動検出して処理\n",
        "# MODE = \"manual\"：start_indexとend_indexで範囲を指定（処理済みはスキップ）\n",
        "# MODE = \"force\"：start_indexとend_indexで範囲を指定（処理済みも再処理）\n",
        "# MODE = \"debug\"：最初の未処理画像1枚のみ処理（デバッグ用）\n",
        "MODE = \"auto\"  # \"auto\", \"manual\", \"force\", \"debug\" のいずれかを指定\n",
        "\n",
        "# manualモードまたはforceモードの場合の範囲指定\n",
        "MANUAL_START_INDEX = 0\n",
        "MANUAL_END_INDEX = 10\n",
        "\n",
        "# ========== メイン実行部分 ==========\n",
        "# デバッグ関数：CSVファイルの状態を確認\n",
        "def check_csv_status(csv_path, threshold=0.5):\n",
        "    \"\"\"CSVファイルの処理状況を確認する関数\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    all_layers_cols = [\n",
        "        f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "        f\"AOI_{threshold}_layer24_m_0\",\n",
        "        f\"AOI_{threshold}_layer24_m_1\",\n",
        "        f\"AOI_{threshold}_layer24_m_2\"\n",
        "    ]\n",
        "\n",
        "    print(f\"CSVファイル: {csv_path}\")\n",
        "    print(f\"総行数: {len(df)}\")\n",
        "\n",
        "    # 各カラムの状態を確認\n",
        "    for col in all_layers_cols:\n",
        "        if col in df.columns:\n",
        "            non_null_count = df[col].notna().sum()\n",
        "            print(f\"  {col}: {non_null_count}/{len(df)} ({non_null_count/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"  {col}: カラムが存在しません\")\n",
        "\n",
        "    # 完全に処理済みの行数\n",
        "    fully_processed = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if all(pd.notna(row[col]) and row[col] is not None for col in all_layers_cols if col in df.columns):\n",
        "            fully_processed += 1\n",
        "\n",
        "    print(f\"\\n完全に処理済みの画像: {fully_processed}/{len(df)} ({fully_processed/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # 最初の5行のデータを表示\n",
        "    print(\"\\n最初の5行のAOI値:\")\n",
        "    for idx in range(min(5, len(df))):\n",
        "        print(f\"  行{idx}: \", end=\"\")\n",
        "        for col in all_layers_cols:\n",
        "            if col in df.columns:\n",
        "                val = df.iloc[idx][col]\n",
        "                if pd.notna(val):\n",
        "                    print(f\"{val:.4f} \", end=\"\")\n",
        "                else:\n",
        "                    print(\"NaN \", end=\"\")\n",
        "        print()\n",
        "\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"使用デバイス: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# バッチ処理モードの設定\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== バッチ処理モードで実行します ===\")\n",
        "\n",
        "    # 処理前のCSV状態を確認\n",
        "    print(\"\\n=== 処理前のCSV状態 ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # is_processed関数を定義（calculate_aoi関数外でも使えるように）\n",
        "    def is_processed(row):\n",
        "        \"\"\"すべてのAOIカラムに値が入っているかチェック\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # 未処理画像のインデックスを取得\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"すべての画像が処理済みです。\")\n",
        "    else:\n",
        "        print(f\"未処理画像数: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # バッチごとに処理\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nバッチ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"インデックス {batch_indices}\")\n",
        "\n",
        "            # バッチ内の画像をまとめて処理\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True, debug_mode=False)\n",
        "\n",
        "            # 処理完了後のクリーンアップ\n",
        "            aggressive_memory_cleanup()\n",
        "\n",
        "            print(f\"バッチ {i//BATCH_SIZE + 1} 完了\")\n",
        "\n",
        "            # バッチが完了したら、再度未処理画像を確認（中断された場合の対策）\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"残り未処理画像数: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== 全バッチ処理完了 ===\")\n",
        "\n",
        "    # 処理後のCSV状態を確認\n",
        "    print(\"\\n=== 処理後のCSV状態 ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "else:\n",
        "    print(\"=== 通常モードで実行します ===\")\n",
        "    print(f\"実行モード: {MODE}\")\n",
        "\n",
        "    # 処理前のCSV状態を確認\n",
        "    print(\"\\n=== 処理前のCSV状態 ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    if MODE == \"auto\":\n",
        "        # 自動モード（未処理画像を検出）\n",
        "        print(\"\\n未処理画像を自動検出して処理します...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"manual\":\n",
        "        # 手動モード（範囲指定、処理済みはスキップ）\n",
        "        print(f\"\\nインデックス {MANUAL_START_INDEX} から {MANUAL_END_INDEX-1} まで処理します（処理済みはスキップ）...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=True, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"force\":\n",
        "        # 強制再処理モード（範囲指定、処理済みも再処理）\n",
        "        print(f\"\\nインデックス {MANUAL_START_INDEX} から {MANUAL_END_INDEX-1} まで強制的に再処理します...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=MANUAL_START_INDEX, end_index=MANUAL_END_INDEX,\n",
        "                     skip_processed=False, debug_mode=False)\n",
        "\n",
        "    elif MODE == \"debug\":\n",
        "        # デバッグモード（最初の未処理画像1枚のみ処理）\n",
        "        print(\"\\nデバッグモード: 最初の未処理画像1枚のみ処理します...\")\n",
        "        # 未処理画像を探す\n",
        "        df_debug = pd.read_csv(csv_path)\n",
        "        debug_idx = None\n",
        "        for idx, row in df_debug.iterrows():\n",
        "            all_layers_cols = [\n",
        "                f\"AOI_{threshold}_layermodel_17_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_20_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layermodel_23_cv3_conv\",\n",
        "                f\"AOI_{threshold}_layer24_m_0\",\n",
        "                f\"AOI_{threshold}_layer24_m_1\",\n",
        "                f\"AOI_{threshold}_layer24_m_2\"\n",
        "            ]\n",
        "            if any(pd.isna(row[col]) or row[col] is None for col in all_layers_cols if col in df_debug.columns):\n",
        "                debug_idx = idx\n",
        "                break\n",
        "\n",
        "        if debug_idx is not None:\n",
        "            print(f\"デバッグ対象: インデックス {debug_idx}, 画像: {df_debug.iloc[debug_idx]['image_basename']}\")\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=debug_idx, end_index=debug_idx+1,\n",
        "                         skip_processed=False, debug_mode=True)\n",
        "        else:\n",
        "            print(\"未処理画像が見つかりませんでした。\")\n",
        "\n",
        "    else:\n",
        "        print(f\"無効なモード: {MODE}\")\n",
        "        print(\"MODE は 'auto', 'manual', 'force', 'debug' のいずれかを指定してください。\")\n",
        "\n",
        "    # 処理後のCSV状態を確認\n",
        "    print(\"\\n=== 処理後のCSV状態 ===\")\n",
        "    check_csv_status(csv_path, threshold)\n",
        "\n",
        "# === 最適化: 最終的なメモリクリーンアップ ===\n",
        "print(\"\\n=== 最終クリーンアップ ===\")\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 最終メモリ状態の表示\n",
        "    memory_monitor = GPUMemoryMonitor()\n",
        "    memory_monitor.display_memory_status(\"最終クリーンアップ後:\")\n",
        "\n",
        "    if MODE == \"debug\":\n",
        "        log_cuda_tensors(\"最終クリーンアップ後:\")\n",
        "\n",
        "# モデルの削除\n",
        "del model\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n処理が完了しました。\")"
      ],
      "metadata": {
        "id": "j0yjAERw1EL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VersionによるAOIの差を確認 (結論：変わりなし)"
      ],
      "metadata": {
        "id": "D_zPZJSgBnbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "\n",
        "# 警告の抑制\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning,\n",
        "                      message=\".*Using a non-full backward hook.*\")\n",
        "\n",
        "# YOLOv5の必要な関数をインポート\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポート\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# シリアライゼーションの設定\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# グローバル変数\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ========== ヘルパー関数 ==========\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    \"\"\"YOLOモデルから特定のレイヤーを見つける\"\"\"\n",
        "    hierarchy = layer_name.split(\"_\")\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "def set_model_gradients(model, layer_name, enable=True):\n",
        "    \"\"\"特定のレイヤーのみ勾配計算を有効化\"\"\"\n",
        "    # まず全体の勾配を無効化\n",
        "    for param in model.model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 特定のレイヤーのみ有効化\n",
        "    if enable:\n",
        "        target_layer = find_yolo_layer(model, layer_name)\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "    \"\"\"Area of Interest (AOI)を計算\"\"\"\n",
        "    total_intersect_pixels = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mask in masks:\n",
        "            # マスクをCPUで処理\n",
        "            mask_cpu = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "            del mask\n",
        "\n",
        "            mask_cpu = np.nan_to_num(mask_cpu, nan=0.0).astype(np.uint8)\n",
        "\n",
        "            # 閾値処理\n",
        "            binary_mask = mask_cpu >= (threshold * 255)\n",
        "            del mask_cpu\n",
        "\n",
        "            x1, y1, x2, y2 = bbox\n",
        "            mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "            intersect_pixels = np.sum(mask_bbox)\n",
        "            total_intersect_pixels += intersect_pixels\n",
        "\n",
        "            del mask_bbox, binary_mask\n",
        "\n",
        "    mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "    AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "    return AOI\n",
        "\n",
        "# ========== YOLOv5物体検出器クラス ==========\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # デフォルトでは勾配計算を無効化\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "                          \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
        "                          \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
        "                          \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
        "                          \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\",\n",
        "                          \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\",\n",
        "                          \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
        "                          \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
        "                          \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5\n",
        "        xc = prediction[..., 4] > conf_thres\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096\n",
        "        max_nms = 30000\n",
        "        time_limit = 10.0\n",
        "        redundant = True\n",
        "        multi_label &= nc > 1\n",
        "        merge = False\n",
        "\n",
        "        import time\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):\n",
        "            x = x[xc[xi]]\n",
        "            log_ = log_[xc[xi]]\n",
        "\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]\n",
        "                v[:, 4] = 1.0\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
        "\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None, agnostic=self.agnostic)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in range(4)]\n",
        "\n",
        "            for i, det in enumerate(prediction):\n",
        "                if len(det):\n",
        "                    det_cpu = det.cpu()\n",
        "                    del det\n",
        "\n",
        "                    for *xyxy, conf, cls in det_cpu:\n",
        "                        xyxy[0] = max(0, xyxy[0])\n",
        "                        xyxy[1] = max(0, xyxy[1])\n",
        "                        xyxy[2] = min(img_size, xyxy[2])\n",
        "                        xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                        bbox = Box.box2box(xyxy,\n",
        "                                           in_source=Box.BoxSource.Torch,\n",
        "                                           to_source=Box.BoxSource.Numpy,\n",
        "                                           return_int=True)\n",
        "                        self.boxes[i].append(bbox)\n",
        "                        self.confidences[i].append(round(conf.item(), 2))\n",
        "                        cls = int(cls.item())\n",
        "                        self.classes[i].append(cls)\n",
        "                        if self.names is not None:\n",
        "                            self.class_names[i].append(self.names[cls])\n",
        "                        else:\n",
        "                            self.class_names[i].append(cls)\n",
        "\n",
        "            del prediction\n",
        "\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        del img\n",
        "\n",
        "        resized_imgs = []\n",
        "        for im in im0:\n",
        "            resized_img = self.yolo_resize(im, new_shape=self.img_size)[0]\n",
        "            resized_imgs.append(resized_img)\n",
        "\n",
        "        img = np.array(resized_imgs)\n",
        "        del resized_imgs, im0\n",
        "\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# ========== バージョン1: 完全初期版（全体勾配 + 1回フォワード + retain_graph=True） ==========\n",
        "class YOLOV5GradCAM_V1_Original:\n",
        "    \"\"\"完全初期版：全体勾配有効化 + 1回のフォワードパス + retain_graph=True\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # 全体の勾配を有効化\n",
        "        for param in self.model.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # 1回のフォワードパス\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # 各クラスに対して同じlogitsを使用\n",
        "        for i, (cls, cls_name) in enumerate(zip(preds[1][0], preds[2][0])):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            if class_idx:\n",
        "                score = logits[0][0][cls]\n",
        "            else:\n",
        "                score = logits[0][0].max()\n",
        "\n",
        "            # retain_graph=Trueを使用（最後のクラス以外）\n",
        "            if i < len(preds[1][0]) - 1:\n",
        "                score.backward(retain_graph=True)\n",
        "            else:\n",
        "                score.backward(retain_graph=False)\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== バージョン2: 全体勾配 + 各クラスフォワード + retain_graph=False ==========\n",
        "class YOLOV5GradCAM_V2_AllGrad:\n",
        "    \"\"\"全体勾配有効化 + 各クラスごとに新しいフォワードパス + retain_graph=False\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # 全体の勾配を有効化\n",
        "        for param in self.model.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # 最初のフォワードパスでtop3を取得\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # 各クラスごとに新しいフォワードパス\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            input_img_clone = input_img.clone()\n",
        "            _, new_logits = self.model(input_img_clone)\n",
        "            del input_img_clone\n",
        "\n",
        "            if class_idx:\n",
        "                score = new_logits[0][0][cls]\n",
        "            else:\n",
        "                score = new_logits[0][0].max()\n",
        "\n",
        "            # retain_graph=False\n",
        "            score.backward()\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                del score, new_logits\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            del score, new_logits\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "            # メモリクリーンアップ\n",
        "            if \"value\" in self.gradients:\n",
        "                del self.gradients[\"value\"]\n",
        "            if \"value\" in self.activations:\n",
        "                del self.activations[\"value\"]\n",
        "\n",
        "            del gradients, activations\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== バージョン3: 選択的勾配 + 各クラスフォワード + retain_graph=False ==========\n",
        "class YOLOV5GradCAM_V3_Optimized:\n",
        "    \"\"\"選択的勾配有効化 + 各クラスごとに新しいフォワードパス + retain_graph=False\"\"\"\n",
        "\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "        self.layer_name = layer_name\n",
        "\n",
        "        self.forward_handle = None\n",
        "        self.backward_handle = None\n",
        "        self.target_layer = None\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0].detach().clone()\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output.detach().clone()\n",
        "            return None\n",
        "\n",
        "        self.target_layer = find_yolo_layer(self.model, layer_name)\n",
        "\n",
        "        # 選択的勾配有効化\n",
        "        set_model_gradients(self.model, layer_name, enable=True)\n",
        "\n",
        "        self.forward_handle = self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.backward_handle = self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def cleanup(self):\n",
        "        if self.forward_handle is not None:\n",
        "            self.forward_handle.remove()\n",
        "            self.forward_handle = None\n",
        "\n",
        "        if self.backward_handle is not None:\n",
        "            self.backward_handle.remove()\n",
        "            self.backward_handle = None\n",
        "\n",
        "        self.gradients.clear()\n",
        "        self.activations.clear()\n",
        "\n",
        "        # レイヤーの勾配計算を無効化\n",
        "        set_model_gradients(self.model, self.layer_name, enable=False)\n",
        "\n",
        "        self.target_layer = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.cleanup()\n",
        "        return False\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # 最初のフォワードパスでtop3を取得\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "            if top3_indices.numel() > 0:\n",
        "                preds[1][0] = top3_indices.tolist()[0]\n",
        "                preds[2][0] = [self.model.names[i] for i in preds[1][0]]\n",
        "                self.cls_names = preds[2][0]\n",
        "            else:\n",
        "                self.cls_names = []\n",
        "\n",
        "            del top3_indices\n",
        "\n",
        "        # 各クラスごとに新しいフォワードパス\n",
        "        for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            input_img_clone = input_img.clone()\n",
        "            _, new_logits = self.model(input_img_clone)\n",
        "            del input_img_clone\n",
        "\n",
        "            if class_idx:\n",
        "                score = new_logits[0][0][cls]\n",
        "            else:\n",
        "                score = new_logits[0][0].max()\n",
        "\n",
        "            # retain_graph=False\n",
        "            score.backward()\n",
        "\n",
        "            if \"value\" not in self.gradients or \"value\" not in self.activations:\n",
        "                print(f\"[WARNING] No gradients or activations for class {cls_name}\")\n",
        "                del score, new_logits\n",
        "                continue\n",
        "\n",
        "            gradients = self.gradients[\"value\"]\n",
        "            activations = self.activations[\"value\"]\n",
        "            b, k, u, v = gradients.size()\n",
        "\n",
        "            if self.method == \"gradcam\":\n",
        "                weights = self._gradcam_weights(gradients, b, k)\n",
        "            elif self.method == \"gradcampp\":\n",
        "                weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "            del score, new_logits\n",
        "\n",
        "            with torch.no_grad():\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                del weights\n",
        "\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "\n",
        "                del saliency_map_min, saliency_map_max\n",
        "\n",
        "            saliency_maps.append(saliency_map)\n",
        "\n",
        "            # メモリクリーンアップ\n",
        "            if \"value\" in self.gradients:\n",
        "                del self.gradients[\"value\"]\n",
        "            if \"value\" in self.activations:\n",
        "                del self.activations[\"value\"]\n",
        "\n",
        "            del gradients, activations\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        del alpha\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "        del alpha_num, alpha_denom\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "        del relu_grad, alpha\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# ========== 3つのバージョンを比較する関数 ==========\n",
        "def compare_all_versions(image_path, model, layer_names, threshold=0.5):\n",
        "    \"\"\"3つのバージョンを全て比較\"\"\"\n",
        "\n",
        "    print(f\"\\n画像を読み込み中: {image_path}\")\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"画像サイズ: {img.shape}\")\n",
        "\n",
        "    # 前処理\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    print(f\"前処理後のテンソルサイズ: {torch_img.shape}\")\n",
        "\n",
        "    # まず物体検出を確認\n",
        "    with torch.no_grad():\n",
        "        [boxes_test, _, _, _], _ = model(torch_img.clone())\n",
        "        if len(boxes_test[0]) == 0:\n",
        "            print(\"物体が検出されませんでした。\")\n",
        "            return None\n",
        "        print(f\"検出された物体数: {len(boxes_test[0])}\")\n",
        "        print(f\"最初の物体のバウンディングボックス: {boxes_test[0][0]}\")\n",
        "\n",
        "    # 結果を格納する辞書\n",
        "    results = {\n",
        "        'layer_name': [],\n",
        "        'aoi_v1_original': [],  # 完全初期版\n",
        "        'aoi_v2_allgrad': [],   # 全体勾配 + 各クラスフォワード\n",
        "        'aoi_v3_optimized': [], # 選択的勾配 + 各クラスフォワード\n",
        "        'diff_v1_v2': [],       # V1とV2の差\n",
        "        'diff_v2_v3': [],       # V2とV3の差\n",
        "        'diff_v1_v3': []        # V1とV3の差\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== 画像: {os.path.basename(image_path)} の比較 ===\")\n",
        "    print(\"-\" * 120)\n",
        "    print(f\"{'レイヤー名':<25} {'V1:完全初期版':>15} {'V2:全体勾配':>15} {'V3:選択的勾配':>15} {'V1-V2差':>12} {'V2-V3差':>12} {'V1-V3差':>12}\")\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    for layer_name in layer_names:\n",
        "        print(f\"\\n処理中のレイヤー: {layer_name}\")\n",
        "\n",
        "        # === V1: 完全初期版 ===\n",
        "        print(\"  V1: 完全初期版（全体勾配 + 1回フォワード + retain_graph=True）...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V1_Original(model=model, layer_name=layer_name,\n",
        "                                          img_size=(640, 640), method=\"gradcampp\") as gradcam_v1:\n",
        "                masks_v1, _, preds_v1, _ = gradcam_v1(torch_img.clone())\n",
        "                boxes_v1 = preds_v1[0]\n",
        "\n",
        "                if len(masks_v1) > 0 and len(boxes_v1) > 0 and len(boxes_v1[0]) > 0:\n",
        "                    mask_v1 = masks_v1[0]\n",
        "                    bbox_v1 = boxes_v1[0][0]\n",
        "                    aoi_v1 = get_aoi(bbox_v1, [mask_v1], threshold)\n",
        "                    print(f\"    AOI値: {aoi_v1}\")\n",
        "                else:\n",
        "                    aoi_v1 = None\n",
        "                    print(\"    検出なし\")\n",
        "        except Exception as e:\n",
        "            print(f\"    エラー: {str(e)}\")\n",
        "            aoi_v1 = None\n",
        "\n",
        "        # === V2: 全体勾配 + 各クラスフォワード ===\n",
        "        print(\"  V2: 全体勾配 + 各クラスフォワード + retain_graph=False...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V2_AllGrad(model=model, layer_name=layer_name,\n",
        "                                          img_size=(640, 640), method=\"gradcampp\") as gradcam_v2:\n",
        "                masks_v2, _, preds_v2, _ = gradcam_v2(torch_img.clone())\n",
        "                boxes_v2 = preds_v2[0]\n",
        "\n",
        "                if len(masks_v2) > 0 and len(boxes_v2) > 0 and len(boxes_v2[0]) > 0:\n",
        "                    mask_v2 = masks_v2[0]\n",
        "                    bbox_v2 = boxes_v2[0][0]\n",
        "                    aoi_v2 = get_aoi(bbox_v2, [mask_v2], threshold)\n",
        "                    print(f\"    AOI値: {aoi_v2}\")\n",
        "                else:\n",
        "                    aoi_v2 = None\n",
        "                    print(\"    検出なし\")\n",
        "        except Exception as e:\n",
        "            print(f\"    エラー: {str(e)}\")\n",
        "            aoi_v2 = None\n",
        "\n",
        "        # === V3: 選択的勾配 + 各クラスフォワード ===\n",
        "        print(\"  V3: 選択的勾配 + 各クラスフォワード + retain_graph=False...\")\n",
        "        try:\n",
        "            with YOLOV5GradCAM_V3_Optimized(model=model, layer_name=layer_name,\n",
        "                                            img_size=(640, 640), method=\"gradcampp\") as gradcam_v3:\n",
        "                masks_v3, _, preds_v3, _ = gradcam_v3(torch_img.clone())\n",
        "                boxes_v3 = preds_v3[0]\n",
        "\n",
        "                if len(masks_v3) > 0 and len(boxes_v3) > 0 and len(boxes_v3[0]) > 0:\n",
        "                    mask_v3 = masks_v3[0]\n",
        "                    bbox_v3 = boxes_v3[0][0]\n",
        "                    aoi_v3 = get_aoi(bbox_v3, [mask_v3], threshold)\n",
        "                    print(f\"    AOI値: {aoi_v3}\")\n",
        "                else:\n",
        "                    aoi_v3 = None\n",
        "                    print(\"    検出なし\")\n",
        "        except Exception as e:\n",
        "            print(f\"    エラー: {str(e)}\")\n",
        "            aoi_v3 = None\n",
        "\n",
        "        # 結果の記録と表示\n",
        "        if aoi_v1 is not None and aoi_v2 is not None and aoi_v3 is not None:\n",
        "            diff_v1_v2 = aoi_v2 - aoi_v1\n",
        "            diff_v2_v3 = aoi_v3 - aoi_v2\n",
        "            diff_v1_v3 = aoi_v3 - aoi_v1\n",
        "\n",
        "            results['layer_name'].append(layer_name)\n",
        "            results['aoi_v1_original'].append(aoi_v1)\n",
        "            results['aoi_v2_allgrad'].append(aoi_v2)\n",
        "            results['aoi_v3_optimized'].append(aoi_v3)\n",
        "            results['diff_v1_v2'].append(diff_v1_v2)\n",
        "            results['diff_v2_v3'].append(diff_v2_v3)\n",
        "            results['diff_v1_v3'].append(diff_v1_v3)\n",
        "\n",
        "            print(f\"{layer_name:<25} {aoi_v1:>15.6f} {aoi_v2:>15.6f} {aoi_v3:>15.6f} \"\n",
        "                  f\"{diff_v1_v2:>+12.6f} {diff_v2_v3:>+12.6f} {diff_v1_v3:>+12.6f}\")\n",
        "        else:\n",
        "            v1_str = f\"{aoi_v1:.6f}\" if aoi_v1 is not None else \"検出なし\"\n",
        "            v2_str = f\"{aoi_v2:.6f}\" if aoi_v2 is not None else \"検出なし\"\n",
        "            v3_str = f\"{aoi_v3:.6f}\" if aoi_v3 is not None else \"検出なし\"\n",
        "            print(f\"{layer_name:<25} {v1_str:>15} {v2_str:>15} {v3_str:>15} {'N/A':>12} {'N/A':>12} {'N/A':>12}\")\n",
        "\n",
        "        # メモリクリーンアップ\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print(\"-\" * 120)\n",
        "\n",
        "    # 統計情報の表示\n",
        "    if results['diff_v1_v2']:\n",
        "        print(f\"\\n統計情報:\")\n",
        "        print(f\"  V1→V2の平均差分: {np.mean(results['diff_v1_v2']):+.6f}\")\n",
        "        print(f\"  V2→V3の平均差分: {np.mean(results['diff_v2_v3']):+.6f}\")\n",
        "        print(f\"  V1→V3の平均差分: {np.mean(results['diff_v1_v3']):+.6f}\")\n",
        "\n",
        "        # 視覚化\n",
        "        if len(results['layer_name']) > 0:\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # AOI値の比較\n",
        "            plt.subplot(2, 2, 1)\n",
        "            x = np.arange(len(results['layer_name']))\n",
        "            width = 0.25\n",
        "\n",
        "            plt.bar(x - width, results['aoi_v1_original'], width, label='V1:完全初期版', alpha=0.8)\n",
        "            plt.bar(x, results['aoi_v2_allgrad'], width, label='V2:全体勾配', alpha=0.8)\n",
        "            plt.bar(x + width, results['aoi_v3_optimized'], width, label='V3:選択的勾配', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('レイヤー')\n",
        "            plt.ylabel('AOI値')\n",
        "            plt.title('各バージョンのAOI値比較')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # 差分の比較\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.plot(x, results['diff_v1_v2'], 'o-', label='V1→V2', alpha=0.8)\n",
        "            plt.plot(x, results['diff_v2_v3'], 's-', label='V2→V3', alpha=0.8)\n",
        "            plt.plot(x, results['diff_v1_v3'], '^-', label='V1→V3', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('レイヤー')\n",
        "            plt.ylabel('差分')\n",
        "            plt.title('バージョン間の差分')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # 相対的な差分（%）\n",
        "            plt.subplot(2, 2, 3)\n",
        "            rel_diff_v1_v2 = [(d/v1)*100 if v1 != 0 else 0 for d, v1 in zip(results['diff_v1_v2'], results['aoi_v1_original'])]\n",
        "            rel_diff_v2_v3 = [(d/v2)*100 if v2 != 0 else 0 for d, v2 in zip(results['diff_v2_v3'], results['aoi_v2_allgrad'])]\n",
        "            rel_diff_v1_v3 = [(d/v1)*100 if v1 != 0 else 0 for d, v1 in zip(results['diff_v1_v3'], results['aoi_v1_original'])]\n",
        "\n",
        "            plt.bar(x - width/2, rel_diff_v1_v2, width, label='V1→V2', alpha=0.8)\n",
        "            plt.bar(x + width/2, rel_diff_v1_v3, width, label='V1→V3', alpha=0.8)\n",
        "\n",
        "            plt.xlabel('レイヤー')\n",
        "            plt.ylabel('相対差分 (%)')\n",
        "            plt.title('V1からの相対的な変化')\n",
        "            plt.xticks(x, [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # ヒートマップ\n",
        "            plt.subplot(2, 2, 4)\n",
        "            data = np.array([results['aoi_v1_original'],\n",
        "                            results['aoi_v2_allgrad'],\n",
        "                            results['aoi_v3_optimized']])\n",
        "            im = plt.imshow(data, aspect='auto', cmap='viridis')\n",
        "            plt.colorbar(im, label='AOI値')\n",
        "            plt.yticks([0, 1, 2], ['V1:完全初期版', 'V2:全体勾配', 'V3:選択的勾配'])\n",
        "            plt.xticks(range(len(results['layer_name'])),\n",
        "                      [name.replace('model_', '') for name in results['layer_name']], rotation=45)\n",
        "            plt.title('AOI値のヒートマップ')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('gradcam_version_comparison.png', dpi=300, bbox_inches='tight')\n",
        "            print(\"\\n比較グラフを 'gradcam_version_comparison.png' として保存しました。\")\n",
        "    else:\n",
        "        print(\"\\n有効な結果が得られませんでした。\")\n",
        "\n",
        "    # 結果をDataFrameとして返す\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ========== メイン実行部分 ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # 作業ディレクトリの設定\n",
        "    os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "    # パスの設定\n",
        "    folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "    csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "    model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "\n",
        "    # 設定\n",
        "    threshold = 0.5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    input_size = (640, 640)\n",
        "    names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "    print(f\"使用デバイス: {device}\")\n",
        "    if device == \"cuda\":\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # モデルのロード\n",
        "    print(\"\\nモデルをロード中...\")\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "    # 比較するレイヤー\n",
        "    target_layers = [\n",
        "        \"model_17_cv3_conv\",\n",
        "        \"model_20_cv3_conv\",\n",
        "        \"model_23_cv3_conv\",\n",
        "        \"model_24_m_0\",\n",
        "        \"model_24_m_1\",\n",
        "        \"model_24_m_2\"\n",
        "    ]\n",
        "\n",
        "    # CSVから最初の画像を自動選択\n",
        "    print(\"\\nCSVファイルから画像を選択中...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    test_image_path = None\n",
        "\n",
        "    for idx in range(min(10, len(df))):\n",
        "        row = df.iloc[idx]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            continue\n",
        "\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                test_image_path = test_path\n",
        "                break\n",
        "\n",
        "        if test_image_path:\n",
        "            break\n",
        "\n",
        "    if test_image_path:\n",
        "        print(f\"テスト画像: {test_image_path}\")\n",
        "\n",
        "        # 3つのバージョンの比較実行\n",
        "        results_df = compare_all_versions(test_image_path, model, target_layers, threshold)\n",
        "\n",
        "        if results_df is not None and not results_df.empty:\n",
        "            results_df.to_csv('gradcam_version_comparison_results.csv', index=False)\n",
        "            print(\"\\n詳細な結果を 'gradcam_version_comparison_results.csv' として保存しました。\")\n",
        "\n",
        "            # 結果の要約を表示\n",
        "            print(\"\\n=== 結果の要約 ===\")\n",
        "            print(results_df)\n",
        "\n",
        "            # 各要素の影響を分析\n",
        "            print(\"\\n=== 各要素の影響分析 ===\")\n",
        "            print(\"1. retain_graph=True → False の影響（V1→V2）:\")\n",
        "            print(f\"   平均差分: {results_df['diff_v1_v2'].mean():+.6f}\")\n",
        "            print(\"\\n2. 全体勾配 → 選択的勾配 の影響（V2→V3）:\")\n",
        "            print(f\"   平均差分: {results_df['diff_v2_v3'].mean():+.6f}\")\n",
        "            print(\"\\n3. 全体的な最適化の影響（V1→V3）:\")\n",
        "            print(f\"   平均差分: {results_df['diff_v1_v3'].mean():+.6f}\")\n",
        "        else:\n",
        "            print(\"\\n比較結果が得られませんでした。\")\n",
        "    else:\n",
        "        print(\"テスト画像が見つかりませんでした。\")\n",
        "\n",
        "    # クリーンアップ\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n処理が完了しました。\")"
      ],
      "metadata": {
        "id": "csH4cVc840Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**上記スクリプトのための保存用csvファイル作成**"
      ],
      "metadata": {
        "id": "y52nNVm4SrDR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up7ykhgI5l3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338fb2c4-a6a2-4bdb-875e-fdf60a580472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "警告: CSVファイルが既に存在します: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n",
            "上書きしますか？ (y/n): n\n",
            "処理を中止しました。\n",
            "\n",
            "作成されたDataFrame:\n",
            "                     AOI_0.5_layermodel_17_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_20_cv3_conv  \\\n",
            "image_basename                                        \n",
            "[apac]FKS_130_R_slit                           None   \n",
            "[apac]FKS_238_L_slit                           None   \n",
            "[apac]FKS_382_L_slit                           None   \n",
            "[apac]TKB_001_R_slit                           None   \n",
            "[apac]fko0074                                  None   \n",
            "\n",
            "                     AOI_0.5_layermodel_23_cv3_conv AOI_0.5_layer24_m_0  \\\n",
            "image_basename                                                            \n",
            "[apac]FKS_130_R_slit                           None                None   \n",
            "[apac]FKS_238_L_slit                           None                None   \n",
            "[apac]FKS_382_L_slit                           None                None   \n",
            "[apac]TKB_001_R_slit                           None                None   \n",
            "[apac]fko0074                                  None                None   \n",
            "\n",
            "                     AOI_0.5_layer24_m_1 AOI_0.5_layer24_m_2  \n",
            "image_basename                                                \n",
            "[apac]FKS_130_R_slit                None                None  \n",
            "[apac]FKS_238_L_slit                None                None  \n",
            "[apac]FKS_382_L_slit                None                None  \n",
            "[apac]TKB_001_R_slit                None                None  \n",
            "[apac]fko0074                       None                None  \n",
            "\n",
            "総画像数: 1039\n",
            "\n",
            "CSVファイルが正常に保存されました: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "###これを押すとcsvが更新されてしまうので注意！！！\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ディレクトリとファイルパス\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "output_file_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "# CSVファイルが既に存在するかチェック\n",
        "if os.path.exists(output_file_path):\n",
        "    print(f\"警告: CSVファイルが既に存在します: {output_file_path}\")\n",
        "    response = input(\"上書きしますか？ (y/n): \").lower()\n",
        "\n",
        "    if response != 'y':\n",
        "        print(\"処理を中止しました。\")\n",
        "        exit()\n",
        "    else:\n",
        "        print(\"既存のファイルを上書きします。\")\n",
        "\n",
        "# 画像ディレクトリが存在するかチェック\n",
        "if not os.path.exists(image_dir):\n",
        "    print(f\"エラー: 画像ディレクトリが見つかりません: {image_dir}\")\n",
        "    exit()\n",
        "\n",
        "# 画像ディレクトリ内のファイルを取得（一般的な画像拡張子をフィルタリング）\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
        "image_files = []\n",
        "\n",
        "# ディレクトリ内のすべてのファイルをチェック\n",
        "try:\n",
        "    for file in os.listdir(image_dir):\n",
        "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
        "            # basenameを取得（拡張子なし）\n",
        "            basename = os.path.splitext(file)[0]\n",
        "            image_files.append(basename)\n",
        "except Exception as e:\n",
        "    print(f\"エラー: 画像ディレクトリの読み込みに失敗しました: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 画像ファイルが見つからない場合の処理\n",
        "if not image_files:\n",
        "    print(f\"警告: 画像ファイルが見つかりませんでした。\")\n",
        "    print(f\"対象ディレクトリ: {image_dir}\")\n",
        "    print(f\"対象拡張子: {', '.join(image_extensions)}\")\n",
        "    exit()\n",
        "\n",
        "# ソート（必要に応じて）\n",
        "image_files.sort()\n",
        "\n",
        "# DataFrameを作成（basenameをインデックスとして）\n",
        "df = pd.DataFrame(index=image_files)\n",
        "df.index.name = 'image_basename'  # インデックス名を設定\n",
        "\n",
        "# 例としてthresholdの値を設定\n",
        "threshold = 0.5\n",
        "\n",
        "# 指定されたレイヤー (originalはconvでなくact??)\n",
        "layers = ['model_17_cv3_conv', 'model_20_cv3_conv', 'model_23_cv3_conv', '24_m_0', '24_m_1', '24_m_2']\n",
        "\n",
        "# 各レイヤーに対して新しい列を作成\n",
        "for layer in layers:\n",
        "    df[f'AOI_{threshold}_layer{layer}'] = None  # 初期値をNoneに設定\n",
        "\n",
        "# DataFrameの最初の数行を表示\n",
        "print(\"\\n作成されたDataFrame:\")\n",
        "print(df.head())\n",
        "print(f\"\\n総画像数: {len(df)}\")\n",
        "\n",
        "# CSVファイルとして保存\n",
        "try:\n",
        "    df.to_csv(output_file_path, index=True)\n",
        "    print(f\"\\nCSVファイルが正常に保存されました: {output_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"エラー: CSVファイルの保存に失敗しました: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tT3myDD_dWHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Expert's annotation**"
      ],
      "metadata": {
        "id": "gIdR9H0J2YjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "metadata": {
        "id": "yPKZg9H8RXow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbbd809-375b-42fb-97c4-0c851d51e8c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: /gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Imagesと/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Masksの中の画像ファイルのbasename(拡張子は違うことあり）を比較＞\n",
        "\n",
        "import os\n",
        "\n",
        "# ディレクトリのパス\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "mask_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Masks\"\n",
        "\n",
        "# 画像ファイルのリスト（拡張子なしのbasename）\n",
        "image_files = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "\n",
        "# マスクファイルのリスト（拡張子なしのbasename）\n",
        "mask_files = [os.path.splitext(f)[0] for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "\n",
        "# 両方のリストに存在するbasenameを特定\n",
        "common_files = sorted(list(set(image_files) & set(mask_files)))\n",
        "\n",
        "# どちらか一方にしか存在しないbasenameを特定\n",
        "image_only = sorted(list(set(image_files) - set(mask_files)))\n",
        "mask_only = sorted(list(set(mask_files) - set(image_files)))\n",
        "\n",
        "print(f\"画像の総数: {len(image_files)}\")\n",
        "print(f\"マスクの総数: {len(mask_files)}\")\n",
        "print(f\"両方に存在するファイル数: {len(common_files)}\")\n",
        "print(f\"画像にのみ存在するファイル数: {len(image_only)}\")\n",
        "print(f\"マスクにのみ存在するファイル数: {len(mask_only)}\")\n",
        "\n",
        "# 結果を表示（オプション）\n",
        "print(\"\\n両方に存在するファイル (先頭10件):\")\n",
        "for f in common_files[:10]:\n",
        "    print(f)\n",
        "\n",
        "print(\"\\n画像にのみ存在するファイル:\")\n",
        "for f in image_only[0:]:\n",
        "    print(f)\n",
        "\n",
        "print(\"\\nマスクにのみ存在するファイル:\")\n",
        "for f in mask_only[0:]:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "U6ACrQR_RXrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390ca302-cfb7-4c58-974b-2fb59e1a1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "画像の総数: 1039\n",
            "マスクの総数: 1007\n",
            "両方に存在するファイル数: 996\n",
            "画像にのみ存在するファイル数: 43\n",
            "マスクにのみ存在するファイル数: 10\n",
            "\n",
            "両方に存在するファイル (先頭10件):\n",
            "[apac]FKS_130_R_slit\n",
            "[apac]FKS_238_L_slit\n",
            "[apac]FKS_382_L_slit\n",
            "[apac]TKB_001_R_slit\n",
            "[apac]fko0074\n",
            "[apac]kei0903_1\n",
            "[apac]kei0934\n",
            "[apac]ttr_0366\n",
            "[apac]ttr_0369_01\n",
            "[apac]ttr_0378\n",
            "\n",
            "画像にのみ存在するファイル:\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]モーレン_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]細菌_KNZ5048\n",
            "[infection]細菌_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[scar]myt 0024\n",
            "[scar]myt 0035\n",
            "[scar]myt 0036\n",
            "[scar]myt 0040\n",
            "[scar]myt 0049\n",
            "[scar]myt 0052\n",
            "[scar]myt 0073\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]翼状片_wky7047\n",
            "\n",
            "マスクにのみ存在するファイル:\n",
            "[infection]細菌_fko0032_01-1\n",
            "[infection]細菌_fko0032_01-2\n",
            "[scar]FKS_258_L_slit-1\n",
            "[scar]myt_0024\n",
            "[scar]myt_0035\n",
            "[scar]myt_0036\n",
            "[scar]myt_0040\n",
            "[scar]myt_0049\n",
            "[scar]myt_0052\n",
            "[scar]myt_0073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ===== 設定 =====\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Masks\"\n",
        "dry_run   = True        # True: 確認だけ, False: 実際にリネーム\n",
        "# ===============\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"比較用キーを生成（大小区別せず, 空白/アンダースコア/ハイフンを除外, 末尾の -1/-2 も除外）\"\"\"\n",
        "    # 末尾の -数字, _数字 を削除\n",
        "    name = re.sub(r'[-_]\\d+$', '', name)\n",
        "    # 空白, アンダースコア, ハイフン を消して小文字化\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()\n",
        "\n",
        "# Images 側: 正規化 → オリジナル名\n",
        "image_key2name = {normalize(os.path.splitext(f)[0]): os.path.splitext(f)[0]\n",
        "                  for f in os.listdir(image_dir)\n",
        "                  if os.path.isfile(os.path.join(image_dir, f))}\n",
        "\n",
        "# Masks 側: 同じく正規化 → [ (basename, 拡張子) , ... ]\n",
        "mask_key2names = defaultdict(list)\n",
        "for f in os.listdir(mask_dir):\n",
        "    if os.path.isfile(os.path.join(mask_dir, f)):\n",
        "        base, ext = os.path.splitext(f)\n",
        "        mask_key2names[normalize(base)].append((base, ext))\n",
        "\n",
        "# ① 1:1 で対応が確定するものだけ自動リネーム候補に\n",
        "rename_plan = []\n",
        "ambiguous   = []      # 同じキーで image と mask が複数ある等\n",
        "no_match    = []      # mask 側に対応する image が無い\n",
        "\n",
        "for key, mask_list in mask_key2names.items():\n",
        "    if key in image_key2name and len(mask_list) == 1:\n",
        "        img_base = image_key2name[key]\n",
        "        mask_base, ext = mask_list[0]\n",
        "        if img_base != mask_base:  # すでに一致していなければ\n",
        "            src = os.path.join(mask_dir, mask_base + ext)\n",
        "            dst = os.path.join(mask_dir, img_base + ext)\n",
        "            rename_plan.append((src, dst))\n",
        "    elif key in image_key2name:\n",
        "        ambiguous.append(mask_list)   # 同キーで mask ファイルが複数\n",
        "    else:\n",
        "        no_match.extend(mask_list)    # そもそも image 側にキー無し\n",
        "\n",
        "# ---- 結果を確認 ----\n",
        "print(f\"自動リネーム候補: {len(rename_plan)} 件\")\n",
        "for src, dst in rename_plan[:20]:\n",
        "    print(f\"  {os.path.basename(src)}  →  {os.path.basename(dst)}\")\n",
        "\n",
        "print(f\"\\nあいまい (手動確認推奨): {len(ambiguous)} 件\")\n",
        "print(f\"マッチしない mask:      {len(no_match)} 件\")\n",
        "\n",
        "# ---- 実行フェーズ ----\n",
        "if not dry_run:\n",
        "    for src, dst in rename_plan:\n",
        "        os.rename(src, dst)\n",
        "    print(f\"\\n{len(rename_plan)} 件リネーム完了\")\n",
        "else:\n",
        "    print(\"\\n※ dry_run=True のためリネームせず終了。チェック後 dry_run=False にして再実行してください。\")\n"
      ],
      "metadata": {
        "id": "20O4wz0A7goa",
        "outputId": "24dfd334-6e1c-4d0c-e522-786a9ad9ca47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "自動リネーム候補: 0 件\n",
            "\n",
            "あいまい (手動確認推奨): 5 件\n",
            "マッチしない mask:      9 件\n",
            "\n",
            "※ dry_run=True のためリネームせず終了。チェック後 dry_run=False にして再実行してください。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ====== 設定 ======\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Masks\"\n",
        "dry_run   = True        # 確認だけなら True、実際にリネームするなら False\n",
        "# ===================\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"比較キーを生成\n",
        "       ・末尾の -1/_2 など 1–2 桁番号だけ削除\n",
        "       ・空白・アンダーバー・ハイフン除去\n",
        "       ・小文字化\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[-_]\\d{1,2}$', '', name)\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()\n",
        "\n",
        "# ── 画像側 ──\n",
        "img_bases = [os.path.splitext(f)[0] for f in os.listdir(image_dir)\n",
        "             if os.path.isfile(os.path.join(image_dir, f))]\n",
        "img_key2base = {normalize(b): b for b in img_bases}\n",
        "\n",
        "# ── マスク側 ──\n",
        "mask_files = [f for f in os.listdir(mask_dir)\n",
        "              if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "mask_key2info = defaultdict(list)  # key → [(base, ext, full_path), …]\n",
        "for f in mask_files:\n",
        "    base, ext = os.path.splitext(f)\n",
        "    key = normalize(base)\n",
        "    mask_key2info[key].append((base, ext, os.path.join(mask_dir, f)))\n",
        "\n",
        "# ── リネーム計画 ──\n",
        "rename_plan, ambiguous, no_match = [], [], []\n",
        "for key, masks in mask_key2info.items():\n",
        "    if key in img_key2base and len(masks) == 1:\n",
        "        img_base = img_key2base[key]\n",
        "        m_base, ext, src = masks[0]\n",
        "        if m_base != img_base:\n",
        "            dst = os.path.join(mask_dir, img_base + ext)\n",
        "            rename_plan.append((src, dst))\n",
        "    elif key in img_key2base:\n",
        "        ambiguous.extend(masks)\n",
        "    else:\n",
        "        no_match.extend(masks)\n",
        "\n",
        "# ── dry-run 出力 ──\n",
        "print(f\"リネーム予定: {len(rename_plan)} 件\")\n",
        "for s, d in rename_plan[:20]:\n",
        "    print(\"  \", os.path.basename(s), \"→\", os.path.basename(d))\n",
        "\n",
        "print(f\"\\n曖昧 (手動確認): {len(ambiguous)} 件\")\n",
        "print(f\"対応する画像なし: {len(no_match)} 件\")\n",
        "\n",
        "# ── 実行 ──\n",
        "if not dry_run:\n",
        "    for src, dst in rename_plan:\n",
        "        try:\n",
        "            os.rename(src, dst)\n",
        "        except FileExistsError:\n",
        "            print(\"  * 既に存在: \", os.path.basename(dst))\n",
        "    print(\"\\nリネーム完了\")\n",
        "\n",
        "# ── リネーム後の完全一致チェック ──\n",
        "# （dry_run でも現状を確認できる）\n",
        "img_keys  = set(img_key2base)\n",
        "mask_keys = set(normalize(os.path.splitext(f)[0]) for f in\n",
        "                os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f)))\n",
        "\n",
        "unmatched_imgs  = sorted(img_key2base[k] for k in img_keys - mask_keys)\n",
        "unmatched_masks = sorted(os.path.splitext(f)[0] for f in mask_files\n",
        "                         if normalize(os.path.splitext(f)[0]) not in img_keys)\n",
        "\n",
        "print(\"\\n====== 最終サマリ ======\")\n",
        "print(\"画像にのみ存在:\", len(unmatched_imgs))\n",
        "print(\"マスクにのみ存在:\", len(unmatched_masks))\n",
        "if unmatched_imgs:\n",
        "    print(\"\\n▼画像のみ\")\n",
        "    for b in unmatched_imgs:\n",
        "        print(b)\n",
        "if unmatched_masks:\n",
        "    print(\"\\n▼マスクのみ\")\n",
        "    for b in unmatched_masks:\n",
        "        print(b)\n"
      ],
      "metadata": {
        "id": "LOkhhHotEFJo",
        "outputId": "6860976c-1b4b-4a3e-a7c0-c213f756d55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "リネーム予定: 7 件\n",
            "   [scar]myt_0073.png → [scar]myt 0073.png\n",
            "   [scar]myt_0035.png → [scar]myt 0035.png\n",
            "   [scar]myt_0036.png → [scar]myt 0036.png\n",
            "   [scar]myt_0040.png → [scar]myt 0040.png\n",
            "   [scar]myt_0049.png → [scar]myt 0049.png\n",
            "   [scar]myt_0052.png → [scar]myt 0052.png\n",
            "   [scar]myt_0024.png → [scar]myt 0024.png\n",
            "\n",
            "曖昧 (手動確認): 4 件\n",
            "対応する画像なし: 2 件\n",
            "\n",
            "====== 最終サマリ ======\n",
            "画像にのみ存在: 36\n",
            "マスクにのみ存在: 2\n",
            "\n",
            "▼画像のみ\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]モーレン_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]細菌_KNZ5048\n",
            "[infection]細菌_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]翼状片_wky7047\n",
            "\n",
            "▼マスクのみ\n",
            "[infection]細菌_fko0032_01-1\n",
            "[infection]細菌_fko0032_01-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "# ===== ディレクトリ設定 =====\n",
        "image_dir = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "mask_dir  = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Masks\"\n",
        "# ==========================\n",
        "\n",
        "def normalize(name: str) -> str:\n",
        "    \"\"\"比較用キーを生成\"\"\"\n",
        "    name = re.sub(r'[-_]\\d+$', '', name)       # 末尾 -1/-2/_1 などを削除\n",
        "    return re.sub(r'[\\s_-]+', '', name).lower()  # 空白・アンダースコアを除去 → 小文字\n",
        "\n",
        "# ── 画像側 ──\n",
        "image_bases = [os.path.splitext(f)[0] for f in os.listdir(image_dir)\n",
        "               if os.path.isfile(os.path.join(image_dir, f))]\n",
        "image_norm2orig = {normalize(b): b for b in image_bases}\n",
        "\n",
        "# ── マスク側 ──\n",
        "mask_bases = [os.path.splitext(f)[0] for f in os.listdir(mask_dir)\n",
        "              if os.path.isfile(os.path.join(mask_dir, f))]\n",
        "mask_norm2orig = {normalize(b): b for b in mask_bases}\n",
        "\n",
        "# ── 一致判定 ──\n",
        "image_keys = set(image_norm2orig)\n",
        "mask_keys  = set(mask_norm2orig)\n",
        "\n",
        "# まだ一致しないもの\n",
        "image_only_keys = image_keys - mask_keys\n",
        "mask_only_keys  = mask_keys  - image_keys\n",
        "\n",
        "unmatched_images = sorted([image_norm2orig[k] for k in image_only_keys])\n",
        "unmatched_masks  = sorted([mask_norm2orig[k]  for k in mask_only_keys])\n",
        "\n",
        "# ── 結果表示 ──\n",
        "print(f\"完全一致しない画像   : {len(unmatched_images)} 件\")\n",
        "print(f\"完全一致しないマスク : {len(unmatched_masks)} 件\")\n",
        "\n",
        "print(\"\\n▼ 画像にのみ存在する basename\")\n",
        "for b in unmatched_images:\n",
        "    print(b)\n",
        "\n",
        "print(\"\\n▼ マスクにのみ存在する basename\")\n",
        "for b in unmatched_masks:\n",
        "    print(b)\n"
      ],
      "metadata": {
        "id": "sPIdNF8K7-tf",
        "outputId": "124ee30f-b9db-49ca-ab1c-e064e9ab572c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "完全一致しない画像   : 43 件\n",
            "完全一致しないマスク : 2 件\n",
            "\n",
            "▼ 画像にのみ存在する basename\n",
            "[bullous]FKS_251_R_slit\n",
            "[bullous]FKS_302_R_slit\n",
            "[deposit]FKS_272_R_slit\n",
            "[immun]FKS_374_R_slit\n",
            "[immun]HRS_391_R_slit\n",
            "[immun]TDC_269_R_slit\n",
            "[immun]モーレン_wky2005_01\n",
            "[infection]FKS_269_R_20240118_slit\n",
            "[infection]細菌_KNZ5048\n",
            "[infection]細菌_KNZ5052\n",
            "[normal]FKS_339_R_slit\n",
            "[normal]FKS_350_R_slit\n",
            "[normal]TDC_263_R_slit\n",
            "[scar]FKS_279_R_20240521_slit\n",
            "[scar]FKS_325_R_slit\n",
            "[scar]HRS_336_R_slit\n",
            "[scar]HRS_409_R_slit\n",
            "[scar]TDC_137_R_slit\n",
            "[scar]myt 0024\n",
            "[scar]myt 0035\n",
            "[scar]myt 0036\n",
            "[scar]myt 0040\n",
            "[scar]myt 0049\n",
            "[scar]myt 0052\n",
            "[scar]myt 0073\n",
            "[tumor]HRS_307_L_slit\n",
            "[tumor]HRS_307_R_slit\n",
            "[tumor]HRS_309_L_slit\n",
            "[tumor]HRS_312_L_slit\n",
            "[tumor]HRS_321_L_slit\n",
            "[tumor]HRS_321_R_slit\n",
            "[tumor]HRS_322_R_slit\n",
            "[tumor]HRS_328_L_slit\n",
            "[tumor]HRS_339_L_slit\n",
            "[tumor]HRS_353_L_slit\n",
            "[tumor]HRS_354_R_slit\n",
            "[tumor]HRS_356_L_slit\n",
            "[tumor]HRS_356_R_slit\n",
            "[tumor]HRS_370_R_slit\n",
            "[tumor]HRS_371_L_slit\n",
            "[tumor]HRS_384_R_slit\n",
            "[tumor]HRS_404_R_slit\n",
            "[tumor]翼状片_wky7047\n",
            "\n",
            "▼ マスクにのみ存在する basename\n",
            "[infection]細菌_fko0032_01-2\n",
            "[scar]myt_0024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7dzfX7nR2RaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOe-1hxV2Rby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FekHTfIU2Rdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7sGrpBQ92Rf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVBBhZUM2RiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLvwguGg2RkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/yolov5-gradcam\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "# from google.colab.patches import cv2_imshow # Colab環境でcv2_imshowを使用する場合にコメント解除\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "import torchvision\n",
        "\n",
        "# YOLOv5の必要な関数をインポート - attempt_loadの場所を修正\n",
        "from utils.general import non_max_suppression as yolo_nms, xywh2xyxy, box_iou\n",
        "from utils.datasets import letterbox\n",
        "from models.experimental import attempt_load  # 修正: models.experimentalから\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポートします\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# これまでに登場したクラスをすべてリストに追加します\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFを設定\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# グローバル変数として定義（エラー回避のため）\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFを設定\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# グローバル変数として定義（エラー回避のため）\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "# ユーザー提供のYOLOV5TorchObjectDetectorクラス\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode=\"eval\",\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "\n",
        "        # === 変更点: モデル全体のrequires_grad_(True)を明示的に設定 ===\n",
        "        # Grad-CAMのためにモデル全体の勾配計算を有効にする\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "        self.model.to(device)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "\n",
        "        if names is None:\n",
        "            print(\"[INFO] fetching names from coco file\")\n",
        "            self.names = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "                          \"traffic light\",\n",
        "                          \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\",\n",
        "                          \"cow\",\n",
        "                          \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
        "                          \"frisbee\",\n",
        "                          \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\",\n",
        "                          \"surfboard\",\n",
        "                          \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\",\n",
        "                          \"apple\",\n",
        "                          \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "                          \"couch\",\n",
        "                          \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "                          \"keyboard\", \"cell phone\",\n",
        "                          \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
        "                          \"teddy bear\",\n",
        "                          \"hair drier\", \"toothbrush\"]\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        # ダミーのフォワードパスはメモリを消費するため、コメントアウト\n",
        "        # img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        # self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        assert 0 <= conf_thres <= 1, f\"Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0\"\n",
        "        assert 0 <= iou_thres <= 1, f\"Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0\"\n",
        "\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:\n",
        "                continue\n",
        "            elif n > max_nms:\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres\n",
        "                weights = iou * scores[None]\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f\"WARNING: NMS time limit {time_limit}s exceeded\")\n",
        "                break\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # === 変更点: 推論部分のwith torch.no_grad()を削除 ===\n",
        "        # Grad-CAMのために勾配計算を有効にする\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "\n",
        "        # 以下の処理はCPUで行うことでGPUメモリを節約\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                # detをCPUに移動\n",
        "                det_cpu = det.cpu()\n",
        "                for *xyxy, conf, cls in det_cpu:\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img\n",
        "\n",
        "# YOLOV5GradCAMクラス\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        # === 変更点: target_layerのrequires_gradをTrueに設定 ===\n",
        "        # モデル全体のrequires_grad_(True)を削除したため、ここでGrad-CAMに必要なレイヤーのみTrueにする\n",
        "        for param in target_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # 初期化時のメモリ消費を抑えるため、ダミーのフォワードパスを削除\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        # モデルのフォワードパスは勾配計算が必要なため、no_grad()を適用しない\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkの計算は勾配不要なのでno_grad()を適用\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapの計算は勾配不要なのでno_grad()を適用\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ゼロ除算を避けるためにepsilonを追加\n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "# find_yolo_layer関数\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "# get_aoi関数\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0\n",
        "\n",
        "   for mask in masks:\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255\n",
        "\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "# calculate_aoi関数（チェックポイント機能付き）\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                  start_index=None, end_index=None, skip_processed=True):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # AOIカラムの定義\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # カラムが存在しない場合は追加\n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    # 処理済み画像の判定関数\n",
        "    def is_processed(row):\n",
        "        \"\"\"すべてのAOIカラムに値が入っているかチェック\"\"\"\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # 処理対象のインデックスを決定\n",
        "    if skip_processed and start_index is None and end_index is None:\n",
        "        # 未処理の画像のみを対象とする\n",
        "        unprocessed_indices = []\n",
        "        for idx, row in df.iterrows():\n",
        "            if not is_processed(row):\n",
        "                unprocessed_indices.append(idx)\n",
        "\n",
        "        if not unprocessed_indices:\n",
        "            print(\"すべての画像が処理済みです。\")\n",
        "            return\n",
        "\n",
        "        print(f\"未処理画像数: {len(unprocessed_indices)}/{len(df)}\")\n",
        "        print(f\"最初の未処理画像インデックス: {unprocessed_indices[0]}\")\n",
        "\n",
        "        # 処理対象インデックスのリスト\n",
        "        target_indices = unprocessed_indices\n",
        "\n",
        "    else:\n",
        "        # 手動でインデックス範囲を指定した場合\n",
        "        if start_index is None:\n",
        "            start_index = 0\n",
        "        if end_index is None:\n",
        "            end_index = len(df)\n",
        "\n",
        "        target_indices = list(range(start_index, end_index))\n",
        "\n",
        "        # skip_processedがTrueの場合、範囲内でも処理済みはスキップ\n",
        "        if skip_processed:\n",
        "            target_indices = [idx for idx in target_indices\n",
        "                            if idx < len(df) and not is_processed(df.iloc[idx])]\n",
        "\n",
        "    print(f\"処理対象画像数: {len(target_indices)}\")\n",
        "\n",
        "    # 進捗状況の表示\n",
        "    processed_count = 0\n",
        "    skipped_count = len(df) - len(target_indices) if skip_processed else 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    # 定期的な保存のカウンター\n",
        "    save_interval = 10  # 10画像ごとに保存\n",
        "\n",
        "    # メインの処理ループ\n",
        "    for i, index in enumerate(tqdm(target_indices, desc=\"Processing images\")):\n",
        "        row = df.iloc[index]\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        # 画像ファイルの検索\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "                # 定期的な中間保存\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    df.to_csv(csv_path, index=False)\n",
        "                    print(f\"中間保存完了: {i + 1}/{len(target_indices)} 画像処理済み\")\n",
        "\n",
        "                    # メモリクリーンアップ\n",
        "                    if device == \"cuda\":\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    # 最終保存\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"\\n処理完了サマリー:\")\n",
        "    print(f\"- 処理した画像: {processed_count}個\")\n",
        "    print(f\"- スキップした画像: {skipped_count}個\")\n",
        "    print(f\"- 見つからなかった画像: {not_found_count}個\")\n",
        "    print(f\"- エラーが発生した画像: {error_count}個\")\n",
        "    print(f\"結果を保存しました: {csv_path}\")\n",
        "\n",
        "# メイン実行部分\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"使用デバイス: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",\n",
        "    \"model_20_cv3_conv\",\n",
        "    \"model_23_cv3_conv\",\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# バッチ処理モードの設定\n",
        "USE_BATCH_MODE = False\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== バッチ処理モードで実行します ===\")\n",
        "\n",
        "    # is_processed関数を定義（calculate_aoi関数外でも使えるように）\n",
        "    def is_processed(row):\n",
        "        \"\"\"すべてのAOIカラムに値が入っているかチェック\"\"\"\n",
        "        all_layers_cols = [\n",
        "            (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "            (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "            (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "            (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "            (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "            (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "        ]\n",
        "        for _, col_name in all_layers_cols:\n",
        "            if pd.isna(row[col_name]) or row[col_name] is None:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    # 未処理画像のインデックスを取得\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    unprocessed_indices = []\n",
        "\n",
        "    for idx, row in df_check.iterrows():\n",
        "        if not is_processed(row):\n",
        "            unprocessed_indices.append(idx)\n",
        "\n",
        "    if not unprocessed_indices:\n",
        "        print(\"すべての画像が処理済みです。\")\n",
        "    else:\n",
        "        print(f\"未処理画像数: {len(unprocessed_indices)}\")\n",
        "\n",
        "        # バッチごとに処理\n",
        "        for i in range(0, len(unprocessed_indices), BATCH_SIZE):\n",
        "            batch_indices = unprocessed_indices[i:i+BATCH_SIZE]\n",
        "            batch_start = batch_indices[0] if batch_indices else 0\n",
        "            batch_end = batch_indices[-1] + 1 if batch_indices else 0\n",
        "\n",
        "            print(f\"\\nバッチ {i//BATCH_SIZE + 1}/{(len(unprocessed_indices)-1)//BATCH_SIZE + 1}: \"\n",
        "                  f\"インデックス {batch_indices}\")\n",
        "\n",
        "            # バッチ内の画像をまとめて処理\n",
        "            calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                         start_index=None, end_index=None, skip_processed=True)\n",
        "\n",
        "            # 処理完了後のクリーンアップ\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(f\"バッチ {i//BATCH_SIZE + 1} 完了\")\n",
        "\n",
        "            # バッチが完了したら、再度未処理画像を確認（中断された場合の対策）\n",
        "            df_check = pd.read_csv(csv_path)\n",
        "            remaining_unprocessed = sum(1 for idx, row in df_check.iterrows() if not is_processed(row))\n",
        "            print(f\"残り未処理画像数: {remaining_unprocessed}\")\n",
        "\n",
        "            if remaining_unprocessed == 0:\n",
        "                break\n",
        "\n",
        "    print(\"\\n=== 全バッチ処理完了 ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== 通常モードで実行します ===\")\n",
        "\n",
        "    # モード選択\n",
        "    mode = input(\"実行モードを選択してください:\\n1: 自動（未処理画像を検出）\\n2: 手動（範囲を指定）\\n3: 強制再処理（範囲を指定、処理済みも再処理）\\n選択 (1/2/3): \")\n",
        "\n",
        "    if mode == \"1\":\n",
        "        # 自動モード\n",
        "        print(\"\\n未処理画像を自動検出して処理します...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     skip_processed=True)\n",
        "\n",
        "    elif mode == \"2\":\n",
        "        # 手動モード（処理済みはスキップ）\n",
        "        start_index = int(input(\"開始インデックスを入力してください: \"))\n",
        "        end_index = int(input(\"終了インデックスを入力してください: \"))\n",
        "        print(f\"\\nインデックス {start_index} から {end_index-1} まで処理します（処理済みはスキップ）...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=True)\n",
        "\n",
        "    elif mode == \"3\":\n",
        "        # 強制再処理モード\n",
        "        start_index = int(input(\"開始インデックスを入力してください: \"))\n",
        "        end_index = int(input(\"終了インデックスを入力してください: \"))\n",
        "        print(f\"\\nインデックス {start_index} から {end_index-1} まで強制的に再処理します...\")\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=start_index, end_index=end_index, skip_processed=False)\n",
        "\n",
        "    else:\n",
        "        print(\"無効な選択です。プログラムを終了します。\")"
      ],
      "metadata": {
        "id": "aHMNNdM1dWJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbMtDZ6MdWNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXxnkbradWPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj6fl4uVCCji"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "#################\n",
        "os.chdir('/content/yolov5-gradcam')\n",
        "\n",
        "import numpy as np\n",
        "from deep_utils.utils.box_utils.boxes import Box\n",
        "import torch\n",
        "from models.experimental import attempt_load\n",
        "from utils.general import xywh2xyxy\n",
        "from utils.datasets import letterbox\n",
        "import cv2\n",
        "import time\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from utils.metrics import box_iou\n",
        "\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample, ModuleList\n",
        "\n",
        "# YOLOv5のカスタムクラスをインポートします\n",
        "from models.yolo import Model, Detect\n",
        "from models.common import Conv, Bottleneck, C3, SPPF, Concat\n",
        "\n",
        "# これまでに登場したクラスをすべてリストに追加します\n",
        "torch.serialization.add_safe_globals([\n",
        "    Model, Detect, Sequential, ModuleList, Conv, Bottleneck, C3, SPPF, Concat,\n",
        "    Conv2d, BatchNorm2d, SiLU, MaxPool2d, Upsample\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class YOLOV5TorchObjectDetector(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_weight,\n",
        "                 device,\n",
        "                 img_size,\n",
        "                 names=None,\n",
        "                 mode='eval',\n",
        "                 confidence=0.25,\n",
        "                 iou_thresh=0.45,\n",
        "                 agnostic_nms=False):\n",
        "        super(YOLOV5TorchObjectDetector, self).__init__()\n",
        "        self.device = device\n",
        "        self.model = None\n",
        "        self.img_size = img_size\n",
        "        self.mode = mode\n",
        "        self.confidence = confidence\n",
        "        self.iou_thresh = iou_thresh\n",
        "        self.agnostic = agnostic_nms\n",
        "        self.model = attempt_load(model_weight, device=device)\n",
        "        print(\"[INFO] Model is loaded\")\n",
        "        self.model.requires_grad_(True)\n",
        "        self.model.to(device)\n",
        "        if self.mode == 'train':\n",
        "            self.model.train()\n",
        "        else:\n",
        "            self.model.eval()\n",
        "        # fetch the names\n",
        "        if names is None:\n",
        "            print('[INFO] fetching names from coco file')\n",
        "            self.names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "                          'traffic light',\n",
        "                          'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
        "                          'cow',\n",
        "                          'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase',\n",
        "                          'frisbee',\n",
        "                          'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
        "                          'surfboard',\n",
        "                          'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
        "                          'apple',\n",
        "                          'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
        "                          'couch',\n",
        "                          'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
        "                          'keyboard', 'cell phone',\n",
        "                          'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
        "                          'teddy bear',\n",
        "                          'hair drier', 'toothbrush']\n",
        "        else:\n",
        "            self.names = names\n",
        "\n",
        "        # preventing cold start\n",
        "        img = torch.zeros((1, 3, *self.img_size), device=device)\n",
        "        self.model(img)\n",
        "\n",
        "    @staticmethod\n",
        "    def non_max_suppression(prediction, logits, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False,\n",
        "                            multi_label=False, labels=(), max_det=300):\n",
        "        \"\"\"Runs Non-Maximum Suppression (NMS) on inference and logits results\n",
        "\n",
        "        Returns:\n",
        "             list of detections, on (n,6) tensor per image [xyxy, conf, cls] and pruned input logits (n, number-classes)\n",
        "        \"\"\"\n",
        "\n",
        "        nc = prediction.shape[2] - 5  # number of classes\n",
        "        xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "        # Checks\n",
        "        assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "        assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "        # Settings\n",
        "        min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "        max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "        time_limit = 10.0  # seconds to quit after\n",
        "        redundant = True  # require redundant detections\n",
        "        multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "        merge = False  # use merge-NMS\n",
        "\n",
        "        t = time.time()\n",
        "        output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "        logits_output = [torch.zeros((0, 80), device=logits.device)] * logits.shape[0]\n",
        "        for xi, (x, log_) in enumerate(zip(prediction, logits)):  # image index, image inference\n",
        "            # Apply constraints\n",
        "            # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "            x = x[xc[xi]]  # confidence\n",
        "            log_ = log_[xc[xi]]\n",
        "            # Cat apriori labels if autolabelling\n",
        "            if labels and len(labels[xi]):\n",
        "                l = labels[xi]\n",
        "                v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "                v[:, :4] = l[:, 1:5]  # box\n",
        "                v[:, 4] = 1.0  # conf\n",
        "                v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "                x = torch.cat((x, v), 0)\n",
        "\n",
        "            # If none remain process next image\n",
        "            if not x.shape[0]:\n",
        "                continue\n",
        "\n",
        "            # Compute conf\n",
        "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "            # log_ *= x[:, 4:5]\n",
        "            # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "            box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "            # Detections matrix nx6 (xyxy, conf, cls)\n",
        "            if multi_label:\n",
        "                i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "                x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "            else:  # best class only\n",
        "                conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "                # log_ = x[:, 5:]\n",
        "                x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "                log_ = log_[conf.view(-1) > conf_thres]\n",
        "            # Filter by class\n",
        "            if classes is not None:\n",
        "                x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "            # Check shape\n",
        "            n = x.shape[0]  # number of boxes\n",
        "            if not n:  # no boxes\n",
        "                continue\n",
        "            elif n > max_nms:  # excess boxes\n",
        "                x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "            # Batched NMS\n",
        "            c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "            boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "            i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "            if i.shape[0] > max_det:  # limit detections\n",
        "                i = i[:max_det]\n",
        "            if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "                # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "                weights = iou * scores[None]  # box weights\n",
        "                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "                if redundant:\n",
        "                    i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "            output[xi] = x[i]\n",
        "            logits_output[xi] = log_[i]\n",
        "            assert log_[i].shape[0] == x[i].shape[0]\n",
        "            if (time.time() - t) > time_limit:\n",
        "                print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "                break  # time limit exceeded\n",
        "\n",
        "        return output, logits_output\n",
        "\n",
        "    @staticmethod\n",
        "    def yolo_resize(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):\n",
        "\n",
        "        return letterbox(img, new_shape=new_shape, color=color, auto=auto, scaleFill=scaleFill, scaleup=scaleup)\n",
        "\n",
        "    def forward(self, img):\n",
        "        prediction, logits, _ = self.model(img, augment=False)\n",
        "        prediction, logits = self.non_max_suppression(prediction, logits, self.confidence, self.iou_thresh,\n",
        "                                                      classes=None,\n",
        "                                                      agnostic=self.agnostic)\n",
        "        self.boxes, self.class_names, self.classes, self.confidences = [[[] for _ in range(img.shape[0])] for _ in\n",
        "                                                                        range(4)]\n",
        "        for i, det in enumerate(prediction):  # detections per image\n",
        "            if len(det):\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    # xyxyの値を修正\n",
        "                    xyxy[0] = max(0, xyxy[0])\n",
        "                    xyxy[1] = max(0, xyxy[1])\n",
        "                    xyxy[2] = min(img_size, xyxy[2])\n",
        "                    xyxy[3] = min(img_size, xyxy[3])\n",
        "\n",
        "                    bbox = Box.box2box(xyxy,\n",
        "                                       in_source=Box.BoxSource.Torch,\n",
        "                                       to_source=Box.BoxSource.Numpy,\n",
        "                                       return_int=True)\n",
        "                    self.boxes[i].append(bbox)\n",
        "                    self.confidences[i].append(round(conf.item(), 2))\n",
        "                    cls = int(cls.item())\n",
        "                    self.classes[i].append(cls)\n",
        "                    if self.names is not None:\n",
        "                        self.class_names[i].append(self.names[cls])\n",
        "                    else:\n",
        "                        self.class_names[i].append(cls)\n",
        "        return [self.boxes, self.classes, self.class_names, self.confidences], logits\n",
        "\n",
        "    def preprocessing(self, img):\n",
        "        if len(img.shape) != 4:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        im0 = img.astype(np.uint8)\n",
        "        img = np.array([self.yolo_resize(im, new_shape=self.img_size)[0] for im in im0])\n",
        "        img = img.transpose((0, 3, 1, 2))\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img / 255.0\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS3iA_8QKWZC"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrjCYpUtJlyR"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "##################\n",
        "## 6-layerで解析##\n",
        "##################\n",
        "##################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# グローバル変数として定義（エラー回避のため）\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method='gradcam'):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == 'eigencam':\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients['value']\n",
        "                activations = self.activations['value']\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == 'gradcam':\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == 'gradcampp':\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                saliency_map = F.relu(saliency_map)\n",
        "                saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "                saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode='bilinear', align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split('_')\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    # 全レイヤーの設定\n",
        "    all_layers = [\n",
        "        ('model_17_cv3_act', f'AOI_{threshold}_layermodel_17_cv3_act'),\n",
        "        ('model_20_cv3_act', f'AOI_{threshold}_layermodel_20_cv3_act'),\n",
        "        ('model_23_cv3_act', f'AOI_{threshold}_layermodel_23_cv3_act'),\n",
        "        ('model_24_m_0', f'AOI_{threshold}_layer24_m_0'),\n",
        "        ('model_24_m_1', f'AOI_{threshold}_layer24_m_1'),\n",
        "        ('model_24_m_2', f'AOI_{threshold}_layer24_m_2')\n",
        "    ]\n",
        "\n",
        "    # 列が存在しない場合は作成\n",
        "    for layer_name, col_name in all_layers:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # 実際に処理された画像の数をカウント\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row['image_basename']\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # 拡張子を試す\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG', '']:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # 全レイヤーで処理\n",
        "                for i, (layer_name, col_name) in enumerate(all_layers):\n",
        "                    if i < len(saliency_methods):\n",
        "                        try:\n",
        "                            masks, logits, [boxes, _, _, _], cls_names = saliency_methods[i](torch_img)\n",
        "\n",
        "                            if len(masks) > 0 and len(boxes) > 0:\n",
        "                                mask = masks[0][0]\n",
        "                                bbox = boxes[0][0]\n",
        "\n",
        "                                aoi = get_aoi(bbox, [mask], threshold)\n",
        "                                df.at[index, col_name] = aoi\n",
        "\n",
        "                        except Exception as e:\n",
        "                            error_count += 1\n",
        "                            if error_count <= 5:\n",
        "                                print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # 進捗表示\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\n処理完了: {processed_count}個の画像を処理\")\n",
        "    print(f\"見つからなかった画像: {not_found_count}個\")\n",
        "    print(f\"エラーが発生した画像: {error_count}個\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"結果を保存しました: {csv_path}\")\n",
        "\n",
        "# メイン実行部分\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"使用デバイス: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# 全レイヤーのtarget_layers（convに修正が必要な場合）\n",
        "target_layers = [\n",
        "    'model_17_cv3_conv',  # actからconvに変更\n",
        "    'model_20_cv3_conv',  # actからconvに変更\n",
        "    'model_23_cv3_conv',  # actからconvに変更\n",
        "    'model_24_m_0',\n",
        "    'model_24_m_1',\n",
        "    'model_24_m_2'\n",
        "]\n",
        "\n",
        "# 実行モード選択\n",
        "USE_BATCH_MODE = False  # バッチ処理モードを使用するかどうか\n",
        "BATCH_SIZE = 20       # バッチサイズ\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== バッチ処理モードで実行します ===\")\n",
        "\n",
        "    # CSVファイルの総行数を確認\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"総画像数: {total_images}\")\n",
        "\n",
        "    # 全レイヤーのsaliency_methods を作成\n",
        "    print(\"\\nGradCAMメソッドを初期化中...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  エラー: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\n初期化完了: {len(saliency_methods)}個のレイヤー\")\n",
        "\n",
        "    # バッチ処理実行\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== バッチ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"画像 {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # メモリ解放\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"バッチ {batch_start//BATCH_SIZE + 1} 完了\")\n",
        "\n",
        "    print(\"\\n=== 全バッチ処理完了 ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== 通常モードで実行します ===\")\n",
        "\n",
        "    # 処理範囲の設定\n",
        "    start_index = 0\n",
        "    end_index = 5  # Noneで全データ処理\n",
        "\n",
        "    # 全レイヤーのsaliency_methods を作成\n",
        "    print(\"GradCAMメソッドを初期化中...\")\n",
        "    saliency_methods = []\n",
        "    for i, layer in enumerate(target_layers):\n",
        "        print(f\"  Layer {i+1}/{len(target_layers)}: {layer}\")\n",
        "        try:\n",
        "            saliency_methods.append(\n",
        "                YOLOV5GradCAM(model=model, layer_name=layer, img_size=input_size, method=\"gradcampp\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"  エラー: {layer} - {str(e)}\")\n",
        "\n",
        "    print(f\"\\n初期化完了: {len(saliency_methods)}個のレイヤー\")\n",
        "    print(\"\\n解析を開始します...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, saliency_methods, start_index, end_index)\n",
        "\n",
        "# レイヤーごとの個別処理（エラーが発生する場合の代替案）\n",
        "\"\"\"\n",
        "print(\"=== レイヤーごとの個別処理モード ===\")\n",
        "for layer_idx, layer_name in enumerate(target_layers):\n",
        "    print(f\"\\n--- レイヤー {layer_idx+1}/6: {layer_name} ---\")\n",
        "\n",
        "    try:\n",
        "        # 単一レイヤーのGradCAMを作成\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name,\n",
        "                                       img_size=input_size, method=\"gradcampp\")\n",
        "\n",
        "        # このレイヤーのみで全データを処理\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model,\n",
        "                     [saliency_method], start_index=0, end_index=None)\n",
        "\n",
        "        # メモリ解放\n",
        "        del saliency_method\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  レイヤー {layer_name} でエラー: {str(e)}\")\n",
        "        continue\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### メモリ節約バージョン ###\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "#from models.yolo_v5_object_detector import YOLOV5TorchObjectDetector\n",
        "# from models.gradcam import YOLOV5GradCAM\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "# PYTORCH_CUDA_ALLOC_CONFを設定\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# グローバル変数として定義（エラー回避のため）\n",
        "global img_size\n",
        "img_size = 640\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640), method=\"gradcam\"):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.method = method\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients[\"value\"] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations[\"value\"] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        # 初期化時のメモリ消費を抑えるため、ダミーのフォワードパスを削除\n",
        "        # device = \"cuda\" if next(self.model.model.parameters()).is_cuda else \"cpu\"\n",
        "        # self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "\n",
        "        preds, logits = self.model(input_img)\n",
        "\n",
        "        # topkの計算は勾配不要なのでno_grad()を適用\n",
        "        with torch.no_grad():\n",
        "            _, top3_indices = torch.topk(logits[0], k=3)\n",
        "\n",
        "        if top3_indices.numel() > 0:\n",
        "            preds[1][0] = top3_indices.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        if self.method == \"eigencam\":\n",
        "            saliency_map = self._eigencam()\n",
        "            saliency_maps.append(saliency_map)\n",
        "        else:\n",
        "            for cls, cls_name in zip(preds[1][0], preds[2][0]):\n",
        "                if class_idx:\n",
        "                    score = logits[0][0][cls]\n",
        "                else:\n",
        "                    score = logits[0][0].max()\n",
        "\n",
        "                self.model.zero_grad()\n",
        "                score.backward(retain_graph=True)\n",
        "                gradients = self.gradients[\"value\"]\n",
        "                activations = self.activations[\"value\"]\n",
        "                b, k, u, v = gradients.size()\n",
        "\n",
        "                if self.method == \"gradcam\":\n",
        "                    weights = self._gradcam_weights(gradients, b, k)\n",
        "                elif self.method == \"gradcampp\":\n",
        "                    weights = self._gradcampp_weights(gradients, activations, score, b, k, u, v)\n",
        "\n",
        "                # saliency_mapの計算は勾配不要なのでno_grad()を適用\n",
        "                with torch.no_grad():\n",
        "                    saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "                    saliency_map = F.relu(saliency_map)\n",
        "                    saliency_map = F.interpolate(saliency_map, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "                    saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "                    # ゼロ除算を避けるためにepsilonを追加\n",
        "                    saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min + 1e-8).data\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def _gradcam_weights(self, gradients, b, k):\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        return weights\n",
        "\n",
        "    def _gradcampp_weights(self, gradients, activations, score, b, k, u, v):\n",
        "        alpha_num = gradients.pow(2)\n",
        "        alpha_denom = gradients.pow(2).mul(2) + \\\n",
        "            activations.mul(gradients.pow(3)).view(b, k, u*v).sum(-1, keepdim=True).view(b, k, 1, 1)\n",
        "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
        "        alpha = alpha_num.div(alpha_denom + 1e-7)\n",
        "\n",
        "        relu_grad = F.relu(score.exp() * gradients)\n",
        "        weights = (relu_grad * alpha).view(b, k, u*v).sum(-1).view(b, k, 1, 1)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def _eigencam(self):\n",
        "        activations = self.activations[\"value\"]\n",
        "        b, k, u, v = activations.size()\n",
        "        activations_reshaped = activations.view(b, k, -1)\n",
        "        # Compute covariance matrix\n",
        "        cov = activations_reshaped @ activations_reshaped.transpose(-1, -2)\n",
        "        cov /= (u * v)\n",
        "        # Compute eigenvectors\n",
        "        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n",
        "        # Select the eigenvector corresponding to the largest eigenvalue\n",
        "        leading_eigenvector = eigenvectors[..., -1].unsqueeze(-1)\n",
        "        # Compute EigenCAM\n",
        "        eigen_cam = (activations_reshaped.transpose(-1, -2) @ leading_eigenvector).view(b, u, v)\n",
        "        eigen_cam = F.relu(eigen_cam)\n",
        "        # Normalize\n",
        "        eigen_cam_min, eigen_cam_max = eigen_cam.min(), eigen_cam.max()\n",
        "        eigen_cam = (eigen_cam - eigen_cam_min) / (eigen_cam_max - eigen_cam_min + 1e-8)\n",
        "        # Resize to match the input image size\n",
        "        eigen_cam = F.interpolate(eigen_cam.unsqueeze(1), size=(640, 640), mode=\"bilinear\", align_corners=False)\n",
        "        return eigen_cam\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "   \"\"\"Find yolov5 layer to calculate GradCAM and GradCAM++\n",
        "\n",
        "   Args:\n",
        "       model: yolov5 model.\n",
        "       layer_name (str): the name of layer with its hierarchical information.\n",
        "\n",
        "   Return:\n",
        "       target_layer: found layer\n",
        "   \"\"\"\n",
        "   hierarchy = layer_name.split(\"_\")\n",
        "   target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "   for h in hierarchy[1:]:\n",
        "       target_layer = target_layer._modules[h]\n",
        "   return target_layer\n",
        "\n",
        "def get_aoi(bbox, masks, threshold):\n",
        "   total_intersect_pixels = 0  # 共通部分のピクセル数のカウンタを初期化\n",
        "\n",
        "   for mask in masks:\n",
        "       # マスクの前処理\n",
        "       mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "       # RuntimeWarning: invalid value encountered in cast 対策\n",
        "       mask = np.nan_to_num(mask, nan=0.0).astype(np.uint8)\n",
        "\n",
        "       mask[mask < threshold * 255] = 0\n",
        "       mask[mask >= threshold * 255] = 255\n",
        "       binary_mask = mask >= threshold * 255  # 閾値を超える部分を二値化\n",
        "\n",
        "       # bboxの範囲内のマスク部分を取得\n",
        "       x1, y1, x2, y2 = bbox\n",
        "       mask_bbox = binary_mask[y1:y2, x1:x2]\n",
        "\n",
        "       # 閾値を超える共通部分のピクセル数をカウント\n",
        "       intersect_pixels = np.sum(mask_bbox)\n",
        "       total_intersect_pixels += intersect_pixels\n",
        "\n",
        "       # mask_bbox のピクセル数を取得\n",
        "       mask_bbox_area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "       # AOI を計算\n",
        "       AOI = total_intersect_pixels / mask_bbox_area if mask_bbox_area > 0 else 0\n",
        "\n",
        "   return AOI\n",
        "\n",
        "def calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index=0, end_index=None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
        "\n",
        "    # 全レイヤーの設定\n",
        "    all_layers_cols = [\n",
        "        (\"model_17_cv3_conv\", f\"AOI_{threshold}_layermodel_17_cv3_conv\"),\n",
        "        (\"model_20_cv3_conv\", f\"AOI_{threshold}_layermodel_20_cv3_conv\"),\n",
        "        (\"model_23_cv3_conv\", f\"AOI_{threshold}_layermodel_23_cv3_conv\"),\n",
        "        (\"model_24_m_0\", f\"AOI_{threshold}_layer24_m_0\"),\n",
        "        (\"model_24_m_1\", f\"AOI_{threshold}_layer24_m_1\"),\n",
        "        (\"model_24_m_2\", f\"AOI_{threshold}_layer24_m_2\")\n",
        "    ]\n",
        "\n",
        "    # 列が存在しない場合は作成\n",
        "    for layer_name, col_name in all_layers_cols:\n",
        "        if col_name not in df.columns:\n",
        "            df[col_name] = None\n",
        "\n",
        "    if end_index is None:\n",
        "        end_index = len(df)\n",
        "\n",
        "    # 実際に処理された画像の数をカウント\n",
        "    processed_count = 0\n",
        "    not_found_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        if index < start_index or index >= end_index:\n",
        "            continue\n",
        "\n",
        "        img_basename = row[\"image_basename\"]\n",
        "\n",
        "        if pd.isna(img_basename):\n",
        "            print(f\"Skipping row {index} due to NaN image_basename\")\n",
        "            continue\n",
        "\n",
        "        # 拡張子を試す\n",
        "        img_found = False\n",
        "        img_path = None\n",
        "\n",
        "        for ext in [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\", \"\"]:\n",
        "            test_path = os.path.join(folder_path, f\"{img_basename}{ext}\")\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "                break\n",
        "\n",
        "        if not img_found:\n",
        "            test_path = os.path.join(folder_path, img_basename)\n",
        "            if os.path.exists(test_path):\n",
        "                img_path = test_path\n",
        "                img_found = True\n",
        "\n",
        "        if img_found:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            if img is None:\n",
        "                print(f\"Failed to read image: {img_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                torch_img = model.preprocessing(img[..., ::-1])\n",
        "\n",
        "                # レイヤーごとに処理\n",
        "                for layer_name, col_name in all_layers_cols:\n",
        "                    try:\n",
        "                        # レイヤーごとにYOLOV5GradCAMインスタンスを生成・破棄\n",
        "                        saliency_method = YOLOV5GradCAM(model=model, layer_name=layer_name, img_size=input_size, method=\"gradcampp\")\n",
        "                        masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "\n",
        "                        if len(masks) > 0 and len(boxes) > 0:\n",
        "                            mask = masks[0][0]\n",
        "                            bbox = boxes[0][0]\n",
        "\n",
        "                            aoi = get_aoi(bbox, [mask], threshold)\n",
        "                            df.at[index, col_name] = aoi\n",
        "\n",
        "                        # メモリ解放\n",
        "                        del saliency_method\n",
        "                        if device == \"cuda\":\n",
        "                            torch.cuda.empty_cache()\n",
        "                        gc.collect()\n",
        "\n",
        "                    except Exception as e:\n",
        "                        error_count += 1\n",
        "                        if error_count <= 5:\n",
        "                            print(f\"Error processing image {img_basename} at layer {layer_name}: {str(e)}\")\n",
        "\n",
        "                processed_count += 1\n",
        "                # 進捗表示\n",
        "                if processed_count % 50 == 0:\n",
        "                    print(f\"Processed {processed_count} images...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:\n",
        "                    print(f\"Error preprocessing image {img_basename}: {str(e)}\")\n",
        "        else:\n",
        "            not_found_count += 1\n",
        "            if not_found_count <= 5:\n",
        "                print(f\"Image not found: {img_basename}\")\n",
        "\n",
        "    print(f\"\\n処理完了: {processed_count}個の画像を処理\")\n",
        "    print(f\"見つからなかった画像: {not_found_count}個\")\n",
        "    print(f\"エラーが発生した画像: {error_count}個\")\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"結果を保存しました: {csv_path}\")\n",
        "\n",
        "# メイン実行部分\n",
        "folder_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Images\"\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/Ueno_Mix1039/Ueno_Mix1039_gradcam++.csv\"\n",
        "\n",
        "threshold = 0.5\n",
        "\n",
        "# モデルとsaliency_methodの定義\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"使用デバイス: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "input_size = (640, 640)\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "\n",
        "# 全レイヤーのtarget_layers（convに修正が必要な場合）\n",
        "target_layers = [\n",
        "    \"model_17_cv3_conv\",  # actからconvに変更\n",
        "    \"model_20_cv3_conv\",  # actからconvに変更\n",
        "    \"model_23_cv3_conv\",  # actからconvに変更\n",
        "    \"model_24_m_0\",\n",
        "    \"model_24_m_1\",\n",
        "    \"model_24_m_2\"\n",
        "]\n",
        "\n",
        "# 実行モード選択\n",
        "USE_BATCH_MODE = False  # バッチ処理モードを使用するかどうか\n",
        "BATCH_SIZE = 20       # バッチサイズ\n",
        "\n",
        "if USE_BATCH_MODE:\n",
        "    print(\"=== バッチ処理モードで実行します ===\")\n",
        "\n",
        "    # CSVファイルの総行数を確認\n",
        "    df_check = pd.read_csv(csv_path)\n",
        "    total_images = len(df_check)\n",
        "    print(f\"総画像数: {total_images}\")\n",
        "\n",
        "    # calculate_aoi関数内でレイヤーごとにインスタンスを生成するため、ここではsaliency_methodsを生成しない\n",
        "    saliency_methods = [] # 空のリストとして定義\n",
        "\n",
        "    print(\"\\n解析を開始します...\")\n",
        "    # バッチ処理実行\n",
        "    for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "        batch_end = min(batch_start + BATCH_SIZE, total_images)\n",
        "        print(f\"\\n=== バッチ {batch_start//BATCH_SIZE + 1}/{(total_images-1)//BATCH_SIZE + 1}: \"\n",
        "              f\"画像 {batch_start+1}-{batch_end}/{total_images} ===\")\n",
        "\n",
        "        calculate_aoi(folder_path, csv_path, threshold, model, target_layers,\n",
        "                     start_index=batch_start, end_index=batch_end)\n",
        "\n",
        "        # メモリ解放\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(f\"バッチ {batch_start//BATCH_SIZE + 1} 完了\")\n",
        "\n",
        "    print(\"\\n=== 全バッチ処理完了 ===\")\n",
        "\n",
        "else:\n",
        "    print(\"=== 通常モードで実行します ===\")\n",
        "\n",
        "    # 処理範囲の設定\n",
        "    start_index = 30\n",
        "    end_index = 100  # Noneで全データ処理\n",
        "\n",
        "    # calculate_aoi関数内でレイヤーごとにインスタンスを生成するため、ここではsaliency_methodsを生成しない\n",
        "    saliency_methods = [] # 空のリストとして定義\n",
        "\n",
        "    print(\"\\n解析を開始します...\")\n",
        "\n",
        "    calculate_aoi(folder_path, csv_path, threshold, model, target_layers, start_index, end_index)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uMYSR4IdpbaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0C6tEWVkpbca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhDcRK1apbeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNL_j2lkuem5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XxKs84GkBwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE7ZYlphCpBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVNlV1lh_rRp"
      },
      "outputs": [],
      "source": [
        "# show_result\n",
        "import pandas as pd\n",
        "\n",
        "# CSVファイルのパス\n",
        "csv_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv\"\n",
        "\n",
        "# DataFrameとしてCSVファイルを読み込む\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# DataFrameの最初の数行を表示する\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGNyvTdM3c4"
      },
      "source": [
        "#**注目点色塗り**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx9KdyNZNzz8"
      },
      "outputs": [],
      "source": [
        "#注目点に色を塗る（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            color = np.array([0,0,0], dtype=np.uint8) #マスクの色：白は[0,0,0]\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + (1 - binary_mask[..., np.newaxis]) * color\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + (1 - black_mask[..., np.newaxis]) * color\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/8.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.1  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEn_KHC-T8C1"
      },
      "outputs": [],
      "source": [
        "# 注目点をblurする（反転あり）\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from google.colab.patches import cv2_imshow\n",
        "from deep_utils import Box, split_extension\n",
        "import gc\n",
        "\n",
        "# パラメータ\n",
        "model_path = \"/gdrive/MyDrive/Deep_learning/CorneAI_nagoya/yolo5_forcresco/weights/eye_nii_2202_onecaseoneimage2_doctorcompare_yolov5s_epoch200_batch16_89.8p/last.pt\"\n",
        "img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/スマホ_serial/99.jpg\"\n",
        "output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "img_size = 640\n",
        "target_layer = 'model_23_cv3_act'\n",
        "method = 'gradcam'\n",
        "device = 'cpu'\n",
        "names = [\"infection\",\"normal\",\"non-infection\",\"scar\",\"tumor\",\"deposit\",\"APAC\",\"lens opacity\",\"bullous\"]\n",
        "\n",
        "def get_res_img(bbox, masks, res_img):\n",
        "    for mask in masks:\n",
        "        mask = mask.squeeze().mul(255).add_(0.5).clamp_(0, 255).detach().cpu().numpy().astype(np.uint8)\n",
        "        heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
        "        n_heatmat = (Box.fill_outer_box(heatmap, bbox) / 255).astype(np.float32)\n",
        "        res_img = res_img / 255\n",
        "        res_img = cv2.add(res_img, n_heatmat)\n",
        "        res_img = (res_img / res_img.max())\n",
        "    return res_img, n_heatmat\n",
        "\n",
        "def put_text_box(bbox, cls_name, res_img):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    cv2.imwrite('temp.jpg', (res_img * 255).astype(np.uint8))\n",
        "    res_img = cv2.imread('temp.jpg')\n",
        "    res_img = Box.put_box(res_img, bbox)\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2.0\n",
        "    color = (0, 255, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    text_size, _ = cv2.getTextSize(cls_name, font, font_scale, thickness)\n",
        "    text_x = 10\n",
        "    text_y = text_size[1] + 10\n",
        "\n",
        "    cv2.putText(res_img, cls_name, (text_x, text_y), font, font_scale, color, thickness)\n",
        "\n",
        "    return res_img\n",
        "\n",
        "def find_yolo_layer(model, layer_name):\n",
        "    hierarchy = layer_name.split('_')\n",
        "    target_layer = model.model._modules[hierarchy[0]]\n",
        "\n",
        "    for h in hierarchy[1:]:\n",
        "        target_layer = target_layer._modules[h]\n",
        "    return target_layer\n",
        "\n",
        "class YOLOV5GradCAM:\n",
        "    def __init__(self, model, layer_name, img_size=(640, 640)):\n",
        "        self.model = model\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        self.cls_names = []\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "\n",
        "        target_layer = find_yolo_layer(self.model, layer_name)\n",
        "        target_layer.register_forward_hook(forward_hook)\n",
        "        target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "        device = 'cuda' if next(self.model.model.parameters()).is_cuda else 'cpu'\n",
        "        self.model(torch.zeros(1, 3, *img_size, device=device))\n",
        "\n",
        "    def forward(self, input_img, class_idx=True):\n",
        "        saliency_maps = []\n",
        "        b, c, h, w = input_img.size()\n",
        "        preds, logits = self.model(input_img)\n",
        "        _, top1_idx = torch.topk(logits[0], k=1)\n",
        "\n",
        "        if top1_idx.numel() > 0:\n",
        "            preds[1][0] = top1_idx.tolist()[0]\n",
        "            preds[2][0] = [names[i] for i in preds[1][0]]\n",
        "            self.cls_names = preds[2][0]\n",
        "        else:\n",
        "            self.cls_names = []\n",
        "\n",
        "        cls, cls_name = preds[1][0][0], preds[2][0][0]\n",
        "        if class_idx:\n",
        "            score = logits[0][0][cls]\n",
        "        else:\n",
        "            score = logits[0][0].max()\n",
        "        self.model.zero_grad()\n",
        "        score.backward(retain_graph=True)\n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "        b, k, u, v = gradients.size()\n",
        "        alpha = gradients.view(b, k, -1).mean(2)\n",
        "        weights = alpha.view(b, k, 1, 1)\n",
        "        saliency_map = (weights * activations).sum(1, keepdim=True)\n",
        "        saliency_map = F.relu(saliency_map)\n",
        "        saliency_map = F.interpolate(saliency_map, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        saliency_map_min, saliency_map_max = saliency_map.min(), saliency_map.max()\n",
        "        saliency_map = (saliency_map - saliency_map_min).div(saliency_map_max - saliency_map_min).data\n",
        "        saliency_maps.append(saliency_map)\n",
        "\n",
        "        return saliency_maps, logits, preds, self.cls_names\n",
        "\n",
        "    def __call__(self, input_img):\n",
        "        return self.forward(input_img)\n",
        "\n",
        "def main(img_path, threshold=0.5):\n",
        "    input_size = (img_size, img_size)\n",
        "    img = cv2.imread(img_path)\n",
        "    model = YOLOV5TorchObjectDetector(model_path, device, img_size=input_size, names=names)\n",
        "    torch_img = model.preprocessing(img[..., ::-1])\n",
        "    if method == 'gradcam':\n",
        "        saliency_method = YOLOV5GradCAM(model=model, layer_name=target_layer, img_size=input_size)\n",
        "    masks, logits, [boxes, _, _, _], cls_names = saliency_method(torch_img)\n",
        "    result = torch_img.squeeze(0).mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    result = result[..., ::-1]\n",
        "\n",
        "    if masks:\n",
        "        mask = masks[0].squeeze().cpu().numpy()\n",
        "        if mask.max() >= threshold:\n",
        "            binary_mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)\n",
        "            black_mask = np.where(mask >= threshold, 0, 1).astype(np.uint8)\n",
        "            binary_mask = cv2.resize(binary_mask, (result.shape[1], result.shape[0]))\n",
        "            black_mask = cv2.resize(black_mask, (result.shape[1], result.shape[0]))\n",
        "\n",
        "            # ぼかし効果を適用\n",
        "            blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            masked_res_img = result * binary_mask[..., np.newaxis] + blurred_result * (1 - binary_mask[..., np.newaxis])\n",
        "            masked_res_img = (masked_res_img / masked_res_img.max()) * 255\n",
        "\n",
        "            black_blurred_result = cv2.GaussianBlur(result, (91, 91), 0)\n",
        "            black_masked_res_img = result * black_mask[..., np.newaxis] + black_blurred_result * (1 - black_mask[..., np.newaxis])\n",
        "            black_masked_res_img = (black_masked_res_img / black_masked_res_img.max()) * 255\n",
        "\n",
        "            cv2_imshow(masked_res_img.astype(np.uint8))\n",
        "            cv2_imshow(black_masked_res_img.astype(np.uint8))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    target_layer = 'model_23_cv3_conv'\n",
        "    img_path = \"/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問/フォトスリット_serial/41.jpg\"\n",
        "    output_dir = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/GradCam_img_slit'\n",
        "    threshold = 0.3  # ここで閾値を指定\n",
        "    if os.path.isdir(img_path):\n",
        "        folder_main(img_path, threshold)\n",
        "    else:\n",
        "        main(img_path, threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69WoakWi-ryq"
      },
      "source": [
        "#**Analyze results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h03rgBUoWi_L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV files\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# Define class names as shown in the image\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\", \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# Create confusion matrices for Slit and Sumaho datasets\n",
        "conf_matrix_slit = pd.crosstab(df_slit['class_num'], df_slit['top1'])\n",
        "conf_matrix_sumaho = pd.crosstab(df_sumaho['class_num'], df_sumaho['top1'])\n",
        "\n",
        "# Rename the index and columns for better readability\n",
        "conf_matrix_slit.index = class_names\n",
        "conf_matrix_slit.columns = class_names\n",
        "conf_matrix_sumaho.index = class_names\n",
        "conf_matrix_sumaho.columns = class_names\n",
        "\n",
        "# Create a figure with higher DPI\n",
        "plt.figure(figsize=(15, 6), dpi=350)  # 横幅を少し広げました\n",
        "\n",
        "# Slit Lamp Data Confusion Matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(conf_matrix_slit, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Slit Lamp Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # 横軸のラベルを45度傾ける\n",
        "\n",
        "# Smartphone Data Confusion Matrix\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(conf_matrix_sumaho, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix for Smartphone Data')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xticks(rotation=45, ha='right')  # 横軸のラベルを45度傾ける\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure at 350 DPI\n",
        "plt.savefig('confusion_matrices_350dpi.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjTt2gTWAAD3"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy for Slit and Sumaho datasets\n",
        "accuracy_slit = (conf_matrix_slit.values.diagonal().sum() / conf_matrix_slit.values.sum()) * 100\n",
        "accuracy_sumaho = (conf_matrix_sumaho.values.diagonal().sum() / conf_matrix_sumaho.values.sum()) * 100\n",
        "\n",
        "accuracy_slit, accuracy_sumaho\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title スマホvsスリット (GradCAM++)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# 両データセットの統計を計算して表示\n",
        "for df, device_name in [(df_slit, \"Slit Lamp\"), (df_sumaho, \"Smartphone\")]:\n",
        "    print(f\"\\n============= {device_name} データの統計値 =============\")\n",
        "\n",
        "    # 各レイヤーの詳細な統計量を計算\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} の統計値 -------\")\n",
        "        print(f\"データ数: {int(stats['count'])}\")\n",
        "        print(f\"平均値: {stats['mean']:.6f}\")\n",
        "        print(f\"標準偏差: {stats['std']:.6f}\")\n",
        "        print(f\"最小値: {stats['min']:.6f}\")\n",
        "        print(f\"10パーセンタイル: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25パーセンタイル: {stats['25%']:.6f}\")\n",
        "        print(f\"中央値: {stats['50%']:.6f}\")\n",
        "        print(f\"75パーセンタイル: {stats['75%']:.6f}\")\n",
        "        print(f\"90パーセンタイル: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"最大値: {stats['max']:.6f}\")\n",
        "\n",
        "        # 歪度と尖度も計算\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"歪度: {skewness:.6f}\")\n",
        "        print(f\"尖度: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3つのレイヤーの合計値の統計を計算\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3レイヤーの合計値の統計 -------\")\n",
        "    print(f\"データ数: {int(sum_stats['count'])}\")\n",
        "    print(f\"平均値: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"標準偏差: {sum_stats['std']:.6f}\")\n",
        "    print(f\"最小値: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10パーセンタイル: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25パーセンタイル: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"中央値: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75パーセンタイル: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90パーセンタイル: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"最大値: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # 相関係数の計算\n",
        "    print(f\"\\n------- レイヤー間の相関係数 -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # セパレータの表示"
      ],
      "metadata": {
        "id": "NDEHCADOprhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v9DUXYHkJ5x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_slit = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_sumaho = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_slit.index = class_names\n",
        "mean_sd_aoi_sumaho.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_slit)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_sumaho)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title スリット（GradCAM vs GradCAM++）\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# 両データセットの統計を計算して表示\n",
        "for df, device_name in [(df_slit, \"GradCAM\"), (df_sumaho, \"GradCAM++\")]:\n",
        "    print(f\"\\n============= {device_name} データの統計値 =============\")\n",
        "\n",
        "    # 各レイヤーの詳細な統計量を計算\n",
        "    for column in columns_of_interest:\n",
        "        stats = df[column].describe()\n",
        "        percentiles = df[column].quantile([0.1, 0.25, 0.75, 0.9])\n",
        "\n",
        "        print(f\"\\n------- {column} の統計値 -------\")\n",
        "        print(f\"データ数: {int(stats['count'])}\")\n",
        "        print(f\"平均値: {stats['mean']:.6f}\")\n",
        "        print(f\"標準偏差: {stats['std']:.6f}\")\n",
        "        print(f\"最小値: {stats['min']:.6f}\")\n",
        "        print(f\"10パーセンタイル: {percentiles[0.1]:.6f}\")\n",
        "        print(f\"25パーセンタイル: {stats['25%']:.6f}\")\n",
        "        print(f\"中央値: {stats['50%']:.6f}\")\n",
        "        print(f\"75パーセンタイル: {stats['75%']:.6f}\")\n",
        "        print(f\"90パーセンタイル: {percentiles[0.9]:.6f}\")\n",
        "        print(f\"最大値: {stats['max']:.6f}\")\n",
        "\n",
        "        # 歪度と尖度も計算\n",
        "        skewness = df[column].skew()\n",
        "        kurtosis = df[column].kurtosis()\n",
        "        print(f\"歪度: {skewness:.6f}\")\n",
        "        print(f\"尖度: {kurtosis:.6f}\")\n",
        "\n",
        "    # 3つのレイヤーの合計値の統計を計算\n",
        "    layer_sum = df[columns_of_interest].sum(axis=1)\n",
        "    sum_stats = layer_sum.describe()\n",
        "    sum_percentiles = layer_sum.quantile([0.1, 0.9])\n",
        "    sum_skewness = layer_sum.skew()\n",
        "    sum_kurtosis = layer_sum.kurtosis()\n",
        "\n",
        "    print(f\"\\n------- 3レイヤーの合計値の統計 -------\")\n",
        "    print(f\"データ数: {int(sum_stats['count'])}\")\n",
        "    print(f\"平均値: {sum_stats['mean']:.6f}\")\n",
        "    print(f\"標準偏差: {sum_stats['std']:.6f}\")\n",
        "    print(f\"最小値: {sum_stats['min']:.6f}\")\n",
        "    print(f\"10パーセンタイル: {sum_percentiles[0.1]:.6f}\")\n",
        "    print(f\"25パーセンタイル: {sum_stats['25%']:.6f}\")\n",
        "    print(f\"中央値: {sum_stats['50%']:.6f}\")\n",
        "    print(f\"75パーセンタイル: {sum_stats['75%']:.6f}\")\n",
        "    print(f\"90パーセンタイル: {sum_percentiles[0.9]:.6f}\")\n",
        "    print(f\"最大値: {sum_stats['max']:.6f}\")\n",
        "\n",
        "    # 相関係数の計算\n",
        "    print(f\"\\n------- レイヤー間の相関係数 -------\")\n",
        "    correlation_matrix = df[columns_of_interest].corr()\n",
        "    print(correlation_matrix.round(6))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)  # セパレータの表示"
      ],
      "metadata": {
        "id": "sMhTeXg39b16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# 列名のリストを作成\n",
        "columns_of_interest = ['AOI_0.5_layer17', 'AOI_0.5_layer20', 'AOI_0.5_layer23', 'AOI_0.5_layer24_m_0', 'AOI_0.5_layer24_m_1', 'AOI_0.5_layer24_m_2']\n",
        "\n",
        "# AOIの平均と標準偏差を計算\n",
        "mean_sd_aoi_gradcam = df_slit.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "mean_sd_aoi_gradcampp = df_sumaho.groupby('top1')[columns_of_interest].agg(['mean', 'std']).reindex(range(len(class_names))).fillna(0)\n",
        "\n",
        "# インデックスにクラス名を設定\n",
        "mean_sd_aoi_gradcam.index = class_names\n",
        "mean_sd_aoi_gradcampp.index = class_names\n",
        "\n",
        "# 結果を表示\n",
        "print(\"Mean ± SD AOI for Slit Lamp Data\")\n",
        "print(mean_sd_aoi_gradcam)\n",
        "print(\"\\nMean ± SD AOI for Smartphone Data\")\n",
        "print(mean_sd_aoi_gradcampp)"
      ],
      "metadata": {
        "id": "XYWp2le2-Vjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2rBLnp6lbjW"
      },
      "source": [
        "##**Layerごとに解析**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkZ4ZbaoUEMX"
      },
      "outputs": [],
      "source": [
        "#@title スリットvsスマホ\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 合算データを作成\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# 青とオレンジのカラーパレットを定義\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# グラフ1: スリットランプとスマートフォンのデータ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ2: 合算データ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ3: slitとsumahoの全体比較\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between Slit Lamp and Smartphone', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GradCAM vs GradCAM++ (スリット)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 16,\n",
        "    'axes.titlesize': 16,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'legend.fontsize': 12,\n",
        "    'figure.titlesize': 18\n",
        "})\n",
        "\n",
        "####################\n",
        "layer = \"23\"\n",
        "####################\n",
        "layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形してSeabornでプロットできるようにする\n",
        "df_slit['Type'] = 'GradCAM'\n",
        "df_sumaho['Type'] = 'GradCAM++'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 合算データを作成\n",
        "df_total = pd.concat([df_slit, df_sumaho])\n",
        "df_total = df_total[['class_name', layer_name]].dropna()\n",
        "\n",
        "# 青とオレンジのカラーパレットを定義\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "# グラフ1: スリットランプとスマートフォンのデータ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=colors)\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Slit Lamp Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "legend.get_title().set_fontsize(14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradcam_overall_comparison.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# グラフ2: 合算データ\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='class_name', y=layer_name, data=df_total, order=class_names, color='#1f77b4')\n",
        "plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Box Plot of AOI_0.5 for Combined Data', fontsize=16, pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# グラフ3: slitとsumahoの全体比較\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='Type', y=layer_name, data=df_combined, palette=colors)\n",
        "plt.xlabel('Type', fontsize=14, labelpad=10)\n",
        "plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "plt.title('Comparison of AOI_0.5 between GradCAM and GradCAM++', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u8ZCGTRoJO7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 14,\n",
        "    'axes.labelsize': 18,\n",
        "    'axes.titlesize': 20,\n",
        "    'xtick.labelsize': 14,\n",
        "    'ytick.labelsize': 14,\n",
        "    'legend.fontsize': 14,\n",
        "    'figure.titlesize': 22\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# レイヤーごとのプロットと統計量計算\n",
        "for layer in [17, 20, 23]:\n",
        "    layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    # グラフの作成（縦横比3:4）\n",
        "    plt.figure(figsize=(12, 9))\n",
        "\n",
        "    # boxplotのスタイルをカスタマイズ（先ほどと同じ灰色のスタイル）\n",
        "    sns.boxplot(x='class_name', y=layer_name, data=df_slit, order=class_names,\n",
        "                color='#CCCCCC',\n",
        "                boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "                medianprops={'color':'black', 'linewidth':2.5},\n",
        "                flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "                whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "                capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "    plt.xlabel('Class', fontsize=18, labelpad=10)\n",
        "    plt.ylabel(layer_name, fontsize=18, labelpad=10)\n",
        "    plt.title(f'Box Plot of {layer_name} for Slit Lamp Data', fontsize=20, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # グラフを保存（高解像度）\n",
        "    plt.savefig(f'boxplot_{layer_name}.png', dpi=350, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 基本統計量の計算と表示\n",
        "    stats = df_slit.groupby('class_name')[layer_name].describe()\n",
        "    # クラスの順序を指定して並び替え\n",
        "    stats = stats.reindex(class_names)\n",
        "    print(f\"\\n{layer_name}の基本統計量:\")\n",
        "    print(stats)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "id": "v5jSgs_QuUly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJlHqzTQpuHP"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# # フォントサイズとスタイルの設定\n",
        "# plt.rcParams.update({\n",
        "#     'font.size': 12,\n",
        "#     'axes.labelsize': 16,\n",
        "#     'axes.titlesize': 16,\n",
        "#     'xtick.labelsize': 12,\n",
        "#     'ytick.labelsize': 12,\n",
        "#     'legend.fontsize': 12,\n",
        "#     'figure.titlesize': 18\n",
        "# })\n",
        "\n",
        "# # ファイルパスを指定\n",
        "# file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "# file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# # CSVファイルを読み込む\n",
        "# df_slit = pd.read_csv(file_path1)\n",
        "# df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# # クラス名を定義\n",
        "# class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# # クラス名を数字に対応させる\n",
        "# df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "# df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# # データフレームを整形してSeabornでプロットできるようにする\n",
        "# df_slit['Type'] = 'Slit'\n",
        "# df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# # 比較するレイヤーのリスト\n",
        "# layers_group1 = [\"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "# layers_group2 = [\"17\", \"20\", \"23\"]\n",
        "\n",
        "# def create_boxplots(layers, group_name):\n",
        "#     for layer in layers:\n",
        "#         layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "#         # 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "#         df_combined = pd.concat([\n",
        "#             df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "#             df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "#         ]).reset_index(drop=True)\n",
        "\n",
        "#         # グラフ: スリットランプとスマートフォンのデータ\n",
        "#         plt.figure(figsize=(15, 10))\n",
        "#         sns.boxplot(x='class_name', y=layer_name, hue='Type', data=df_combined, order=class_names, palette=['#333333', '#999999'])\n",
        "#         plt.xlabel('Class', fontsize=14, labelpad=10)\n",
        "#         plt.ylabel(layer_name, fontsize=14, labelpad=10)\n",
        "#         plt.title(f'Box Plot of {layer_name} for Slit Lamp and Smartphone Data', fontsize=16, pad=20)\n",
        "#         plt.xticks(rotation=45, ha='right')\n",
        "#         legend = plt.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "#         legend.get_title().set_fontsize(14)\n",
        "#         plt.tight_layout()\n",
        "#         plt.show()\n",
        "\n",
        "#     # レイヤー間の比較（幅を半分に）\n",
        "#     plt.figure(figsize=(7.5, 10))  # 幅を半分に変更\n",
        "#     df_melted = pd.melt(df_slit, id_vars=['class_name'], value_vars=[f'AOI_0.5_layer{layer}' for layer in layers], var_name='Layer', value_name='Value')\n",
        "#     sns.boxplot(x='Layer', y='Value', data=df_melted, palette='Blues')\n",
        "#     plt.xlabel('Layer', fontsize=14, labelpad=10)\n",
        "#     plt.ylabel('AOI_0.5 Value', fontsize=14, labelpad=10)\n",
        "#     plt.title(f'Comparison of AOI_0.5 Values\\nAcross Different Layers\\n({group_name})', fontsize=16, pad=20)\n",
        "#     plt.xticks(rotation=45, ha='right')\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # グループ1のグラフを作成\n",
        "# create_boxplots(layers_group1, \"Group 1: 24_m_0, 24_m_1, 24_m_2\")\n",
        "\n",
        "# # グループ2のグラフを作成\n",
        "# create_boxplots(layers_group2, \"Group 2: 17, 20, 23\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#レイヤー毎のAOI値（17, 20, 23）(24-0, 24-1, 24-2)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# フォントサイズとスタイルの設定をさらに大きく\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,          # 16 → 18\n",
        "    'axes.labelsize': 24,     # 22 → 24\n",
        "    'axes.titlesize': 26,     # 24 → 26\n",
        "    'xtick.labelsize': 18,    # 16 → 18\n",
        "    'ytick.labelsize': 18,    # 16 → 18\n",
        "    'legend.fontsize': 18,    # 16 → 18\n",
        "    'figure.titlesize': 28    # 26 → 28\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 2つのグループに分けるレイヤーのリスト\n",
        "layers_group1 = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\"]\n",
        "layers_group2 = [\"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# グラフ1（3:4のアスペクト比）\n",
        "plt.figure(figsize=(9, 12))  # 3:4の比率\n",
        "\n",
        "df_melted_1 = pd.melt(df[layers_group1], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_1['Layer'] = df_melted_1['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_1,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayers 17, 20, and 23', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_group1.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# グラフ2（3:4のアスペクト比）\n",
        "plt.figure(figsize=(9, 12))  # 3:4の比率\n",
        "\n",
        "df_melted_2 = pd.melt(df[layers_group2], var_name='Layer', value_name='AOI Value')\n",
        "df_melted_2['Layer'] = df_melted_2['Layer'].map(layer_mapping)\n",
        "\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted_2,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values\\nLayer 24 Outputs', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_group2.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示\n",
        "print(\"\\n基本統計量 - グループ1 (Layers 17, 20, 23):\")\n",
        "print(df[layers_group1].describe())\n",
        "print(\"\\n基本統計量 - グループ2 (Layer 24 Outputs):\")\n",
        "print(df[layers_group2].describe())"
      ],
      "metadata": {
        "id": "C92rF-kAx8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 18,\n",
        "    'axes.labelsize': 24,\n",
        "    'axes.titlesize': 26,\n",
        "    'xtick.labelsize': 18,\n",
        "    'ytick.labelsize': 18,\n",
        "    'legend.fontsize': 18,\n",
        "    'figure.titlesize': 28\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# すべてのレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# データを整形\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# グラフ作成（3:4のアスペクト比）\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# boxplotのスタイルをカスタマイズ\n",
        "sns.boxplot(x='Layer', y='AOI Value', data=df_melted,\n",
        "            color='#CCCCCC',\n",
        "            boxprops={'facecolor':'#CCCCCC', 'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=24, labelpad=15)\n",
        "plt.ylabel('AOI Value', fontsize=24, labelpad=15)\n",
        "plt.title('Comparison of AOI Values Across Different Layers', fontsize=26, pad=25)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_all.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示\n",
        "print(\"\\n基本統計量:\")\n",
        "print(df[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis 検定を実施\n",
        "h_statistic, p_value = stats.kruskal(*[df[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results:')\n",
        "print(f'H-statistic: {h_statistic:.3f}')\n",
        "print(f'p-value: {p_value:.3e}')\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）\n",
        "dunn = posthoc_dunn(df_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (p-values):')\n",
        "print(dunn.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）\n",
        "significant_pairs = []\n",
        "for i in dunn.index:\n",
        "    for j in dunn.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn.loc[i, j] < 0.05:\n",
        "                significant_pairs.append((i, j, dunn.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (p < 0.05):')\n",
        "for pair in significant_pairs:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "31icyQtM1UNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit_posthocs --q\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scikit_posthocs import posthoc_dunn\n",
        "\n",
        "# フォントサイズとスタイルの設定\n",
        "plt.rcParams.update({\n",
        "    'font.size': 22,  # フォントサイズを大きめに設定\n",
        "    'axes.labelsize': 28,  # 軸ラベルのフォントサイズ\n",
        "    'axes.titlesize': 30,  # タイトルのフォントサイズ\n",
        "    'xtick.labelsize': 24,  # x軸の目盛りラベルのフォントサイズ\n",
        "    'ytick.labelsize': 24,  # y軸の目盛りラベルのフォントサイズ\n",
        "    'legend.fontsize': 24,  # 凡例のフォントサイズ\n",
        "    'figure.titlesize': 32  # フィギュアタイトルのフォントサイズ\n",
        "})\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_gradcam++.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df1 = pd.read_csv(file_path1)\n",
        "df2 = pd.read_csv(file_path2)\n",
        "\n",
        "# 使用するレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# レイヤー名を変更するマッピング\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "\n",
        "# データを整形\n",
        "df1_melted = pd.melt(df1[layers], var_name='Layer', value_name='AOI Value')\n",
        "df1_melted['Layer'] = df1_melted['Layer'].map(layer_mapping)\n",
        "df1_melted['Method'] = 'Grad-CAM'\n",
        "\n",
        "df2_melted = pd.melt(df2[layers], var_name='Layer', value_name='AOI Value')\n",
        "df2_melted['Layer'] = df2_melted['Layer'].map(layer_mapping)\n",
        "df2_melted['Method'] = 'Grad-CAM++'\n",
        "\n",
        "# データを結合し、インデックスを振り直す\n",
        "df_combined = pd.concat([df1_melted, df2_melted], ignore_index=True)\n",
        "# あるいは\n",
        "# df_combined = pd.concat([df1_melted, df2_melted])\n",
        "# df_combined = df_combined.reset_index(drop=True)\n",
        "\n",
        "# グラフ作成（4:3のアスペクト比）\n",
        "plt.figure(figsize=(16, 12))  # 4:3の比率になるように調整\n",
        "\n",
        "# boxplotのスタイルをカスタマイズ\n",
        "sns.boxplot(x='Layer', y='AOI Value', hue='Method', data=df_combined,\n",
        "            palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
        "            boxprops={'edgecolor':'black', 'linewidth': 1.5},\n",
        "            medianprops={'color':'black', 'linewidth':2.5},\n",
        "            flierprops={'marker':'o', 'markerfacecolor':'white', 'markeredgecolor':'black', 'markersize':6},\n",
        "            whiskerprops={'color':'black', 'linewidth':1.5},\n",
        "            capprops={'color':'black', 'linewidth':1.5})\n",
        "\n",
        "plt.xlabel('Layer', fontsize=28, labelpad=15)  # 軸ラベルのフォントサイズをさらに大きく\n",
        "plt.ylabel('AOI Value', fontsize=28, labelpad=15)  # 軸ラベルのフォントサイズをさらに大きく\n",
        "plt.title('Comparison of AOI Values Across Different Layers and Methods', fontsize=30, pad=25)  # タイトルのフォントサイズをさらに大きく\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Type')\n",
        "plt.tight_layout()\n",
        "\n",
        "# グラフを保存（350dpi）\n",
        "plt.savefig('boxplot_aoi_comparison_all_combined.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 基本統計量の計算と表示 (Grad-CAM)\n",
        "print(\"\\n基本統計量 (Grad-CAM):\")\n",
        "print(df1[layers].describe())\n",
        "\n",
        "# 基本統計量の計算と表示 (Grad-CAM++)\n",
        "print(\"\\n基本統計量 (Grad-CAM++):\")\n",
        "print(df2[layers].describe())\n",
        "\n",
        "# Kruskal-Wallis 検定を実施 (Grad-CAM)\n",
        "h_statistic1, p_value1 = stats.kruskal(*[df1[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM):')\n",
        "print(f'H-statistic: {h_statistic1:.3f}')\n",
        "print(f'p-value: {p_value1:.3e}')\n",
        "\n",
        "# Kruskal-Wallis 検定を実施 (Grad-CAM++)\n",
        "h_statistic2, p_value2 = stats.kruskal(*[df2[layer] for layer in layers])\n",
        "\n",
        "print('\\nKruskal-Wallis test results (Grad-CAM++):')\n",
        "print(f'H-statistic: {h_statistic2:.3f}')\n",
        "print(f'p-value: {p_value2:.3e}')\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）(Grad-CAM)\n",
        "dunn1 = posthoc_dunn(df1_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM) (p-values):')\n",
        "print(dunn1.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# Dunn's testによる多重比較（Bonferroni補正）(Grad-CAM++)\n",
        "dunn2 = posthoc_dunn(df2_melted, val_col='AOI Value', group_col='Layer', p_adjust='bonferroni')\n",
        "\n",
        "print('\\nDunn\\'s test results (Grad-CAM++) (p-values):')\n",
        "print(dunn2.round(4))  # p値を4桁に丸める\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）(Grad-CAM)\n",
        "significant_pairs1 = []\n",
        "for i in dunn1.index:\n",
        "    for j in dunn1.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn1.loc[i, j] < 0.05:\n",
        "                significant_pairs1.append((i, j, dunn1.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (Grad-CAM) (p < 0.05):')\n",
        "for pair in significant_pairs1:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')\n",
        "\n",
        "# 有意差のあるペアを抽出（p < 0.05）(Grad-CAM++)\n",
        "significant_pairs2 = []\n",
        "for i in dunn2.index:\n",
        "    for j in dunn2.columns:\n",
        "        if i < j:  # 重複を避けるため、上三角行列のみを見る\n",
        "            if dunn2.loc[i, j] < 0.05:\n",
        "                significant_pairs2.append((i, j, dunn2.loc[i, j]))\n",
        "\n",
        "print('\\n有意差のあるペア (Grad-CAM++) (p < 0.05):')\n",
        "for pair in significant_pairs2:\n",
        "    print(f'{pair[0]} vs {pair[1]}: p = {pair[2]:.4f}')"
      ],
      "metadata": {
        "id": "mRmADr6uMAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mt7gOfde1UPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgFUt3AFWwvA"
      },
      "outputs": [],
      "source": [
        "####\n",
        "#### スリット vs スマホの比較\n",
        "####\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# 各クラスごとに対応のあるt検定を行う\n",
        "results = []\n",
        "for class_name in class_names:\n",
        "    slit_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "    sumaho_data = df_sumaho[df_sumaho['class_name'] == class_name][layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "    # 対応のあるデータを取るため、最小の長さに合わせる\n",
        "    min_length = min(len(slit_data), len(sumaho_data))\n",
        "    slit_data = slit_data[:min_length]\n",
        "    sumaho_data = sumaho_data[:min_length]\n",
        "\n",
        "    t_stat, p_value = stats.ttest_rel(slit_data, sumaho_data)\n",
        "\n",
        "    # スリットランプデータの統計値\n",
        "    slit_mean = slit_data.mean()\n",
        "    slit_std = slit_data.std()\n",
        "\n",
        "    # スマートフォンデータの統計値\n",
        "    sumaho_mean = sumaho_data.mean()\n",
        "    sumaho_std = sumaho_data.std()\n",
        "\n",
        "    results.append({\n",
        "        'class_name': class_name,\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_value,\n",
        "        'slit_mean': slit_mean,\n",
        "        'slit_std': slit_std,\n",
        "        'sumaho_mean': sumaho_mean,\n",
        "        'sumaho_std': sumaho_std\n",
        "    })\n",
        "\n",
        "# 全クラスまとめた対応のあるt検定\n",
        "all_slit_data = df_slit[layer_name].dropna().reset_index(drop=True)\n",
        "all_sumaho_data = df_sumaho[layer_name].dropna().reset_index(drop=True)\n",
        "\n",
        "# 対応のあるデータを取るため、最小の長さに合わせる\n",
        "min_length_all = min(len(all_slit_data), len(all_sumaho_data))\n",
        "all_slit_data = all_slit_data[:min_length_all]\n",
        "all_sumaho_data = all_sumaho_data[:min_length_all]\n",
        "\n",
        "t_stat_all, p_value_all = stats.ttest_rel(all_slit_data, all_sumaho_data)\n",
        "\n",
        "# 全クラスまとめた統計値\n",
        "all_slit_mean = all_slit_data.mean()\n",
        "all_slit_std = all_slit_data.std()\n",
        "all_sumaho_mean = all_sumaho_data.mean()\n",
        "all_sumaho_std = all_sumaho_data.std()\n",
        "\n",
        "# 結果を追加\n",
        "results.append({\n",
        "    'class_name': 'All Classes',\n",
        "    't_stat': t_stat_all,\n",
        "    'p_value': p_value_all,\n",
        "    'slit_mean': all_slit_mean,\n",
        "    'slit_std': all_slit_std,\n",
        "    'sumaho_mean': all_sumaho_mean,\n",
        "    'sumaho_std': all_sumaho_std\n",
        "})\n",
        "\n",
        "# 結果をデータフレームに変換して表示\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results['p_value'] = df_results['p_value'].map(lambda x: f'{x:.3f}')\n",
        "\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjAiQjnMYHjN"
      },
      "outputs": [],
      "source": [
        "# #クラス毎の差（スマホ＋スリット）\n",
        "# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# # ANOVAを実行\n",
        "# anova_result = stats.f_oneway(\n",
        "#     df_combined[df_combined['class_name'] == 'infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'normal'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'non-infection'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'scar'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'tumor'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'deposit'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'APAC'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'lens opacity'][layer_name],\n",
        "#     df_combined[df_combined['class_name'] == 'bullous'][layer_name]\n",
        "# )\n",
        "\n",
        "# print(f\"ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "# # 事後検定（TukeyのHSD検定）を実行\n",
        "# tukey_result = pairwise_tukeyhsd(df_combined[layer_name], df_combined['class_name'])\n",
        "\n",
        "# # 結果を表示\n",
        "# print(tukey_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JihM5ooC3VCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"全ての値が同じかチェック\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def dunn_test(groups, group_names):\n",
        "    \"\"\"\n",
        "    Dunn's testの実装\n",
        "    \"\"\"\n",
        "    # 全データをランク付け\n",
        "    all_data = np.concatenate(groups)\n",
        "    ranks = stats.rankdata(all_data)\n",
        "\n",
        "    # グループごとの平均ランクを計算\n",
        "    start = 0\n",
        "    mean_ranks = []\n",
        "    ns = []\n",
        "    for group in groups:\n",
        "        n = len(group)\n",
        "        group_ranks = ranks[start:start + n]\n",
        "        mean_ranks.append(np.mean(group_ranks))\n",
        "        ns.append(n)\n",
        "        start += n\n",
        "\n",
        "    # 全ペアの組み合わせでDunn's testを実行\n",
        "    N = len(ranks)\n",
        "    k = len(groups)\n",
        "    comparisons = []\n",
        "    p_values = []\n",
        "\n",
        "    for i in range(k):\n",
        "        for j in range(i + 1, k):\n",
        "            # 平均ランクの差\n",
        "            diff = abs(mean_ranks[i] - mean_ranks[j])\n",
        "\n",
        "            # 標準誤差\n",
        "            se = np.sqrt((N * (N + 1) / 12) * (1/ns[i] + 1/ns[j]))\n",
        "\n",
        "            # z統計量\n",
        "            z = diff / se\n",
        "\n",
        "            # p値（両側検定）\n",
        "            p = 2 * (1 - stats.norm.cdf(abs(z)))\n",
        "\n",
        "            comparisons.append((group_names[i], group_names[j]))\n",
        "            p_values.append(p)\n",
        "\n",
        "    # Bonferroni補正\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, method='bonferroni')\n",
        "\n",
        "    # 有意な結果を返す\n",
        "    significant_results = []\n",
        "    for (name1, name2), p_corr, is_rej in zip(comparisons, p_corrected, rejected):\n",
        "        if is_rej:  # p < 0.05 after correction\n",
        "            significant_results.append({\n",
        "                'group1': name1,\n",
        "                'group2': name2,\n",
        "                'p_value': p_corr\n",
        "            })\n",
        "\n",
        "    return significant_results\n",
        "\n",
        "for layer in layers:\n",
        "    # レイヤー名を設定\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # 欠損値を除外してデータを準備\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # クラスごとのデータを収集（欠損値を除外）\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # 少なくとも2つのグループが必要\n",
        "            # 全ての値が同じかチェック\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\n全てのクラスで同じ値: {same_value:.4f}\")\n",
        "                print(\"統計的検定は不要です。\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testを実行\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-test結果:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # Dunn's testを実行\n",
        "                significant_pairs = dunn_test(class_groups, valid_class_names)\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\n有意差のあるペア (Bonferroni補正後 p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}\")\n",
        "        else:\n",
        "            print(f\"\\n警告: {layer_name}の解析に十分なデータがありません。\")\n",
        "    else:\n",
        "        print(f\"\\n警告: {layer_name}に有効なデータがありません。\")"
      ],
      "metadata": {
        "id": "9mmq-hNs_4Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path)\n",
        "\n",
        "# クラス名を定義（指定された順序）\n",
        "class_names = [\"Normal\", \"Infectious keratitis\", \"Non-infection keratitis\",\n",
        "               \"Scar\", \"Tumor\", \"Deposit\", \"APAC\", \"Lens opacity\", \"Bullous keratopathy\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [17, 20, 23, \"24_m_0\", \"24_m_1\", \"24_m_2\"]\n",
        "\n",
        "def check_all_values_same(groups):\n",
        "    \"\"\"全ての値が同じかチェック\"\"\"\n",
        "    all_values = np.concatenate([group.values for group in groups])\n",
        "    return np.all(all_values == all_values[0]), all_values[0]\n",
        "\n",
        "def perform_pairwise_mannwhitney(groups, group_names, alpha=0.05):\n",
        "    \"\"\"ペアワイズMann-Whitney U検定を実行\"\"\"\n",
        "    n_groups = len(groups)\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_groups):\n",
        "        for j in range(i+1, n_groups):\n",
        "            try:\n",
        "                stat, p_value = mannwhitneyu(groups[i], groups[j], alternative='two-sided')\n",
        "                n1, n2 = len(groups[i]), len(groups[j])\n",
        "                effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "                if p_value < alpha:\n",
        "                    results.append({\n",
        "                        'group1': group_names[i],\n",
        "                        'group2': group_names[j],\n",
        "                        'p_value': p_value,\n",
        "                        'effect_size': effect_size\n",
        "                    })\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "    return results\n",
        "\n",
        "for layer in layers:\n",
        "    # レイヤー名を設定\n",
        "    if isinstance(layer, int):\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "    else:\n",
        "        layer_name = f\"AOI_0.5_layer{layer}\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"\\nAnalysis for {layer_name}\")\n",
        "    print('='*80)\n",
        "\n",
        "    # 欠損値を除外してデータを準備\n",
        "    data_for_analysis = df_slit[[layer_name, 'class_name']].dropna()\n",
        "\n",
        "    if len(data_for_analysis) > 0:\n",
        "        # クラスごとのデータを収集（欠損値を除外）\n",
        "        class_groups = []\n",
        "        valid_class_names = []\n",
        "\n",
        "        for class_name in class_names:\n",
        "            group_data = df_slit[df_slit['class_name'] == class_name][layer_name].dropna()\n",
        "            if len(group_data) > 0:\n",
        "                class_groups.append(group_data)\n",
        "                valid_class_names.append(class_name)\n",
        "\n",
        "        if len(class_groups) > 1:  # 少なくとも2つのグループが必要\n",
        "            # 全ての値が同じかチェック\n",
        "            is_all_same, same_value = check_all_values_same(class_groups)\n",
        "\n",
        "            if is_all_same:\n",
        "                print(f\"\\n全てのクラスで同じ値: {same_value:.4f}\")\n",
        "                print(\"統計的検定は不要です。\")\n",
        "                continue\n",
        "\n",
        "            # Kruskal-Wallis H-testを実行\n",
        "            h_statistic, p_value = stats.kruskal(*class_groups)\n",
        "\n",
        "            print(f\"\\nKruskal-Wallis H-test結果:\")\n",
        "            print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "            print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "            if p_value < 0.05:\n",
        "                # ペアワイズMann-Whitney U検定を実行\n",
        "                significant_pairs = perform_pairwise_mannwhitney(\n",
        "                    class_groups,\n",
        "                    valid_class_names\n",
        "                )\n",
        "\n",
        "                if significant_pairs:\n",
        "                    print(\"\\n有意差のあるペア (p < 0.05):\")\n",
        "                    for result in significant_pairs:\n",
        "                        print(f\"{result['group1']:25} vs {result['group2']:25}: \"\n",
        "                              f\"p = {result['p_value']:.4e}, effect size = {result['effect_size']:.4f}\")\n",
        "        else:\n",
        "            print(f\"\\n警告: {layer_name}の解析に十分なデータがありません。\")\n",
        "    else:\n",
        "        print(f\"\\n警告: {layer_name}に有効なデータがありません。\")"
      ],
      "metadata": {
        "id": "NShTeYYE3VEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "from scipy.stats import mannwhitneyu\n",
        "\n",
        "# ファイルパスを指定\n",
        "file_path = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 解析するレイヤーのリスト\n",
        "layers = [\"AOI_0.5_layer17\", \"AOI_0.5_layer20\", \"AOI_0.5_layer23\",\n",
        "          \"AOI_0.5_layer24_m_0\", \"AOI_0.5_layer24_m_1\", \"AOI_0.5_layer24_m_2\"]\n",
        "\n",
        "# データを長形式に変換\n",
        "df_melted = pd.melt(df[layers], var_name='Layer', value_name='AOI Value')\n",
        "\n",
        "# レイヤー名を簡略化\n",
        "layer_mapping = {\n",
        "    \"AOI_0.5_layer17\": \"Layer 17\",\n",
        "    \"AOI_0.5_layer20\": \"Layer 20\",\n",
        "    \"AOI_0.5_layer23\": \"Layer 23\",\n",
        "    \"AOI_0.5_layer24_m_0\": \"Layer 24_0\",\n",
        "    \"AOI_0.5_layer24_m_1\": \"Layer 24_1\",\n",
        "    \"AOI_0.5_layer24_m_2\": \"Layer 24_2\"\n",
        "}\n",
        "df_melted['Layer'] = df_melted['Layer'].map(layer_mapping)\n",
        "\n",
        "# 欠損値を除外\n",
        "df_melted = df_melted.dropna()\n",
        "\n",
        "# Kruskal-Wallis H-testを実行\n",
        "groups = [group['AOI Value'].values for name, group in df_melted.groupby('Layer')]\n",
        "h_statistic, p_value = stats.kruskal(*groups)\n",
        "\n",
        "print(\"Kruskal-Wallis H-test結果:\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"p-value: {p_value:.4e}\")\n",
        "\n",
        "# レイヤーのペアワイズ比較（Mann-Whitney U test）を実行\n",
        "layer_names = sorted(df_melted['Layer'].unique())\n",
        "significant_pairs = []\n",
        "\n",
        "print(\"\\n有意差のあるペア (p < 0.05):\")\n",
        "for i, layer1 in enumerate(layer_names):\n",
        "    for layer2 in layer_names[i+1:]:\n",
        "        group1 = df_melted[df_melted['Layer'] == layer1]['AOI Value']\n",
        "        group2 = df_melted[df_melted['Layer'] == layer2]['AOI Value']\n",
        "\n",
        "        try:\n",
        "            stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
        "            n1, n2 = len(group1), len(group2)\n",
        "            effect_size = 1 - (2 * stat) / (n1 * n2)  # Common language effect size\n",
        "\n",
        "            if p_value < 0.05:  # Bonferroni補正を適用する場合は 0.05/len(pairs) を使用\n",
        "                print(f\"{layer1:10} vs {layer2:10}: p = {p_value:.4e}, effect size = {effect_size:.4f}\")\n",
        "                significant_pairs.append((layer1, layer2, p_value, effect_size))\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "# 各レイヤーの基本統計量（中央値と四分位数を含む）\n",
        "print(\"\\n各レイヤーの基本統計量:\")\n",
        "stats_summary = df_melted.groupby('Layer')['AOI Value'].agg([\n",
        "    ('count', 'count'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('std', 'std'),\n",
        "    ('Q1', lambda x: x.quantile(0.25)),\n",
        "    ('Q3', lambda x: x.quantile(0.75))\n",
        "]).round(4)\n",
        "\n",
        "print(stats_summary)\n",
        "\n",
        "# マトリックス形式で有意差を表示\n",
        "print(\"\\n有意差マトリックス (★: p < 0.05)\")\n",
        "matrix = pd.DataFrame(index=layer_names, columns=layer_names)\n",
        "np.fill_diagonal(matrix.values, '-')\n",
        "matrix = matrix.fillna('　')\n",
        "\n",
        "for pair in significant_pairs:\n",
        "    matrix.loc[pair[0], pair[1]] = \"★\"\n",
        "    matrix.loc[pair[1], pair[0]] = \"★\"\n",
        "\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "VJkb_wra3VGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59zc1fuTqBK7"
      },
      "source": [
        "###**ANOVA_heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exil8PT2ZzsS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'})\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6M5cbfdFlv3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# ファイルパスを設定\n",
        "file_path1 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_slit_total_updated.csv'\n",
        "file_path2 = '/gdrive/MyDrive/研究/進行中の研究/角膜スマートフォンAIプロジェクト/前原の240問_GradCAM/maehara_sumaho_total_updated.csv'\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 必要なカラムだけを選択し、欠損値を削除して結合\n",
        "df_combined = pd.concat([\n",
        "    df_slit[['class_name', layer_name, 'Type']].dropna(),\n",
        "    df_sumaho[['class_name', layer_name, 'Type']].dropna()\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# 事後検定（TukeyのHSD検定）を実行\n",
        "tukey_result = pairwise_tukeyhsd(endog=df_combined[layer_name], groups=df_combined['class_name'])\n",
        "\n",
        "# 結果をDataFrameに変換\n",
        "tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "# マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "# マトリックスにp値を入力\n",
        "for i in range(len(tukey_result_df)):\n",
        "    group1 = tukey_result_df.loc[i, 'group1']\n",
        "    group2 = tukey_result_df.loc[i, 'group2']\n",
        "    p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "    matrix.loc[group1, group2] = p_value\n",
        "    matrix.loc[group2, group1] = p_value\n",
        "\n",
        "# 同じクラス間のセルをNaNに設定\n",
        "np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "# カスタムカラーマップを作成（オレンジ、グレー、白）\n",
        "colors = ['#FFA500', '#D3D3D3', 'white']  # オレンジ、グレー、白\n",
        "custom_cmap = ListedColormap(colors)\n",
        "\n",
        "# p値に基づいてデータを作成（NaNは2、p<0.05は0、それ以外は1）\n",
        "color_data = np.where(np.isnan(matrix), 2, np.where(matrix < 0.05, 0, 1))\n",
        "\n",
        "# ヒートマップを描画\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(color_data,\n",
        "            annot=matrix,  # 元のp値を表示\n",
        "            fmt=\".3f\",\n",
        "            cmap=custom_cmap,\n",
        "            cbar=False,  # カラーバーを非表示に\n",
        "            xticklabels=class_names,  # x軸のラベルをクラス名に設定\n",
        "            yticklabels=class_names)  # y軸のラベルをクラス名に設定\n",
        "\n",
        "# カスタムカラーバーを追加\n",
        "sm = plt.cm.ScalarMappable(cmap=ListedColormap(colors[:2]), norm=plt.Normalize(vmin=0, vmax=1))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, label='p-value', ticks=[0.25, 0.75])\n",
        "cbar.set_ticklabels(['p < 0.05', 'p ≥ 0.05'])\n",
        "\n",
        "plt.title('Pairwise Comparison P-Values (Tukey HSD)')\n",
        "plt.xticks(rotation=45, ha='right')  # x軸のラベルを45度回転\n",
        "plt.yticks(rotation=0)  # y軸のラベルを水平に\n",
        "plt.tight_layout()  # レイアウトを調整\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbxW3P2OqFTb"
      },
      "source": [
        "###**Tukey_sumaho/slit別**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m7Z9FCzbSmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "def perform_anova_and_tukey(df, title):\n",
        "    # 欠損値を削除\n",
        "    df = df.dropna(subset=[layer_name])\n",
        "\n",
        "    # ANOVAを実行\n",
        "    anova_result = stats.f_oneway(\n",
        "        df[df['class_name'] == 'infection'][layer_name],\n",
        "        df[df['class_name'] == 'normal'][layer_name],\n",
        "        df[df['class_name'] == 'non-infection'][layer_name],\n",
        "        df[df['class_name'] == 'scar'][layer_name],\n",
        "        df[df['class_name'] == 'tumor'][layer_name],\n",
        "        df[df['class_name'] == 'deposit'][layer_name],\n",
        "        df[df['class_name'] == 'APAC'][layer_name],\n",
        "        df[df['class_name'] == 'lens opacity'][layer_name],\n",
        "        df[df['class_name'] == 'bullous'][layer_name]\n",
        "    )\n",
        "\n",
        "    print(f\"{title} ANOVA result: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
        "\n",
        "    # 事後検定（TukeyのHSD検定）を実行\n",
        "    tukey_result = pairwise_tukeyhsd(endog=df[layer_name], groups=df['class_name'])\n",
        "\n",
        "    # 結果をDataFrameに変換\n",
        "    tukey_result_df = pd.DataFrame(data=tukey_result._results_table.data[1:], columns=tukey_result._results_table.data[0])\n",
        "\n",
        "    # マトリックス図を作成するためにクラス名をインデックスとして使用\n",
        "    matrix = pd.DataFrame(np.zeros((len(class_names), len(class_names))), index=class_names, columns=class_names)\n",
        "\n",
        "    # マトリックスにp値を入力\n",
        "    for i in range(len(tukey_result_df)):\n",
        "        group1 = tukey_result_df.loc[i, 'group1']\n",
        "        group2 = tukey_result_df.loc[i, 'group2']\n",
        "        p_value = float(tukey_result_df.loc[i, 'p-adj'])\n",
        "        matrix.loc[group1, group2] = p_value\n",
        "        matrix.loc[group2, group1] = p_value\n",
        "\n",
        "    # 同じクラス間のセルをNaNに設定\n",
        "    np.fill_diagonal(matrix.values, np.nan)\n",
        "\n",
        "    # ヒートマップを描画\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(matrix, annot=True, fmt=\".3f\", cmap=\"coolwarm\", cbar_kws={'label': 'p-value'}, mask=matrix.isnull())\n",
        "    plt.title(f'Pairwise Comparison P-Values (Tukey HSD) - {title}')\n",
        "    plt.show()\n",
        "\n",
        "# スリットランプデータの解析\n",
        "perform_anova_and_tukey(df_slit, \"Slit Lamp Data\")\n",
        "\n",
        "# スマートフォンデータの解析\n",
        "perform_anova_and_tukey(df_sumaho, \"Smartphone Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ubPn9Erqnid"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# CSVファイルを読み込む\n",
        "df_slit = pd.read_csv(file_path1)\n",
        "df_sumaho = pd.read_csv(file_path2)\n",
        "\n",
        "# クラス名を定義\n",
        "class_names = [\"infection\", \"normal\", \"non-infection\", \"scar\", \"tumor\", \"deposit\", \"APAC\", \"lens opacity\", \"bullous\"]\n",
        "\n",
        "# クラス名を数字に対応させる\n",
        "df_slit['class_name'] = df_slit['top1'].map(dict(enumerate(class_names)))\n",
        "df_sumaho['class_name'] = df_sumaho['top1'].map(dict(enumerate(class_names)))\n",
        "\n",
        "# データフレームを整形\n",
        "df_slit['Type'] = 'Slit'\n",
        "df_sumaho['Type'] = 'Sumaho'\n",
        "\n",
        "# 両方のデータを結合\n",
        "df_combined = pd.concat([df_slit, df_sumaho]).reset_index(drop=True)\n",
        "\n",
        "# \"scar\" + \"non-infection\" グループとその他のクラスに分類\n",
        "df_combined['group'] = df_combined['class_name'].apply(lambda x: 'scar + non-infection' if x in ['scar', 'non-infection'] else 'others')\n",
        "\n",
        "# グループごとにデータを抽出\n",
        "scar_non_infection_group = df_combined[df_combined['group'] == 'scar + non-infection'][layer_name].dropna()\n",
        "others_group = df_combined[df_combined['group'] == 'others'][layer_name].dropna()\n",
        "\n",
        "# t検定を実行\n",
        "t_stat, p_value = stats.ttest_ind(scar_non_infection_group, others_group)\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "# 統計値を計算\n",
        "scar_non_infection_mean = scar_non_infection_group.mean()\n",
        "scar_non_infection_std = scar_non_infection_group.std()\n",
        "others_mean = others_group.mean()\n",
        "others_std = others_group.std()\n",
        "\n",
        "print(f\"scar + non-infection mean: {scar_non_infection_mean}, std: {scar_non_infection_std}\")\n",
        "print(f\"others mean: {others_mean}, std: {others_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcxqbJ_GrbPY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP2qxBYFl8gtN4TFFYrxzEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}